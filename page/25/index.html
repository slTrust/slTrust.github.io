
 <!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
  
    <title>Almost</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Stevin">
    

    
    <meta property="og:type" content="website">
<meta property="og:title" content="Almost">
<meta property="og:url" content="http://yoursite.com/page/25/index.html">
<meta property="og:site_name" content="Almost">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Almost">

    
    <link rel="alternative" href="/atom.xml" title="Almost" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>
</html>
  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Almost" title="Almost"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Almost">Almost</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:yoursite.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/30/Py007-01-08requests处理验证码/" title="Py007-01-08requests处理验证码" itemprop="url">Py007-01-08requests处理验证码</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-30T14:23:15.000Z" itemprop="datePublished"> Published 2018-11-30</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="待更新"><a href="#待更新" class="headerlink" title="待更新"></a>待更新</h3>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/30/Py007-01-07requests高级操作cookie和代理/" title="Py007-01-07requests高级操作cookie和代理" itemprop="url">Py007-01-07requests高级操作cookie和代理</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-30T13:27:14.000Z" itemprop="datePublished"> Published 2018-11-30</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="requests模块高级："><a href="#requests模块高级：" class="headerlink" title="requests模块高级："></a>requests模块高级：</h3><h4 id="cookie："><a href="#cookie：" class="headerlink" title="cookie："></a>cookie：</h4><blockquote>
<p>基于用户的用户数据</p>
<ul>
<li>需求：爬取张三用户的豆瓣网的个人主页页面数据</li>
</ul>
</blockquote>
<ul>
<li>在没有登录的情况下浏览器访问 <a href="https://www.douban.com/people/185687620/" target="_blank" rel="noopener">https://www.douban.com/people/185687620/</a></li>
<li>发现不是对应主页信息而是要求你登录的页面</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#问题：没有获取个人主页的页面数据</span><br><span class="line">#原因：爬虫程序没有严格遵从浏览器的请求流程</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://www.douban.com/people/185687620/&apos;</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">    &#125;</span><br><span class="line">#2.发起请求</span><br><span class="line">response = requests.get(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">#3.获取页面数据</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">#4.持久化存储</span><br><span class="line">with open(&apos;./douban.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(page_text)</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="无法访问对应页码的成因"><a href="#无法访问对应页码的成因" class="headerlink" title="无法访问对应页码的成因"></a>无法访问对应页码的成因</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">结果发现，写入到文件中的数据，不是张三个人页面的数据，而是豆瓣登陆的首页面，why？首先我们来回顾下cookie的相关概念及作用：</span><br></pre></td></tr></table></figure>
<ul>
<li><p>cookie概念：当用户通过浏览器首次访问一个域名时，访问的web服务器会给客户端发送数据，以保持web服务器与客户端之间的状态保持，这些数据就是cookie。</p>
</li>
<li><p>cookie作用：我们在浏览器中，经常涉及到数据的交换，比如你登录邮箱，登录一个页面。我们经常会在此时设置30天内记住我，或者自动登录选项。那么它们是怎么记录信息的呢，答案就是今天的主角cookie了，Cookie是由HTTP服务器设置的，保存在浏览器中，但HTTP协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。就像我们去超市买东西，没有积分卡的情况下，我们买完东西之后，超市没有我们的任何消费信息，但我们办了积分卡之后，超市就有了我们的消费信息。cookie就像是积分卡，可以保存积分，商品就是我们的信息，超市的系统就像服务器后台，http协议就是交易的过程。</p>
</li>
<li><p>经过cookie的相关介绍，其实你已经知道了为什么上述案例中爬取到的不是张三个人信息页，而是登录页面。那应该如何抓取到张三的个人信息页呢？</p>
</li>
</ul>
<blockquote>
<p>思路：</p>
</blockquote>
<ol>
<li>我们需要使用爬虫程序对豆瓣的登录时的请求进行一次抓取，获取请求中的cookie数据</li>
<li>在使用个人信息页的url进行请求时，该请求需要携带 1 中的cookie，只有携带了cookie后，服务器才可识别这次请求的用户信息，方可响应回指定的用户信息页数据</li>
</ol>
<h4 id="cookie作用：服务器端使用cookie来记录客户端的状态信息。"><a href="#cookie作用：服务器端使用cookie来记录客户端的状态信息。" class="headerlink" title="cookie作用：服务器端使用cookie来记录客户端的状态信息。"></a>cookie作用：服务器端使用cookie来记录客户端的状态信息。</h4><blockquote>
<p>实现流程：</p>
</blockquote>
<ol>
<li>执行登录操作（获取cookie）</li>
<li>在发起个人主页请求时，需要将cookie携带到该请求中</li>
</ol>
<p>注意：session对象：发送请求（会将cookie对象进行自动存储）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">session = requests.session()</span><br><span class="line">#1.发起登录请求：将cookie获取，切存储到session对象中</span><br><span class="line">login_url = &apos;https://accounts.douban.com/login&apos;</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;source&quot;: &quot;None&quot;,</span><br><span class="line">    &quot;redir&quot;: &quot;https://www.douban.com/people/185687620/&quot;,</span><br><span class="line">    &quot;form_email&quot;: &quot;15027900535&quot;,</span><br><span class="line">    &quot;form_password&quot;: &quot;bobo@15027900535&quot;,</span><br><span class="line">    &quot;login&quot;: &quot;登录&quot;,</span><br><span class="line">&#125;</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line">#使用session发起post请求</span><br><span class="line">login_response = session.post(url=login_url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line">#2.对个人主页发起请求（session（cookie）），获取响应页面数据</span><br><span class="line">url = &apos;https://www.douban.com/people/185687620/&apos;</span><br><span class="line">response = session.get(url=url,headers=headers)</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">with open(&apos;./douban110.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(page_text)</span><br></pre></td></tr></table></figure>
<h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><blockquote>
<p>基于requests模块的代理操作</p>
</blockquote>
<ul>
<li>什么是代理</li>
</ul>
<p>代理就是第三方代替本体处理相关事务。例如：生活中的代理：代购，中介，微商……</p>
<ul>
<li>爬虫中为什么需要使用代理</li>
</ul>
<p>一些网站会有相应的反爬虫措施，例如很多网站会检测某一段时间某个IP的访问次数，如果访问频率太快以至于看起来不像正常访客，它可能就会会禁止这个IP的访问。所以我们需要设置一些代理IP，每隔一段时间换一个代理IP，就算IP被禁止，依然可以换个IP继续爬取。</p>
<ul>
<li>代理的分类：</li>
</ul>
<p>正向代理：代理客户端获取数据。正向代理是为了保护客户端防止被追究责任。</p>
<p>反向代理：代理服务器提供数据。反向代理是为了保护服务器或负责负载均衡。</p>
<ul>
<li>免费代理ip提供网站</li>
</ul>
<ol>
<li><p><a href="http://www.goubanjia.com/" target="_blank" rel="noopener">http://www.goubanjia.com/</a> (推荐)</p>
</li>
<li><p>西祠代理</p>
</li>
<li><p>快代理</p>
</li>
</ol>
<blockquote>
<p>代理实例</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.baidu.com/s?ie=utf-8&amp;wd=ip&apos;</span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#将代理ip封装到字典</span><br><span class="line">proxy = &#123;</span><br><span class="line">    &apos;http&apos;:&apos;77.73.69.120:3128&apos;,</span><br><span class="line">   # &apos;https&apos;:&apos;xxxx&apos; 也可以是https</span><br><span class="line">&#125;</span><br><span class="line">#更换网路IP</span><br><span class="line">response = requests.get(url=url,proxies=proxy,headers=headers)</span><br><span class="line"></span><br><span class="line">with open(&apos;./daili.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(response.text)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意点</p>
</blockquote>
<ul>
<li>请求的协议要与代理ip的协议统一</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/30/Py007-01-06requests综合实战/" title="Py007-01-06requests综合实战" itemprop="url">Py007-01-06requests综合实战</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-30T13:08:18.000Z" itemprop="datePublished"> Published 2018-11-30</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="综合项目实战"><a href="#综合项目实战" class="headerlink" title="综合项目实战"></a>综合项目实战</h3><ul>
<li>需求：爬取搜狗知乎某一个词条对应一定范围页码表示的页面数据</li>
</ul>
<blockquote>
<p>分析网址</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 原始url</span><br><span class="line">baseUrl = &apos;https://zhihu.sogou.com/zhihu?query=%E9%92%89%E9%92%89&amp;ie=utf8&amp;w=&amp;oq=dd&amp;ri=1&amp;sourceid=sugg&amp;stj=1%3B0%3B0%3B0&amp;stj2=0&amp;stj0=1&amp;stj1=0&amp;hp=0&amp;hp1=&amp;sut=1909&amp;sst0=1543542920653&amp;lkt=3%2C1543542918642%2C1543542920549&apos;</span><br><span class="line"></span><br><span class="line"># 移除get参数的url</span><br><span class="line">url2 = &apos;https://zhihu.sogou.com/zhihu&apos;</span><br><span class="line"></span><br><span class="line"># get 参数 = &apos;query=%E9%92%89%E9%92%89&amp;ie=utf8&amp;w=&amp;oq=dd&amp;ri=1&amp;sourceid=sugg&amp;stj=1%3B0%3B0%3B0&amp;stj2=0&amp;stj0=1&amp;stj1=0&amp;hp=0&amp;hp1=&amp;sut=1909&amp;sst0=1543542920653&amp;lkt=3%2C1543542918642%2C1543542920549&apos;</span><br><span class="line">params = &#123;</span><br><span class="line">    query: 钉钉</span><br><span class="line">    ie: utf8</span><br><span class="line">    w: </span><br><span class="line">    oq: dd</span><br><span class="line">    ri: 1</span><br><span class="line">    sourceid: sugg</span><br><span class="line">    stj: 1;0;0;0</span><br><span class="line">    stj2: 0</span><br><span class="line">    stj0: 1</span><br><span class="line">    stj1: 0</span><br><span class="line">    hp: 0</span><br><span class="line">    hp1: </span><br><span class="line">    sut: 1909</span><br><span class="line">    sst0: 1543542920653</span><br><span class="line">    lkt: 3,1543542918642,1543542920549</span><br><span class="line">&#125;</span><br><span class="line"># 移除无用参数</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;query&apos;: 搜索词,</span><br><span class="line">    &apos;page&apos;: 页码,</span><br><span class="line">    &apos;ie&apos;: &apos;utf-8&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>代码实现</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># 前三页页面数据（1，2，3）</span><br><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 创建一个文件夹</span><br><span class="line">if not os.path.exists(&apos;./pages&apos;):</span><br><span class="line">    os.mkdir(&apos;./pages&apos;)</span><br><span class="line"></span><br><span class="line">word = input(&apos;enter a word:&apos;)</span><br><span class="line"></span><br><span class="line"># 动态指定页码的范围</span><br><span class="line">start_pageNum = int(input(&apos;enter a start pageNum:&apos;))</span><br><span class="line">end_pageNum = int(input(&apos;enter a end pageNum:&apos;))</span><br><span class="line"># 自定义请求头信息</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"># 1.指定url:设计成一个具有通用的url</span><br><span class="line"></span><br><span class="line"># 请求url</span><br><span class="line">url = &apos;https://zhihu.sogou.com/zhihu&apos;</span><br><span class="line">for page in range(start_pageNum, end_pageNum + 1):</span><br><span class="line">    param = &#123;</span><br><span class="line">        &apos;query&apos;: word,</span><br><span class="line">        &apos;page&apos;: page,</span><br><span class="line">        &apos;ie&apos;: &apos;utf-8&apos;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url=url, params=param, headers=headers)</span><br><span class="line"></span><br><span class="line">    # 获取响应中的页面数据（指定页码（page））</span><br><span class="line">    page_text = response.text</span><br><span class="line"></span><br><span class="line">    # 进行持久化存储</span><br><span class="line">    fileName = word + str(page) + &apos;.html&apos;</span><br><span class="line">    filePath = &apos;pages/&apos; + fileName</span><br><span class="line">    with open(filePath, &apos;w&apos;, encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line">        print(&apos;第%d页数据写入成功&apos; % page)</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/29/Py007-01-05requests模块之post和ajax/" title="Py007-01-05requests模块之post和ajax" itemprop="url">Py007-01-05requests模块之post和ajax</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-29T15:52:05.000Z" itemprop="datePublished"> Published 2018-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="requests模块之post请求"><a href="#requests模块之post请求" class="headerlink" title="requests模块之post请求"></a>requests模块之post请求</h3><blockquote>
<p>简单需求，登录豆瓣网，获取成功后的数据</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 1.post请求的url</span><br><span class="line">url = &apos;https://accounts.douban.com/login&apos;</span><br><span class="line"></span><br><span class="line"># 封装post请求参数数据</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;source&quot;:&apos;movie&apos;,</span><br><span class="line">    &apos;redir&apos;:&apos;https://movie.douban.com/&apos;,</span><br><span class="line">    &apos;form_email&apos;:&apos;15027900535&apos;,</span><br><span class="line">    &apos;form_password&apos;:&apos;bobo@15027900535&apos;,</span><br><span class="line">    &apos;login&apos;:&apos;登录&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"># 2 发起post请求</span><br><span class="line">response = requests.post(url=url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line"># 3 获取响应对象中的页面数据</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line"># 4 持久化存储</span><br><span class="line">with open(&apos;movie.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">    f.write(page_text)</span><br></pre></td></tr></table></figure>
<h3 id="ajax的请求"><a href="#ajax的请求" class="headerlink" title="ajax的请求"></a>ajax的请求</h3><h4 id="基于ajax的get请求"><a href="#基于ajax的get请求" class="headerlink" title="基于ajax的get请求"></a>基于ajax的get请求</h4><blockquote>
<p>抓取豆瓣电影首页-排行榜-某一分类的电影的下拉加载数据</p>
</blockquote>
<ul>
<li>排行榜 <a href="https://movie.douban.com/chart" target="_blank" rel="noopener">https://movie.douban.com/chart</a></li>
<li>喜剧分类 <a href="https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&amp;type=24&amp;interval_id=100:90&amp;action=" target="_blank" rel="noopener">https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&amp;type=24&amp;interval_id=100:90&amp;action=</a></li>
<li>下拉加载的动态数据接口 <a href="https://movie.douban.com/j/chart/top_list?type=24&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20" target="_blank" rel="noopener">https://movie.douban.com/j/chart/top_list?type=24&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 原始url</span><br><span class="line"># baseurl = &apos;https://movie.douban.com/j/chart/top_list?type=24&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20&apos;</span><br><span class="line"></span><br><span class="line"># 指定url</span><br><span class="line">url = &apos;https://movie.douban.com/j/chart/top_list&apos;</span><br><span class="line"></span><br><span class="line"># 封装ajax中  get参数</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;type&apos;:24,</span><br><span class="line">    &apos;interval_id&apos;:&apos;100:90&apos;,</span><br><span class="line">    &apos;action&apos;:&apos;&apos;,</span><br><span class="line">    &apos;start&apos;:40,</span><br><span class="line">    &apos;limit&apos;:20</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params,headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<h4 id="基于ajax的post请求"><a href="#基于ajax的post请求" class="headerlink" title="基于ajax的post请求"></a>基于ajax的post请求</h4><p>获取肯德基城市餐厅位置数据</p>
<ul>
<li>肯德基官网 <a href="http://www.kfc.com.cn/kfccda/index.aspx" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/index.aspx</a></li>
<li>餐厅位置查询页面 <a href="http://www.kfc.com.cn/kfccda/storelist/index.aspx" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/storelist/index.aspx</a></li>
<li>搜索查询 北京 餐厅列表  看浏览器里的network里的抓包信息 获得url = <a href="http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword</a></li>
<li>餐厅列表有分页但是是  post的 继续分析他的请求参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword</span><br><span class="line"></span><br><span class="line">参数</span><br><span class="line">cname: </span><br><span class="line">pid: </span><br><span class="line">keyword: 上海</span><br><span class="line">pageIndex: 2</span><br><span class="line">pageSize: 10</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 封装ajax中  post参数</span><br><span class="line">data = &#123;</span><br><span class="line">    &apos;cname&apos;:&apos;&apos;,</span><br><span class="line">    &apos;pid&apos;:&apos;&apos;,</span><br><span class="line">    &apos;keyword&apos;: &apos;上海&apos;,</span><br><span class="line">    &apos;pageIndex&apos;:2,</span><br><span class="line">    &apos;pageSize&apos;:10</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url=url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结论</p>
</blockquote>
<ul>
<li>如果使用requests发起ajax请求 跟普通get/post请求是一样的</li>
<li>无法通过地址栏请求，通过浏览器开发者工具抓包信息获取请求的url</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/29/Py007-01-04requests模块/" title="Py007-01-04requests模块" itemprop="url">Py007-01-04requests模块</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-29T15:09:15.000Z" itemprop="datePublished"> Published 2018-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="requests模块"><a href="#requests模块" class="headerlink" title="requests模块"></a>requests模块</h3><ul>
<li>什么是requests模块</li>
</ul>
<p>requests模块是python中原生的基于网络请求的模块，其主要作用是用来模拟浏览器发起请求。功能强大，用法简洁高效。在爬虫领域中占据着半壁江山的地位。</p>
<ul>
<li>为什么要使用requests模块</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt; 因为在使用urllib模块的时候，会有诸多不便之处，总结如下：</span><br><span class="line">    1 手动处理url编码</span><br><span class="line">    2 手动处理post请求参数</span><br><span class="line">    3 处理cookie和代理操作繁琐</span><br><span class="line">        &gt; cookie</span><br><span class="line">        - 创建一个cookiejar对象</span><br><span class="line">        - 创建一个handler对象</span><br><span class="line">        - 创建一个opener对象</span><br><span class="line"></span><br><span class="line">        &gt; 代理</span><br><span class="line">        - 创建handler对象，代理ip和端口封装到该对象</span><br><span class="line">        - 创建opener对象</span><br><span class="line"></span><br><span class="line">&gt; 使用requests模块：</span><br><span class="line">    - 自动处理url编码</span><br><span class="line">    - 自动处理post请求参数</span><br><span class="line">    - 大大简化cookie和代理操作</span><br><span class="line">    。。。</span><br></pre></td></tr></table></figure>
<h4 id="requests如何使用"><a href="#requests如何使用" class="headerlink" title="requests如何使用"></a>requests如何使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 安装</span><br><span class="line">pip install requests</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用流程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 指定url</span><br><span class="line">2. 使用requests模块发请求</span><br><span class="line">3. 获取响应数据</span><br><span class="line">4. 进行持久化存储</span><br></pre></td></tr></table></figure>
<h4 id="通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固"><a href="#通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固" class="headerlink" title="通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固"></a>通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固</h4><ul>
<li>get</li>
<li>post</li>
<li>ajax的get</li>
<li>ajax的post</li>
<li>综合</li>
</ul>
<h4 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 指定url</span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line"># 发get请求,get方法会返回请求成功的响应内容</span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"></span><br><span class="line"># 获取响应中的数据值：text可以获取响应对象中字符串形式的页面数据</span><br><span class="line">page_data = response.text</span><br><span class="line"></span><br><span class="line">print(page_data)</span><br><span class="line"></span><br><span class="line"># 持久化存储</span><br><span class="line">with open(&apos;./sogou.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(page_data)</span><br></pre></td></tr></table></figure>
<h4 id="requests其他常用属性"><a href="#requests其他常用属性" class="headerlink" title="requests其他常用属性"></a>requests其他常用属性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"></span><br><span class="line">page_data = response.text</span><br><span class="line"># 获取响应对象中二进制(byte)类型的数据</span><br><span class="line">print(response.content)</span><br><span class="line"></span><br><span class="line"># 返回响应的状态码 </span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"># 获取响应的头信息(字典的形式)</span><br><span class="line">print(response.headers)</span><br><span class="line"></span><br><span class="line"># 获取请求的url</span><br><span class="line">print(response.url)</span><br></pre></td></tr></table></figure>
<h4 id="request携带参数的get请求方式一"><a href="#request携带参数的get请求方式一" class="headerlink" title="request携带参数的get请求方式一"></a>request携带参数的get请求方式一</h4><blockquote>
<p>获取搜狗搜索结果对应的界面数据爬取 </p>
</blockquote>
<p>获取周杰伦对应的搜狗页面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.sogou.com/web?query=周杰伦&amp;ie=utf-8&apos;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"></span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">with open(&apos;./zhou.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">    f.write(page_text)</span><br></pre></td></tr></table></figure>
<p>此时url里的中文参数无需手动处理(urllib需要手动处理)</p>
<h4 id="request携带参数的get请求方式二"><a href="#request携带参数的get请求方式二" class="headerlink" title="request携带参数的get请求方式二"></a>request携带参数的get请求方式二</h4><ul>
<li>params参数(字典的形势传递get参数)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># get请求方式二</span><br><span class="line">url = &apos;https://www.sogou.com/web&apos;</span><br><span class="line"></span><br><span class="line"># 将参数封装到字典里</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;,</span><br><span class="line">    &apos;ie&apos;:&apos;utf-8&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params)</span><br><span class="line"></span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">print(page_text)</span><br></pre></td></tr></table></figure>
<h4 id="requests之自定义请求头信息"><a href="#requests之自定义请求头信息" class="headerlink" title="requests之自定义请求头信息"></a>requests之自定义请求头信息</h4><ul>
<li>headers 请求头参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 将参数封装到字典里</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;,</span><br><span class="line">    &apos;ie&apos;:&apos;utf-8&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params,headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/29/Py007-01-03内置urllib模块补充/" title="Py007-01-03内置urllib模块补充" itemprop="url">Py007-01-03内置urllib模块补充</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-29T14:59:14.000Z" itemprop="datePublished"> Published 2018-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="urllib补充"><a href="#urllib补充" class="headerlink" title="urllib补充"></a>urllib补充</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 补充说明：</span><br><span class="line">urlopen函数原型：</span><br><span class="line">    urllib.request.urlopen(url, data=None, timeout=&lt;object object at 0x10af327d0&gt;, *, cafile=None, capath=None, cadefault=False, context=None)</span><br><span class="line"></span><br><span class="line">在上一节中我们只使用了该函数中的第一个参数url。在日常开发中，我们能用的只有url和data这两个参数。</span><br><span class="line"></span><br><span class="line">url参数：指定向哪个url发起请求</span><br><span class="line">data参数：可以将post请求中携带的参数封装成字典的形式传递给该参数</span><br><span class="line"></span><br><span class="line">urlopen函数返回的响应对象，相关函数调用介绍：</span><br><span class="line">response.headers()：获取响应头信息</span><br><span class="line">response.getcode()：获取响应状态码</span><br><span class="line">response.geturl()：获取请求的url</span><br><span class="line">response.read()：获取响应中的数据值（字节类型）</span><br></pre></td></tr></table></figure>
<h4 id="二进制数据的爬取"><a href="#二进制数据的爬取" class="headerlink" title="二进制数据的爬取"></a>二进制数据的爬取</h4><blockquote>
<p>爬取网络上的某张图片数据，且存储到磁盘</p>
</blockquote>
<ul>
<li>方法 1：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://pic.qiushibaike.com/system/pictures/12112/121121212/medium/ZOAND29U4NKNEWEF.jpg&apos;</span><br><span class="line">#2.发起请求:使用urlopen函数发起请求，该函数返回一个响应对象</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">#3.获取响应对象中的图片二进制类型的数据</span><br><span class="line">img_data = response.read()</span><br><span class="line">#4.持久化存储：将爬取的图片写入本地进行保存</span><br><span class="line">with open(&apos;./tupian.png&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(img_data)</span><br></pre></td></tr></table></figure>
<ul>
<li>方法 2：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">url = &apos;https://pic.qiushibaike.com/system/pictures/12112/121121212/medium/ZOAND29U4NKNEWEF.jpg&apos;</span><br><span class="line"># 函数原型：urllib.request.urlretrieve(url, filename=None)</span><br><span class="line"># 作用：对url发起请求，且将响应中的数据值写入磁盘进行存储</span><br><span class="line">urllib.request.urlretrieve(url=url,filename=&apos;./img.png&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="url的特性"><a href="#url的特性" class="headerlink" title="url的特性"></a>url的特性</h4><p>url必须为ASCII编码的数据值。所以我们在爬虫代码中编写url时，如果url中存在非ASCII编码的数据值，则必须对其进行ASCII编码后，该url方可被使用。</p>
<blockquote>
<p>案例：爬取使用搜狗根据指定词条搜索到的页面数据（例如爬取词条为‘周杰伦’的页面数据）</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?query=周杰伦&apos;</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<p>【注意】上述代码中url存在非ascii编码的数据，则该url无效。如果对其发起请求，则会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 15-17: ordinal not in range</span><br></pre></td></tr></table></figure>
<blockquote>
<p>所以必须对url中的非ascii的数据进行ascii的编码，则该url方可被发起请求：</p>
</blockquote>
<ul>
<li>方法 1：使用quote函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?query=%s&apos;</span><br><span class="line">#对url中的非ascii进行编码.quote函数可以对非ascii的数据值进行ascii的编码</span><br><span class="line">word = urllib.parse.quote(&apos;周杰伦&apos;)</span><br><span class="line">#将编码后的数据值拼接回url中</span><br><span class="line">url = format(url%word)</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<ul>
<li>方法2： 使用urlencode函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?&apos;</span><br><span class="line">#将get请求中url携带的参数封装至字典中</span><br><span class="line">param = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;</span><br><span class="line">&#125;</span><br><span class="line">#对url中的非ascii进行编码</span><br><span class="line">param = urllib.parse.urlencode(param)</span><br><span class="line">#将编码后的数据值拼接回url中</span><br><span class="line">url += param </span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦1.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="通过自定义请求对象，用于伪装爬虫程序请求的身份。"><a href="#通过自定义请求对象，用于伪装爬虫程序请求的身份。" class="headerlink" title="通过自定义请求对象，用于伪装爬虫程序请求的身份。"></a>通过自定义请求对象，用于伪装爬虫程序请求的身份。</h4><p>之前在讲解http常用请求头信息时，我们讲解过User-Agent参数，简称为UA，该参数的作用是用于表明本次请求载体的身份标识。如果我们通过浏览器发起的请求，则该请求的载体为当前浏览器，则UA参数的值表明的是当前浏览器的身份标识表示的一串数据。如果我们使用爬虫程序发起的一个请求，则该请求的载体为爬虫程序，那么该请求的UA为爬虫程序的身份标识表示的一串数据。有些网站会通过辨别请求的UA来判别该请求的载体是否为爬虫程序，如果为爬虫程序，则不会给该请求返回响应，那么我们的爬虫程序则也无法通过请求爬取到该网站中的数据值，这也是反爬虫的一种初级技术手段。那么为了防止该问题的出现，则我们可以给爬虫程序的UA进行伪装，伪装成某款浏览器的身份标识。</p>
<p>上述案例中，我们是通过request模块中的urlopen发起的请求，该请求对象为urllib中内置的默认请求对象，我们无法对其进行UA进行更改操作。urllib还为我们提供了一种自定义请求对象的方式，我们可以通过自定义请求对象的方式，给该请求对象中的UA进行伪装（更改）操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?&apos;</span><br><span class="line">#将get请求中url携带的参数封装至字典中</span><br><span class="line">param = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;</span><br><span class="line">&#125;</span><br><span class="line">#对url中的非ascii进行编码</span><br><span class="line">param = urllib.parse.urlencode(param)</span><br><span class="line">#将编码后的数据值拼接回url中</span><br><span class="line">url += param </span><br><span class="line"></span><br><span class="line">#封装自定义的请求头信息的字典：</span><br><span class="line">#将浏览器的UA数据获取，封装到一个字典中。该UA值可以通过抓包工具或者浏览器自带的开发者工具中获取某请求，从中获取UA的值</span><br><span class="line">#注意：在headers字典中可以封装任意的请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos; : &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos;</span><br><span class="line">    &#125;</span><br><span class="line">#自定义请求对象，可以在该请求对象中添加自定义的请求头信息</span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line">#使用自定义请求对象发起请求</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="携带参数的post请求"><a href="#携带参数的post请求" class="headerlink" title="携带参数的post请求"></a>携带参数的post请求</h4><blockquote>
<p>案例：百度翻译发起post请求</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">#通过抓包工具抓取post请求的url</span><br><span class="line">post_url=&apos;https://fanyi.baidu.com/sug&apos;</span><br><span class="line">#封装post请求参数</span><br><span class="line">data=&#123;</span><br><span class="line">    &quot;kw&quot;:&quot;dog&quot;</span><br><span class="line">&#125;</span><br><span class="line">data=urllib.parse.urlencode(data)</span><br><span class="line">#自定义请求头信息字典</span><br><span class="line">headers=&#123;</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#自定义请求对象，然后将封装好的post请求参数赋值给Requst方法的data参数。</span><br><span class="line">#data参数：用来存储post请求的参数</span><br><span class="line">request=urllib.request.Request(post_url,data=data.encode(),headers=headers)</span><br><span class="line">#自定义的请求对象中的参数（data必须为bytes类型）</span><br><span class="line">response=urllib.request.urlopen(request)</span><br><span class="line">response.read()</span><br></pre></td></tr></table></figure>
<h3 id="urllib模块的高级操作"><a href="#urllib模块的高级操作" class="headerlink" title="urllib模块的高级操作"></a>urllib模块的高级操作</h3><blockquote>
<ol>
<li>代理</li>
</ol>
</blockquote>
<ul>
<li><p>什么是代理：代理就是第三方代替本体处理相关事务。例如：生活中的代理：代购，中介，微商……</p>
</li>
<li><p>爬虫中为什么需要使用代理？</p>
<blockquote>
<p>一些网站会有相应的反爬虫措施，例如很多网站会检测某一段时间某个IP的访问次数，如果访问频率太快以至于看起来不像正常访客，它可能就会会禁止这个IP的访问。所以我们需要设置一些代理IP，每隔一段时间换一个代理IP，就算IP被禁止，依然可以换个IP继续爬取。</p>
</blockquote>
</li>
<li><p>代理的分类：</p>
</li>
</ul>
<p>正向代理：代理客户端获取数据。正向代理是为了保护客户端防止被追究责任。</p>
<p>反向代理：代理服务器提供数据。反向代理是为了保护服务器或负责负载均衡。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#1.创建处理器对象，在其内部封装代理ip和端口</span><br><span class="line">handler=urllib.request.ProxyHandler(proxies=&#123;&apos;http&apos;:&apos;95.172.58.224:52608&apos;&#125;)</span><br><span class="line">#2.创建opener对象，然后使用该对象发起一个请求</span><br><span class="line">opener=urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">url=&apos;http://www.baidu.com/s?ie=UTF-8&amp;wd=ip&apos;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url, headers=headers)</span><br><span class="line"></span><br><span class="line">#使用opener对象发起请求，该请求对应的ip即为我们设置的代理ip</span><br><span class="line">response = opener.open(request)</span><br><span class="line"></span><br><span class="line">with open(&apos;./daili.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(response.read())</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 2.cookie</p>
</blockquote>
<p>引言：有些时候，我们在使用爬虫程序去爬取一些用户相关信息的数据（爬取张三“人人网”个人主页数据）时，如果使用之前requests模块常规操作时，往往达不到我们想要的目的，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">#指定url</span><br><span class="line">url = &apos;http://www.renren.com/289676607/profile&apos;</span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">    &#125;</span><br><span class="line">#自定义请求对象</span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line">#发起请求</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">with open(&apos;./renren.html&apos;,&apos;w&apos;) as fp:</span><br><span class="line">    fp.write(response.read().decode())</span><br></pre></td></tr></table></figure>
<p>【注意】上述代码中，我们爬取到的是登录首页面，而不是张三的个人主页也面。why？首先我们来回顾下cookie的相关概念及作用</p>
<pre><code>- cookie概念：当用户通过浏览器首次访问一个域名时，访问的web服务器会给客户端发送数据，以保持web服务器与客户端之间的状态保持，这些数据就是cookie。

- cookie作用：我们在浏览器中，经常涉及到数据的交换，比如你登录邮箱，登录一个页面。我们经常会在此时设置30天内记住我，或者自动登录选项。那么它们是怎么记录信息的呢，答案就是今天的主角cookie了，Cookie是由HTTP服务器设置的，保存在浏览器中，但HTTP协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。就像我们去超市买东西，没有积分卡的情况下，我们买完东西之后，超市没有我们的任何消费信息，但我们办了积分卡之后，超市就有了我们的消费信息。cookie就像是积分卡，可以保存积分，商品就是我们的信息，超市的系统就像服务器后台，http协议就是交易的过程。 

- 经过cookie的相关介绍，其实你已经知道了为什么上述案例中爬取到的不是张三个人信息页，而是登录页面。那应该如何抓取到张三的个人信息页呢？
</code></pre><p>　　思路：</p>
<p>　　　　1.我们需要使用爬虫程序对人人网的登录时的请求进行一次抓取，获取请求中的cookie数据</p>
<p>　　　　2.在使用个人信息页的url进行请求时，该请求需要携带 1 中的cookie，只有携带了cookie后，服务器才可识别这次请求的用户信息，方可响应回指定的用户信息页数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cookiejar对象：</span><br><span class="line">    - 作用：自动保存请求中的cookie数据信息</span><br><span class="line">    - 注意：必须和handler和opener一起使用</span><br><span class="line">cookiejar使用流程：</span><br><span class="line">    - 创建一个cookiejar对象</span><br><span class="line">      import http.cookiejar</span><br><span class="line">      cj = http.cookiejar.CookieJar()</span><br><span class="line">    - 通过cookiejar创建一个handler</span><br><span class="line">      handler = urllib.request.HTTPCookieProcessor(cj)</span><br><span class="line">    - 根据handler创建一个opener</span><br><span class="line">      opener = urllib.request.build_opener(handler)</span><br><span class="line">    - 使用opener.open方法去发送请求，且将响应中的cookie存储到openner对象中，后续的请求如果使用openner发起，则请求中就会携带了cookie</span><br></pre></td></tr></table></figure>
<p>使用cookiejar实现爬取人人网个人主页页面数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#使用cookiejar实现人人网的登陆</span><br><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">import http.cookiejar</span><br><span class="line">cj = http.cookiejar.CookieJar() #请求中的cookie会自动存储到cj对象中</span><br><span class="line">#创建处理器对象(携带cookiejar对象的)</span><br><span class="line">handler=urllib.request.HTTPCookieProcessor(cj)</span><br><span class="line">#创建opener对象 （携带cookiejar对象）</span><br><span class="line">opener=urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">#要让cookiejar获取请求中的cookie数据值</span><br><span class="line">url=&apos;http://www.renren.com/ajaxLogin/login?1=1&amp;uniqueTimestamp=201873958471&apos;</span><br><span class="line">#自定义一个请求对象，让该对象作为opener的open函数中的参数</span><br><span class="line">data=&#123;</span><br><span class="line">    &quot;email&quot;:&quot;www.zhangbowudi@qq.com&quot;,</span><br><span class="line">    &quot;icode&quot;:&quot;&quot;,</span><br><span class="line">    &quot;origURL&quot;:&quot;http://www.renren.com/home&quot;,</span><br><span class="line">    &quot;domain&quot;:&quot;renren.com&quot;,</span><br><span class="line">    &quot;key_id&quot;:&quot;1&quot;,</span><br><span class="line">    &quot;captcha_type&quot;:&quot;web_login&quot;,</span><br><span class="line">    &quot;password&quot;:&quot;40dc65b82edd06d064b54a0fc6d202d8a58c4cb3d2942062f0f7dd128511fb9b&quot;,</span><br><span class="line">    &quot;rkey&quot;:&quot;41b44b0d062d3ca23119bc8b58983104&quot;,</span><br><span class="line">  </span><br><span class="line"> &apos;f&apos;:&quot;https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DpPKf2680yRLbbZMVdntJpyPGwrSk2BtpKlEaAuKFTsW%26wd%3D%26eqid%3Deee20f380002988c000000025b7cbb80&quot;</span><br><span class="line">&#125;</span><br><span class="line">data=urllib.parse.urlencode(data).encode()</span><br><span class="line">request=urllib.request.Request(url,data=data)</span><br><span class="line">opener.open(request)</span><br><span class="line"></span><br><span class="line">#获取当前用户的二级子页面</span><br><span class="line">s_url=&apos;http://www.renren.com/289676607/profile&apos;</span><br><span class="line">#该次请求中就携带了cookie</span><br><span class="line">resonse=opener.open(s_url)</span><br><span class="line"></span><br><span class="line">with open(&apos;./renren.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(resonse.read())</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/29/Py007-01-02内置urllib模块/" title="Py007-01-02内置urllib模块" itemprop="url">Py007-01-02内置urllib模块</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-29T14:58:37.000Z" itemprop="datePublished"> Published 2018-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <blockquote>
<p>urllib模块非爬虫的重点，核心为后面的request以及各种框架</p>
</blockquote>
<h3 id="内置urllib"><a href="#内置urllib" class="headerlink" title="内置urllib"></a>内置urllib</h3><p>概念：urllib是Python自带的一个用于爬虫的库，其主要作用就是可以通过代码模拟浏览器发送请求。其常被用到的子模块在Python3中的为urllib.request和urllib.parse，在Python2中是urllib和urllib2。</p>
<ul>
<li>作用：可以使用代码模拟浏览器发起请求。request  parse</li>
</ul>
<blockquote>
<p>使用流程：</p>
</blockquote>
<ol>
<li>指定url</li>
<li>发请求</li>
<li>获取页面数据</li>
<li>持久化存储</li>
</ol>
<h4 id="需求1爬取搜狗首页的页面数据"><a href="#需求1爬取搜狗首页的页面数据" class="headerlink" title="需求1爬取搜狗首页的页面数据"></a>需求1爬取搜狗首页的页面数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line">#2.发起请求:urlopen可以根据指定的url发起请求，切返回一个响应对象</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line"></span><br><span class="line">#3.获取页面数据:read函数返回的就是响应对象中存储的页面数据(byte)</span><br><span class="line">page_text = response.read()</span><br><span class="line"></span><br><span class="line">#4.持久化存储</span><br><span class="line">with open(&apos;./sougou.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    # fp.write(page_text)</span><br><span class="line">    # print(page_text) # 字节码</span><br><span class="line">    print(page_text.decode()) # 转化为字符串</span><br><span class="line">    fp.write(page_text.decode())#使用decode将page_text转成字符串类型</span><br><span class="line">    print(&apos;写入数据成功&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="需求2搜狗搜关键字"><a href="#需求2搜狗搜关键字" class="headerlink" title="需求2搜狗搜关键字"></a>需求2搜狗搜关键字</h4><blockquote>
<p>注意：</p>
</blockquote>
<ul>
<li>urllib模块的get参数如果有中文要自行转义，否则报错UnicodeEncodeError</li>
<li>使用urllib.parse.quote(“中文参数”) 转义中文</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 需求：爬取指定词条所对应的页面数据</span><br><span class="line"></span><br><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#指定url</span><br><span class="line">url = &apos;https://www.sogou.com/web?query=&apos;</span><br><span class="line">#url特性：url不可以存在非ASCII编码的字符数据</span><br><span class="line">word = urllib.parse.quote(&quot;人民币&quot;)</span><br><span class="line">url += word #有效的url</span><br><span class="line"></span><br><span class="line">#发请求</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line"></span><br><span class="line">#获取页面数据</span><br><span class="line">page_text = response.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with open(&apos;renminbi.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(page_text)</span><br><span class="line">    </span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">如果</span><br><span class="line">word=&apos;人民币&apos;</span><br><span class="line">不经过转义则直接报错</span><br><span class="line">UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 15-17: ordinal not in range(128)</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<p>是不是很麻烦如果多个参数就要多次处理？</p>
<h4 id="使用UA来伪装"><a href="#使用UA来伪装" class="headerlink" title="使用UA来伪装"></a>使用UA来伪装</h4><ul>
<li>反爬机制：网站检查请求的UA，如果发现UA是爬虫程序，则拒绝提供网站数据。</li>
<li>User-Agent(UA)：请求载体的身份标识.</li>
<li>反反爬机制：伪装爬虫程序请求的UA</li>
</ul>
<blockquote>
<p>爬去百度首页</p>
</blockquote>
<ul>
<li>如果不用UA伪装，爬去的页面是不对的(百度已经做了反爬处理)</li>
<li>获取UA打开浏览器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.开发者工具</span><br><span class="line">2.检查</span><br><span class="line">3.Network</span><br><span class="line">4.访问百度</span><br><span class="line">5.找到Request Headers的请求信息里的User-Agent字符串</span><br><span class="line">形如</span><br><span class="line">Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.baidu.com/&apos;</span><br><span class="line"></span><br><span class="line">#UA伪装</span><br><span class="line">#1.自定制一个请求对象</span><br><span class="line">headers = &#123;</span><br><span class="line">    #存储任意的请求头信息</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36&apos;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line">#该请求对象的UA进行了成功的伪装</span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">#2.针对自定制的请求对象发起请求</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>
<h4 id="使用urllib发起post请求"><a href="#使用urllib发起post请求" class="headerlink" title="使用urllib发起post请求"></a>使用urllib发起post请求</h4><blockquote>
<p>需求：爬取百度翻译的翻译结果</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://fanyi.baidu.com/sug&apos;</span><br><span class="line"></span><br><span class="line">#post请求携带的参数进行处理  流程：</span><br><span class="line">#1.将post请求参数封装到字典</span><br><span class="line">data = &#123;</span><br><span class="line">    &apos;kw&apos;:&apos;西瓜&apos;</span><br><span class="line">&#125;</span><br><span class="line">#2.使用parse模块中的urlencode(返回值类型为str)进行编码处理</span><br><span class="line">data = urllib.parse.urlencode(data)</span><br><span class="line">#3.将步骤2的编码结果转换成byte类型</span><br><span class="line">data = data.encode()</span><br><span class="line"></span><br><span class="line">#2.发起post请求:urlopen函数的data参数表示的就是经过处理之后的post请求携带的参数</span><br><span class="line">response = urllib.request.urlopen(url=url,data=data)</span><br><span class="line"></span><br><span class="line">response.read()</span><br><span class="line">res = response.read()</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/29/Py007-01-01爬虫初识/" title="Py007-01-01爬虫初识" itemprop="url">Py007-01-01爬虫初识</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-29T14:57:17.000Z" itemprop="datePublished"> Published 2018-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="python网络爬虫的简单介绍"><a href="#python网络爬虫的简单介绍" class="headerlink" title="python网络爬虫的简单介绍"></a>python网络爬虫的简单介绍</h3><blockquote>
<p>什么是爬虫</p>
</blockquote>
<p>爬虫就是通过编写程序模拟浏览器上网，然后让其去互联网上抓取数据的过程。</p>
<blockquote>
<p>哪些语言可以实现爬虫</p>
</blockquote>
<ol>
<li><p>php：可以实现爬虫。php被号称是全世界最优美的语言（当然是其自己号称的，就是王婆卖瓜的意思），但是php在实现爬虫中支持多线程和多进程方面做的不好。</p>
</li>
<li><p>java：可以实现爬虫。java可以非常好的处理和实现爬虫，是唯一可以与python并驾齐驱且是python的头号劲敌。但是java实现爬虫代码较为臃肿，重构成本较大。</p>
</li>
<li><p>c、c++：可以实现爬虫。但是使用这种方式实现爬虫纯粹是是某些人（大佬们）能力的体现，却不是明智和合理的选择。</p>
</li>
<li><p>python：可以实现爬虫。python实现和处理爬虫语法简单，代码优美，支持的模块繁多，学习成本低，具有非常强大的框架（scrapy等）且一句难以言表的好！没有但是！</p>
</li>
</ol>
<h4 id="爬虫的分类"><a href="#爬虫的分类" class="headerlink" title="爬虫的分类"></a>爬虫的分类</h4><blockquote>
<h4 id="1-通用爬虫：通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。-简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理-提取关键字、去掉广告-，最后提供一个用户检索接口。"><a href="#1-通用爬虫：通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。-简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理-提取关键字、去掉广告-，最后提供一个用户检索接口。" class="headerlink" title="1. 通用爬虫：通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。  简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理(提取关键字、去掉广告)，最后提供一个用户检索接口。"></a>1. 通用爬虫：通用爬虫是搜索引擎（Baidu、Google、Yahoo等）“抓取系统”的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。  简单来讲就是尽可能的；把互联网上的所有的网页下载下来，放到本地服务器里形成备分，在对这些网页做相关处理(提取关键字、去掉广告)，最后提供一个用户检索接口。</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">搜索引擎如何抓取互联网上的网站数据？</span><br><span class="line">门户网站主动向搜索引擎公司提供其网站的url</span><br><span class="line">搜索引擎公司与DNS服务商合作，获取网站的url</span><br><span class="line">门户网站主动挂靠在一些知名网站的友情链接中</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="2-聚焦爬虫：聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。"><a href="#2-聚焦爬虫：聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。" class="headerlink" title="2.聚焦爬虫：聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。"></a>2.聚焦爬虫：聚焦爬虫是根据指定的需求抓取网络上指定的数据。例如：获取豆瓣上电影的名称和影评，而不是获取整张页面中所有的数据值。</h4></blockquote>
<h4 id="robots-txt协议"><a href="#robots-txt协议" class="headerlink" title="robots.txt协议"></a>robots.txt协议</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果自己的门户网站中的指定页面中的数据不想让爬虫程序爬取到的话，那么则可以通过编写一个robots.txt的协议文件来约束爬虫程序的数据爬取。robots协议的编写格式可以观察淘宝网的robots（访问www.taobao.com/robots.txt即可）。但是需要注意的是，该协议只是相当于口头的协议，并没有使用相关技术进行强制管制，所以该协议是防君子不防小人。但是我们在学习爬虫阶段编写的爬虫程序可以先忽略robots协议。</span><br></pre></td></tr></table></figure>
<h4 id="反爬虫"><a href="#反爬虫" class="headerlink" title="反爬虫"></a>反爬虫</h4><ul>
<li>门户网站通过相应的策略和技术手段，防止爬虫程序进行网站数据的爬取。</li>
</ul>
<h4 id="反反爬虫"><a href="#反反爬虫" class="headerlink" title="反反爬虫"></a>反反爬虫</h4><ul>
<li>爬虫程序通过相应的策略和技术手段，破解门户网站的反爬虫手段，从而爬取到相应的数据。</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/27/Py008-01-05选择排序和插入排序/" title="Py008-01-05选择排序和插入排序" itemprop="url">Py008-01-05选择排序和插入排序</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-27T13:21:51.000Z" itemprop="datePublished"> Published 2018-11-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="选择排序-体育老师一指禅法"><a href="#选择排序-体育老师一指禅法" class="headerlink" title="选择排序(体育老师一指禅法)"></a>选择排序(体育老师一指禅法)</h3><p>思路：</p>
<p>在一个列表里每次把最小的值取出来。</p>
<blockquote>
<p>简易版本(不建议用)</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">li = [3,2,4,6,5,1,8,7,9]</span><br><span class="line"></span><br><span class="line">def select_sort1(li):</span><br><span class="line">    list_new = []</span><br><span class="line">    for i in range(len(li)):</span><br><span class="line">        min_val = min(li)</span><br><span class="line">        list_new.append(min_val)</span><br><span class="line">        li.remove(min_val)</span><br><span class="line"></span><br><span class="line">    return list_new</span><br><span class="line"></span><br><span class="line">res = select_sort1(li)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">我们知道  冒泡排序是交换彼此直接的位置，属于原地排序</span><br><span class="line"></span><br><span class="line">而这个版本的选择排序  </span><br><span class="line">每次都对原始数据进行删除  假如是长度10000的集合  如果满足条件的值在中间5000的位置  那么后边的要依次  索引-1</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>优化版</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def select_sort2(li):</span><br><span class="line">    for i in range(len(li)-1): # i是第几趟</span><br><span class="line">        min_loc = i</span><br><span class="line">        for j in range(i+1,len(li)):</span><br><span class="line">            if li[j]&lt;li[min_loc]:</span><br><span class="line">                min_loc = j</span><br><span class="line">        li[i],li[min_loc] = li[min_loc],li[i]</span><br><span class="line">        print(li)</span><br><span class="line"></span><br><span class="line">res2 = select_sort2(li)</span><br><span class="line">print(res2)</span><br></pre></td></tr></table></figure>
<ul>
<li>一趟排序记录最小的数，放到第一个</li>
<li>再一趟排序记录 列表无序区最小的数，放到第二个位置</li>
<li>。。。。</li>
<li>关键点：有序区 和 无序区，无序区最小值的位置</li>
</ul>
<blockquote>
<h4 id="时间复杂度：O-n-n"><a href="#时间复杂度：O-n-n" class="headerlink" title="时间复杂度：O(n*n)"></a>时间复杂度：O(n*n)</h4></blockquote>
<h3 id="插入排序-起扑克牌法"><a href="#插入排序-起扑克牌法" class="headerlink" title="插入排序(起扑克牌法)"></a>插入排序(起扑克牌法)</h3><p>你玩过扑克吗？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">你是如何摸牌的？</span><br><span class="line">比如 arr = [5,1,3,2,4]</span><br><span class="line"></span><br><span class="line">()内为你摸到的牌</span><br><span class="line"></span><br><span class="line">第一次摸到5 </span><br><span class="line">(5) [1,3,2,4]</span><br><span class="line">第二次摸到1   1与5比=&gt;小 交换位置</span><br><span class="line">(1,5) [3,2,4]</span><br><span class="line">第三次摸到3   3与5比=&gt;小 交换位置   3与1比=&gt;大 不交换</span><br><span class="line">(1,3,5) [2,4]</span><br><span class="line">第四次摸到2   2与5比=&gt;小 交换位置   2与3比=&gt;小 交换位置   2与1比=&gt;大 不交换</span><br><span class="line">(1,2,3,5) [4]</span><br><span class="line">第五次摸到4  4与5比=&gt;小 交换位置   4与3比=&gt;大  不交换</span><br></pre></td></tr></table></figure>
<p>插入排序代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def insert_sort(li):</span><br><span class="line">    for i in range(1,len(li)): # 表示摸到的牌的下标</span><br><span class="line">        tmp = li[i]</span><br><span class="line">        j = i-1 # 手里牌的下标</span><br><span class="line">        while j&gt;=0 and li[j]&gt;tmp:</span><br><span class="line">            li[j+1] = li[j]</span><br><span class="line">            j -= 1</span><br><span class="line">        li[j+1] = tmp</span><br><span class="line">        print(li)</span><br><span class="line"></span><br><span class="line">li = [3,2,4,6,5,1,8,7,9]</span><br><span class="line"></span><br><span class="line">insert_sort(li)</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="时间复杂度：-O-n-n"><a href="#时间复杂度：-O-n-n" class="headerlink" title="时间复杂度： O(n*n)"></a>时间复杂度： O(n*n)</h4></blockquote>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M08/">M08</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/26/Py008-01-04排序/" title="Py008-01-04排序" itemprop="url">Py008-01-04排序</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-26T14:44:28.000Z" itemprop="datePublished"> Published 2018-11-26</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><ul>
<li>排序：将一组‘无序’的记录序列调整为‘有序’的记录列表</li>
<li><p>列表排序<br>  输入：列表<br>  输出：有序列表</p>
</li>
<li><p>生序和降序</p>
</li>
<li>内置排序函数：sort()</li>
</ul>
<h4 id="常见排序"><a href="#常见排序" class="headerlink" title="常见排序"></a>常见排序</h4><blockquote>
<p>low B 三人组</p>
</blockquote>
<ul>
<li>冒泡排序</li>
<li>选择排序</li>
<li>插入排序</li>
</ul>
<blockquote>
<p>NB 三人组</p>
</blockquote>
<ul>
<li>快速排序</li>
<li>堆排序</li>
<li>归并排序</li>
</ul>
<blockquote>
<p>其他排序</p>
</blockquote>
<ul>
<li>希尔</li>
<li>计数</li>
<li>基数</li>
</ul>
<h4 id="冒泡排序-Bubble-sort-体育委员两两比较法"><a href="#冒泡排序-Bubble-sort-体育委员两两比较法" class="headerlink" title="冒泡排序 (Bubble sort) 体育委员两两比较法"></a>冒泡排序 (Bubble sort) 体育委员两两比较法</h4><ul>
<li>列表每两个相邻的数，如果后面的比前面的大，则交换这两个数</li>
<li>一趟排序完成后，则无序区减少一个数，有序区增加一个数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">初始列表</span><br><span class="line">5 2 3 1 4</span><br><span class="line"></span><br><span class="line">第一趟 </span><br><span class="line">    5 2 3 1 4</span><br><span class="line">    第一次</span><br><span class="line">    2 5 3 1 4</span><br><span class="line">    第二次</span><br><span class="line">    2 3 5 1 4</span><br><span class="line">    第三次</span><br><span class="line">    2 3 1 5 4</span><br><span class="line">    第四次</span><br><span class="line">    2 3 1 4 5  此时最大的数5已经站定了</span><br><span class="line"></span><br><span class="line">第二趟</span><br><span class="line">    2 3 1 4</span><br><span class="line">    第一次</span><br><span class="line">    2 3 1 4 </span><br><span class="line">    第二次</span><br><span class="line">    2 1 3 4</span><br><span class="line">    第三次</span><br><span class="line">    2 1 3 4  此时最大的数4已经站定了</span><br><span class="line"></span><br><span class="line">第三趟</span><br><span class="line">    2 1 3</span><br><span class="line">    第一次</span><br><span class="line">    1 2 3</span><br><span class="line">    第二次</span><br><span class="line">    1 2 3    此时最大的数3已经站定了</span><br><span class="line"></span><br><span class="line">第四趟</span><br><span class="line">    1 2</span><br><span class="line">    第一次</span><br><span class="line">    1 2      此时最大的数2已经站定了</span><br><span class="line"></span><br><span class="line">最后 1 2 3 4 5</span><br></pre></td></tr></table></figure>
<p>冒泡排序代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def bubble_sort(li):</span><br><span class="line">    for i in range(len(li)-1): # 第几趟</span><br><span class="line">        for j in range(len(li) - i - 1):</span><br><span class="line">            if li[j] &gt; li[j+1]:# 大就交换</span><br><span class="line">                li[j],li[j+1] = li[j+1],li[j]</span><br><span class="line">        print(li)</span><br><span class="line"></span><br><span class="line">li = [3,2,4,6,5,1,8,7,9]</span><br><span class="line"></span><br><span class="line">bubble_sort(li)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">结果如下</span><br><span class="line">[2, 3, 4, 5, 1, 6, 7, 8, 9]</span><br><span class="line">[2, 3, 4, 1, 5, 6, 7, 8, 9]</span><br><span class="line">[2, 3, 1, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[2, 1, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>时间复杂度：O(n*n)</p>
</blockquote>
<blockquote>
<h4 id="冒泡排序的优化"><a href="#冒泡排序的优化" class="headerlink" title="冒泡排序的优化"></a>冒泡排序的优化</h4></blockquote>
<p>如果一趟列表中没有发生交换则代表已经排好序了  则后面无需进行那么多次的比较了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def bubble_sort2(li):</span><br><span class="line">    for i in range(len(li)-1): # 第几趟</span><br><span class="line">        exchange = False</span><br><span class="line">        for j in range(len(li) - i - 1):</span><br><span class="line">            if li[j] &gt; li[j+1]:# 大就交换</span><br><span class="line">                li[j],li[j+1] = li[j+1],li[j]</span><br><span class="line">                exchange = True</span><br><span class="line">        print(li)</span><br><span class="line">        if not exchange:</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">li = [3,2,4,6,5,1,8,7,9]</span><br><span class="line"></span><br><span class="line">bubble_sort2(li)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[2, 3, 4, 5, 1, 6, 7, 8, 9]</span><br><span class="line">[2, 3, 4, 1, 5, 6, 7, 8, 9]</span><br><span class="line">[2, 3, 1, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[2, 1, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">[1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M08/">M08</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/24/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><a class="page-number" href="/page/27/">27</a><span class="space">&hellip;</span><a class="page-number" href="/page/49/">49</a><a class="extend next" rel="next" href="/page/26/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  


  

  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/oak/" title="oak">oak<sup>71</sup></a></li>
			
		
			
				<li><a href="/tags/前端知识点/" title="前端知识点">前端知识点<sup>43</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>37</sup></a></li>
			
		
			
				<li><a href="/tags/Node后端/" title="Node后端">Node后端<sup>34</sup></a></li>
			
		
			
				<li><a href="/tags/M06/" title="M06">M06<sup>29</sup></a></li>
			
		
			
				<li><a href="/tags/fullstack/" title="fullstack">fullstack<sup>27</sup></a></li>
			
		
			
				<li><a href="/tags/M07/" title="M07">M07<sup>27</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>26</sup></a></li>
			
		
			
				<li><a href="/tags/M08/" title="M08">M08<sup>25</sup></a></li>
			
		
			
				<li><a href="/tags/M04/" title="M04">M04<sup>22</sup></a></li>
			
		
			
				<li><a href="/tags/M03/" title="M03">M03<sup>20</sup></a></li>
			
		
			
				<li><a href="/tags/M02/" title="M02">M02<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/React入门/" title="React入门">React入门<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/ReactWheels/" title="ReactWheels">ReactWheels<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/TS入门/" title="TS入门">TS入门<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/M01/" title="M01">M01<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/vue/" title="vue">vue<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/ES6速学/" title="ES6速学">ES6速学<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/mongodb/" title="mongodb">mongodb<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>7</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">Weibo</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=&verifier=b3593ceb&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Larry Page in Google. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/2176287895" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="Stevin">Stevin</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>












<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
