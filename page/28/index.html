<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Almost</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Almost">
<meta property="og:url" content="http://yoursite.com/page/28/index.html">
<meta property="og:site_name" content="Almost">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Almost">
  
    <link rel="alternate" href="/atom.xml" title="Almost" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Almost</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Py007-03-02权限系统介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/Py007-03-02权限系统介绍/" class="article-date">
  <time datetime="2018-12-04T15:07:43.000Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/04/Py007-03-02权限系统介绍/">Py007-03-02权限系统介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="权限"><a href="#权限" class="headerlink" title="权限"></a>权限</h3><ul>
<li><p>回顾电影 《2012》 那些能登上“诺亚方舟”的人都是有“权”有“势”的人，而不能登上船的人就只能等死。——说明他们 level不够</p>
</li>
<li><p>公司里员工的薪水——保密</p>
</li>
</ul>
<h4 id="为什么开发权限组件？"><a href="#为什么开发权限组件？" class="headerlink" title="为什么开发权限组件？"></a>为什么开发权限组件？</h4><blockquote>
<p>为什么那些在公司的老员工开发系统时，比你要快。</p>
</blockquote>
<ul>
<li>年限多，写的熟，经验多</li>
<li>老油条</li>
<li>最最重要的是——自己的组件(通用性的组件)</li>
</ul>
<h4 id="web程序中什么是权限"><a href="#web程序中什么是权限" class="headerlink" title="web程序中什么是权限"></a>web程序中什么是权限</h4><blockquote>
<h4 id="一个url"><a href="#一个url" class="headerlink" title="一个url"></a>一个url</h4></blockquote>
<blockquote>
<h4 id="一个url-1"><a href="#一个url-1" class="headerlink" title="一个url"></a>一个url</h4></blockquote>
<blockquote>
<h4 id="一个url-2"><a href="#一个url-2" class="headerlink" title="一个url"></a>一个url</h4></blockquote>
<blockquote>
<p>结论</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">url ==&gt; 权限</span><br><span class="line"></span><br><span class="line">人  ==&gt; url</span><br></pre></td></tr></table></figure>
<h3 id="第一版权限控制"><a href="#第一版权限控制" class="headerlink" title="第一版权限控制"></a>第一版权限控制</h3><ul>
<li>多对多关系</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">用户表 </span><br><span class="line">id  user</span><br><span class="line">1   马云 CEO</span><br><span class="line">2   CTO</span><br><span class="line">3   技术</span><br><span class="line">4   前台</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">权限表</span><br><span class="line">id  auth</span><br><span class="line">1   user/list/  查看用户列表</span><br><span class="line">2   user/add/   增加用户</span><br><span class="line">3   user/edit/  编辑用户</span><br><span class="line">4   user/del/   删除用户</span><br><span class="line">5   sale/list   商品销售列表</span><br><span class="line">6   sale/add</span><br><span class="line">7   sale/edit</span><br><span class="line">8   sale/del</span><br></pre></td></tr></table></figure>
<blockquote>
<p>一些虚拟条件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 用户权限关系表</span><br><span class="line">id uid aid</span><br><span class="line"></span><br><span class="line">CEO——1,2,3,4,5,6,7,8所有权限</span><br><span class="line">CTO——1,2,3,4,5,6</span><br><span class="line">技术——1,2,3,4</span><br><span class="line">前台——1,2</span><br></pre></td></tr></table></figure>
<blockquote>
<p>问题来了</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//假设又来了一个cto</span><br><span class="line">此时这个cto要新增  1，2，3，4，5，6的权限</span><br><span class="line"></span><br><span class="line">如果此时ceo宣布 cto权限加2个</span><br><span class="line">此时就是两个cto操作2*2 新增数据为4条</span><br><span class="line"></span><br><span class="line">过了一周，ceo说，cto权限删除3个</span><br><span class="line">此时就是两个cto操作2*3 删除数据为6条</span><br></pre></td></tr></table></figure>
<blockquote>
<p>此时问题就来了，每次新增用户和新增权限都会导致批量的处理权限</p>
</blockquote>
<ul>
<li>麻烦</li>
</ul>
<blockquote>
<h4 id="角色的概念来了"><a href="#角色的概念来了" class="headerlink" title="角色的概念来了"></a>角色的概念来了</h4></blockquote>
<ul>
<li>不再给人指定权限，而是针对角色</li>
<li>每个角色对应一个权限列表 一个或者多个</li>
<li>每个用户对应一个角色</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/04/Py007-03-02权限系统介绍/" data-id="cklhjp25n00kbulfy6yzyugnl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-03-01crm介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/04/Py007-03-01crm介绍/" class="article-date">
  <time datetime="2018-12-04T15:00:43.000Z" itemprop="datePublished">2018-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/04/Py007-03-01crm介绍/">Py007-03-01crm介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="CRM"><a href="#CRM" class="headerlink" title="CRM"></a>CRM</h3><p>你一定听说过crm</p>
<blockquote>
<p>但是这次要分为三个目标来讲述crm</p>
</blockquote>
<ul>
<li>权限</li>
<li>stark组件</li>
<li>CRM</li>
</ul>
<h4 id="权限组件"><a href="#权限组件" class="headerlink" title="权限组件"></a>权限组件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通用的权限组件</span><br></pre></td></tr></table></figure>
<h4 id="stark组件"><a href="#stark组件" class="headerlink" title="stark组件"></a>stark组件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 20分钟的实现</span><br><span class="line">通用的增删改查组件</span><br></pre></td></tr></table></figure>
<h4 id="CRM系统"><a href="#CRM系统" class="headerlink" title="CRM系统"></a>CRM系统</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">真实业务</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">权限+增删改查</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/04/Py007-03-01crm介绍/" data-id="cklhjp25m00kaulfyy28do5kq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-13分布式爬虫简介" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-13分布式爬虫简介/" class="article-date">
  <time datetime="2018-12-02T14:12:31.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-13分布式爬虫简介/">Py007-02-13分布式爬虫简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="分布式爬虫概念"><a href="#分布式爬虫概念" class="headerlink" title="分布式爬虫概念"></a>分布式爬虫概念</h3><ol>
<li>多台机器可以执行同一个爬虫程序，实现网站数据的分布爬取</li>
<li>原生的scrapy是不可以实现分布式爬虫的<ul>
<li>调度器无法在多台机器共享</li>
<li>管道无法共享</li>
</ul>
</li>
<li>scrapy-redis组件：专门为scrapy开发的一套组件。<ul>
<li>该组件可以让scrapy实现分布式</li>
</ul>
</li>
</ol>
<h3 id="scrapy-redis组件"><a href="#scrapy-redis组件" class="headerlink" title="scrapy-redis组件"></a>scrapy-redis组件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 下载</span><br><span class="line">pip install scrapy-redis</span><br></pre></td></tr></table></figure>
<h4 id="分布式爬取的流程"><a href="#分布式爬取的流程" class="headerlink" title="分布式爬取的流程"></a>分布式爬取的流程</h4><blockquote>
<h4 id="1-redis配置-事先安装好redis"><a href="#1-redis配置-事先安装好redis" class="headerlink" title="1. redis配置(事先安装好redis)"></a>1. redis配置(事先安装好redis)</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">我的是 5.0版本</span><br><span class="line">切换到 redis5.0.0 的文件目录</span><br><span class="line">找到redis.conf</span><br><span class="line"></span><br><span class="line">    - 注释掉 bind 127.0.0.1 ::1  ==&gt; # bind 127.0.0.1 ::1</span><br><span class="line">    - 关闭保护模式 protected-mode yes ==&gt; protected-mode no</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="2-redis服务器的开启-基于配置文件进行开启"><a href="#2-redis服务器的开启-基于配置文件进行开启" class="headerlink" title="2. redis服务器的开启 基于配置文件进行开启"></a>2. redis服务器的开启 基于配置文件进行开启</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-server  ../redis.conf</span><br></pre></td></tr></table></figure>
<p>再开一个客户端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-cli</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="3-创建scrapy工程"><a href="#3-创建scrapy工程" class="headerlink" title="3. 创建scrapy工程"></a>3. 创建scrapy工程</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject redisPro</span><br><span class="line"></span><br><span class="line">cd redisPro</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意： 此时要创建一个基于 crawlSpider 的应用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 我们现在就爬取  糗百 糗图的数据</span><br><span class="line">scrapy genspider -t crawl qiubai www.qiushibaike.com/pic</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="4-导入RedisCrawlSpider-类-，然后将爬虫文件修改成基于该类的源文件"><a href="#4-导入RedisCrawlSpider-类-，然后将爬虫文件修改成基于该类的源文件" class="headerlink" title="4. 导入RedisCrawlSpider 类 ，然后将爬虫文件修改成基于该类的源文件"></a>4. 导入RedisCrawlSpider 类 ，然后将爬虫文件修改成基于该类的源文件</h4></blockquote>
<p>spiders/qiubai.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line"># class QiubaiSpider(CrawlSpider):</span><br><span class="line"># 修改原先继承的 CrawlSpider 为 RedisCrawlSpider</span><br><span class="line">class QiubaiSpider(RedisCrawlSpider):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="5-将-start-urls-修改成-redis-key-‘调度器队列的名称’"><a href="#5-将-start-urls-修改成-redis-key-‘调度器队列的名称’" class="headerlink" title="5. 将 start_urls 修改成 redis_key = ‘调度器队列的名称’"></a>5. 将 start_urls 修改成 redis_key = ‘调度器队列的名称’</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">class QiubaiSpider(CrawlSpider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line"></span><br><span class="line">    # 此时要注释掉 allowed_domains 和 start_urls</span><br><span class="line">    # allowed_domains = [&apos;https://www.qiushibaike.com/pic&apos;]</span><br><span class="line">    # start_urls = [&apos;http://https://www.qiushibaike.com/pic/&apos;]</span><br><span class="line"></span><br><span class="line">    # redis_key 它代表调度器队列的名称</span><br><span class="line">    redis_key = &apos;qiubaispider&apos; # 该行代码和 start_urls 一样</span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=r&apos;Items/&apos;), callback=&apos;parse_item&apos;, follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        #i[&apos;domain_id&apos;] = response.xpath(&apos;//input[@id=&quot;sid&quot;]/@value&apos;).extract()</span><br><span class="line">        #i[&apos;name&apos;] = response.xpath(&apos;//div[@id=&quot;name&quot;]&apos;).extract()</span><br><span class="line">        #i[&apos;description&apos;] = response.xpath(&apos;//div[@id=&quot;description&quot;]&apos;).extract()</span><br><span class="line">        return i</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="6-将项目的管道和调度器配置成基于-scrapy-redis的组件"><a href="#6-将项目的管道和调度器配置成基于-scrapy-redis的组件" class="headerlink" title="6. 将项目的管道和调度器配置成基于 scrapy-redis的组件"></a>6. 将项目的管道和调度器配置成基于 scrapy-redis的组件</h4></blockquote>
<blockquote>
<p>修改爬虫的代码</p>
</blockquote>
<ul>
<li>处理链接提取器</li>
<li>处理规则处理器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">class QiubaiSpider(CrawlSpider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line"></span><br><span class="line">    # 此时要注释掉 allowed_domains 和 start_urls</span><br><span class="line">    # allowed_domains = [&apos;https://www.qiushibaike.com/pic&apos;]</span><br><span class="line">    # start_urls = [&apos;http://https://www.qiushibaike.com/pic/&apos;]</span><br><span class="line"></span><br><span class="line">    # redis_key 它代表调度器队列的名称</span><br><span class="line">    redis_key = &apos;qiubaispider&apos; # 该行代码和 start_urls 一样</span><br><span class="line"></span><br><span class="line">    # 糗百糗图  底部分页器</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    &lt;a href=&quot;/pic/page/2?s=5145873&quot; rel=&quot;nofollow&quot;&gt;</span><br><span class="line">    解析规则</span><br><span class="line">    /pic/page/2/?s=5145873</span><br><span class="line">    为啥有个s=5145873  刷新下页面后  发现 s会变化 所以可以把s参数忽略</span><br><span class="line">    /pic/page/\d+</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    link = LinkExtractor(allow=r&apos;/pic/page/\d+&apos;)</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(link, callback=&apos;parse_item&apos;, follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        div_list = response.xpath(&apos;//div[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            img_url = &apos;https:&apos;+ div.xpath(&apos;.//div[@class=&quot;thumb&quot;]/a/img/@src&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<blockquote>
<p>定义items  </p>
</blockquote>
<p>items.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class RedisproItem(scrapy.Item):</span><br><span class="line">    # define the fields for your item here like:</span><br><span class="line">    # name = scrapy.Field()</span><br><span class="line">    img_url = scrapy.Field()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>导入items类</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line">from scrapy_redis.spiders import RedisCrawlSpider</span><br><span class="line">from redisPro.items import RedisproItem</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(RedisCrawlSpider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line"></span><br><span class="line">    # 此时要注释掉 allowed_domains 和 start_urls</span><br><span class="line">    # allowed_domains = [&apos;https://www.qiushibaike.com/pic&apos;]</span><br><span class="line">    # start_urls = [&apos;http://https://www.qiushibaike.com/pic/&apos;]</span><br><span class="line"></span><br><span class="line">    # redis_key 它代表调度器队列的名称</span><br><span class="line">    redis_key = &apos;qiubaispider&apos; # 该行代码和 start_urls 一样</span><br><span class="line"></span><br><span class="line">    # 糗百糗图  底部分页器</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    &lt;a href=&quot;/pic/page/2?s=5145873&quot; rel=&quot;nofollow&quot;&gt;</span><br><span class="line">    解析规则</span><br><span class="line">    /pic/page/2/?s=5145873</span><br><span class="line">    为啥有个s=5145873  刷新下页面后  发现 s会变化 所以可以把s参数忽略</span><br><span class="line">    /pic/page/\d+</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    link = LinkExtractor(allow=r&apos;/pic/page/\d+&apos;)</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(link, callback=&apos;parse_item&apos;, follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        div_list = response.xpath(&apos;//div[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            img_url = &apos;https:&apos;+ div.xpath(&apos;.//div[@class=&quot;thumb&quot;]/a/img/@src&apos;).extract_first()</span><br><span class="line">            item = RedisproItem()</span><br><span class="line">            item[&apos;img_url&apos;] = img_url</span><br><span class="line"></span><br><span class="line">            # 现在是 scrapy的管道， 此时要使用 scrapy_redis 提供的管道 去settings.py 里修改配置</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="使用scrapy-redis-提供的管道-RedisPipeline"><a href="#使用scrapy-redis-提供的管道-RedisPipeline" class="headerlink" title="使用scrapy_redis 提供的管道 RedisPipeline"></a>使用scrapy_redis 提供的管道 RedisPipeline</h4></blockquote>
<ul>
<li>修改settings.py</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;scrapy_redis.pipelines.RedisPipeline&apos;: 400,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="同时对-RedisPipeline-的调度器进行配置"><a href="#同时对-RedisPipeline-的调度器进行配置" class="headerlink" title="同时对 RedisPipeline 的调度器进行配置"></a>同时对 RedisPipeline 的调度器进行配置</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 使用scrapy-redis组件的去重队列</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"># 使用scrapy-redis组件自己的调度器</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># 是否允许暂停 </span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">SCHEDULER_PERSIST = True 代表 如果分布的一台机器宕机了，</span><br><span class="line">当机器重启后  继续爬虫时， 从上次失败的地方开始爬取，而不是从头爬取</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="7-REDIS-HOST-配置"><a href="#7-REDIS-HOST-配置" class="headerlink" title="7. REDIS_HOST 配置"></a>7. REDIS_HOST 配置</h4></blockquote>
<p>爬虫程序为分布式的所以不仅部署在你的机子上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">REDIS_HOST = &apos;redis服务的ip地址&apos;</span><br><span class="line">REDIS_PORT = 6379</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="8-执行爬虫文件"><a href="#8-执行爬虫文件" class="headerlink" title="8.执行爬虫文件"></a>8.执行爬虫文件</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 此时要定位到 爬虫文件的目录 </span><br><span class="line">cd redisPro/spiders</span><br><span class="line">scrapy runspider qiubai.py</span><br><span class="line"></span><br><span class="line"># 最后你发现有一句</span><br><span class="line">linstening at xxxx 代表监听在一个端口</span><br></pre></td></tr></table></figure>
<h4 id="还记得刚刚开启的redis-cli吗？"><a href="#还记得刚刚开启的redis-cli吗？" class="headerlink" title="还记得刚刚开启的redis-cli吗？"></a>还记得刚刚开启的redis-cli吗？</h4><blockquote>
<p>此时你要告诉redis-spider 起始url</p>
</blockquote>
<blockquote>
<p>还记得刚刚起名的 redis_key 吗？</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis_key = &apos;qiubaispider&apos;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>糗百-糗图的起始url</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.qiushibaike.com/pic/</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在redis-cli里 输入如下信息</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lpush 队列名称  起始url</span><br><span class="line"></span><br><span class="line"># 我们当前应用为</span><br><span class="line"></span><br><span class="line">lpush qiubaispider https://www.qiushibaike.com/pic/</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;&apos;finish_reason&apos;: &apos;finished&apos;,</span><br><span class="line"> &apos;finish_time&apos;: datetime.datetime(2018, 12, 2, 15, 26, 8, 106343),</span><br><span class="line"> &apos;log_count/DEBUG&apos;: 1,</span><br><span class="line"> &apos;log_count/INFO&apos;: 7,</span><br><span class="line"> &apos;memusage/max&apos;: 49971200,</span><br><span class="line"> &apos;memusage/startup&apos;: 49971200,</span><br><span class="line"> &apos;start_time&apos;: datetime.datetime(2018, 12, 2, 15, 26, 8, 96989)&#125;</span><br><span class="line">2018-12-02 23:26:08 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="出现这个就去看看-继承的类是否是-RedisCrawlSpider"><a href="#出现这个就去看看-继承的类是否是-RedisCrawlSpider" class="headerlink" title="出现这个就去看看 继承的类是否是 RedisCrawlSpider"></a>出现这个就去看看 继承的类是否是 RedisCrawlSpider</h4></blockquote>
<blockquote>
<h4 id="出现这个就去看看-继承的类是否是-RedisCrawlSpider-1"><a href="#出现这个就去看看-继承的类是否是-RedisCrawlSpider-1" class="headerlink" title="出现这个就去看看 继承的类是否是 RedisCrawlSpider"></a>出现这个就去看看 继承的类是否是 RedisCrawlSpider</h4></blockquote>
<blockquote>
<h4 id="出现这个就去看看-继承的类是否是-RedisCrawlSpider-2"><a href="#出现这个就去看看-继承的类是否是-RedisCrawlSpider-2" class="headerlink" title="出现这个就去看看 继承的类是否是 RedisCrawlSpider"></a>出现这个就去看看 继承的类是否是 RedisCrawlSpider</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class QiubaiSpider(RedisCrawlSpider)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-13分布式爬虫简介/" data-id="cklhjp25l00k7ulfy91pm6z9n" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-12scrapy之CrawlSpider" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-12scrapy之CrawlSpider/" class="article-date">
  <time datetime="2018-12-02T12:36:00.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-12scrapy之CrawlSpider/">Py007-02-12scrapy之CrawlSpider</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<h4 id="如果想对网址进行全站的爬取？"><a href="#如果想对网址进行全站的爬取？" class="headerlink" title="如果想对网址进行全站的爬取？"></a>如果想对网址进行全站的爬取？</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">解决方案：</span><br><span class="line">1. 手动请求的发送 (yield) 递归调用</span><br><span class="line"></span><br><span class="line">2. CrawlSpider(推荐)</span><br></pre></td></tr></table></figure>
<h3 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h3><ul>
<li>其实就是Spider的一个子类。</li>
<li>功能更加强大(链接提取器，规则解析器)</li>
</ul>
<h3 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h3><blockquote>
<p>项目初始化</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject crawlSpiderPro</span><br><span class="line"></span><br><span class="line">cd crawlSpiderPro</span><br><span class="line"></span><br><span class="line"># 以前生成应用是 scrapy genspider chouti dig.chouti.com</span><br><span class="line"></span><br><span class="line"># 此时我们的项目是 CrawlSpider的  所以 生成命令要修改一下</span><br><span class="line"># 此时我们的项目是 CrawlSpider的  所以 生成命令要修改一下</span><br><span class="line"># 此时我们的项目是 CrawlSpider的  所以 生成命令要修改一下</span><br><span class="line"></span><br><span class="line">scrapy genspider -t crawl  chouti dig.chouti.com</span><br></pre></td></tr></table></figure>
<blockquote>
<p>创建基于CrawlSpider的的爬虫文件</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t 爬虫名称 起始url</span><br></pre></td></tr></table></figure>
<blockquote>
<p>此时 spiders/chouti.py 内容如下</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"># 链接提取器</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line"># 规则解析器</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line">class ChoutiSpider(CrawlSpider):</span><br><span class="line">    name = &apos;chouti&apos;</span><br><span class="line">    # allowed_domains = [&apos;dig.chouti.com&apos;]</span><br><span class="line">    start_urls = [&apos;https://dig.chouti.com/&apos;]</span><br><span class="line">    </span><br><span class="line">    # 实例化一个链接提取器</span><br><span class="line">    # 链接提取器：用来提取指定的链接 (url)</span><br><span class="line">    # allow参数：赋值一个正则表达式</span><br><span class="line">    # 链接提取器就可以根据正则在页面中提取指定的链接</span><br><span class="line">    # 提取到的链接会全部交给规则解析器</span><br><span class="line">    link = LinkExtractor(allow=r&apos;Items/&apos;)</span><br><span class="line">    rules = (</span><br><span class="line">        # 实例化一个规则解析器</span><br><span class="line">        # 规则解析器接收了 链接提取器发送的链接后，就会对这些链接发起请求，获取链接对应的页面内容，就会根据指定的规则对页面内容中指定的数据值进行解析</span><br><span class="line">        # callback:指定一个解析规则(方法/函数)</span><br><span class="line">        Rule(link, callback=&apos;parse_item&apos;, follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<blockquote>
<p>settings.py修改</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 伪装请求载体身份</span><br><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br></pre></td></tr></table></figure>
<h4 id="经过如上初始化-我们来对抽屉新热榜进行-爬取"><a href="#经过如上初始化-我们来对抽屉新热榜进行-爬取" class="headerlink" title="经过如上初始化 我们来对抽屉新热榜进行 爬取"></a>经过如上初始化 我们来对抽屉新热榜进行 爬取</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">访问 https://dig.chouti.com/</span><br><span class="line"></span><br><span class="line">看到底部的分页器中的 a标签的href=&quot;/all/hot/recent/12&quot;</span><br><span class="line"></span><br><span class="line">这就是对应的页面url路径</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="修改代码里的-rule"><a href="#修改代码里的-rule" class="headerlink" title="修改代码里的 rule"></a>修改代码里的 rule</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"># 链接提取器</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line"># 规则解析器</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ChoutiSpider(CrawlSpider):</span><br><span class="line">    name = &apos;chouti&apos;</span><br><span class="line">    # allowed_domains = [&apos;dig.chouti.com&apos;]</span><br><span class="line">    start_urls = [&apos;https://dig.chouti.com/&apos;]</span><br><span class="line"></span><br><span class="line">    # 实例化一个链接提取器</span><br><span class="line">    # 链接提取器：用来提取指定的链接 (url)</span><br><span class="line">    # allow参数：赋值一个正则表达式</span><br><span class="line">    # 链接提取器就可以根据正则在页面中提取指定的链接</span><br><span class="line">    # 提取到的链接会全部交给规则解析器</span><br><span class="line">    link = LinkExtractor(allow=r&apos;/all/hot/recent/\d+&apos;)</span><br><span class="line">    rules = (</span><br><span class="line">        # 实例化一个规则解析器</span><br><span class="line">        # 规则解析器接收了 链接提取器发送的链接后，就会对这些链接发起请求，获取链接对应的页面内容，就会根据指定的规则对页面内容中指定的数据值进行解析</span><br><span class="line">        # callback:指定一个解析规则(方法/函数)</span><br><span class="line">        # follow: 先设置为False 后期详细说明</span><br><span class="line">        Rule(link, callback=&apos;parse_item&apos;, follow=False),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        print(response)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行命令</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl chouti --nolog</span><br><span class="line"></span><br><span class="line"># 打印如下信息</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/1&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/5&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/4&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/7&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/9&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/3&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/6&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/10&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/2&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/8&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="follow参数"><a href="#follow参数" class="headerlink" title="follow参数"></a>follow参数</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># follow代表 是否将提取器继续作用到 链接提取器取出的链接所表示的页面数据中</span><br><span class="line"></span><br><span class="line"># 刚刚执行爬虫命令后</span><br><span class="line">scrapy crawl chouti --nolog</span><br><span class="line"># 仅仅打印了 10个页码的链接</span><br><span class="line"># 而我们在 到第十页的时候 https://dig.chouti.com/all/hot/recent/10</span><br><span class="line"></span><br><span class="line"># 底部的分页器是</span><br><span class="line">1. 。。。 7 8 9 10 11 12 13 14 </span><br><span class="line"></span><br><span class="line"># 这就意味着 当到第十页的时候  会继续提取后面的页面链接 ==&gt; 直到最后一页数据</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="修改follow-True"><a href="#修改follow-True" class="headerlink" title="修改follow:True"></a>修改follow:True</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"># 链接提取器</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line"># 规则解析器</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ChoutiSpider(CrawlSpider):</span><br><span class="line">    name = &apos;chouti&apos;</span><br><span class="line">    # allowed_domains = [&apos;dig.chouti.com&apos;]</span><br><span class="line">    start_urls = [&apos;https://dig.chouti.com/&apos;]</span><br><span class="line"></span><br><span class="line">    # 实例化一个链接提取器</span><br><span class="line">    # 链接提取器：用来提取指定的链接 (url)</span><br><span class="line">    # allow参数：赋值一个正则表达式</span><br><span class="line">    # 链接提取器就可以根据正则在页面中提取指定的链接</span><br><span class="line">    # 提取到的链接会全部交给规则解析器</span><br><span class="line">    link = LinkExtractor(allow=r&apos;/all/hot/recent/\d+&apos;)</span><br><span class="line">    rules = (</span><br><span class="line">        # 实例化一个规则解析器</span><br><span class="line">        # 规则解析器接收了 链接提取器发送的链接后，就会对这些链接发起请求，获取链接对应的页面内容，就会根据指定的规则对页面内容中指定的数据值进行解析</span><br><span class="line">        # callback:指定一个解析规则(方法/函数)</span><br><span class="line">        # follow: 先设置为False 后期详细说明</span><br><span class="line">        # follow代表 是否将提取器继续作用到 链接提取器取出的链接所表示的页面数据中</span><br><span class="line">        Rule(link, callback=&apos;parse_item&apos;, follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        print(response)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>再次执行爬虫命令</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl chouti --nolog</span><br><span class="line"></span><br><span class="line"># 打印如下</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/1&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/2&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/3&gt;</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/115&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/116&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/117&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/118&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/119&gt;</span><br><span class="line">&lt;200 https://dig.chouti.com/all/hot/recent/120&gt;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-12scrapy之CrawlSpider/" data-id="cklhjp25l00k6ulfyzcx2kett" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-11scrapy之请求传参" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-11scrapy之请求传参/" class="article-date">
  <time datetime="2018-12-02T10:01:49.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-11scrapy之请求传参/">Py007-02-11scrapy之请求传参</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="请求传参"><a href="#请求传参" class="headerlink" title="请求传参"></a>请求传参</h3><p>爬取的数据值不在同一页面中</p>
<blockquote>
<p>需求爬取  <a href="http://www.id97.com" target="_blank" rel="noopener">www.id97.com</a> 电影数据和对应电影详情页面的数据</p>
</blockquote>
<h4 id="项目初始化"><a href="#项目初始化" class="headerlink" title="项目初始化"></a>项目初始化</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject moviePro</span><br><span class="line"></span><br><span class="line">cd moviePro/</span><br><span class="line"></span><br><span class="line">scrapy genspider movie www.id97.com</span><br></pre></td></tr></table></figure>
<blockquote>
<p>spiders/movie.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MovieSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;movie&apos;</span><br><span class="line">    # allowed_domains = [&apos;www.id97.com&apos;]</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    www.id97.com 的域名已经换了 www.55xia.com</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    start_urls = [&apos;http://www.id97.com/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 电影名称，类型，导演，语言，片长</span><br><span class="line">        # 浏览器里 找到电影列表外层容器 copy-xpath</span><br><span class="line">        div_list = response.xpath(&apos;/html/body/div[1]/div[2]/div&apos;)</span><br><span class="line">        for div in div_list:</span><br><span class="line">            name = div.xpath(&apos;.//div[@class=&quot;meta&quot;]/h1/a/text()&apos;).extract_first()</span><br><span class="line">            kind = div.xpath(&apos;.//div[@class=&quot;otherinfo&quot;]//text()&apos;).extract_first()</span><br><span class="line">            url = div.xpath(&apos;.//div[@class=&quot;meta&quot;]/h1/a/@href&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 而此时 导演和演员  不再当前页面 在对应的详情页面</span><br><span class="line">            # 需要对url发请求，获取页面数据，进行数据解析</span><br><span class="line"></span><br><span class="line">            yield scrapy.Request(url=url,callback=self.secondParse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 专门用于处理二级自页面的解析函数</span><br><span class="line">    def secondParse(self,response):</span><br><span class="line">        author = response.xpath(&apos;/html/body/div[1]/div/div/div[1]/div[1]/div[2]/table/tbody/tr[1]/td[2]/a/text()&apos;).extract_first()</span><br><span class="line">        language = response.xpath(&apos;/html/body/div[1]/div/div/div[1]/div[1]/div[2]/table/tbody/tr[6]/td[2]/text()&apos;).extract_first()</span><br><span class="line">        longTime = response.xpath(&apos;/html/body/div[1]/div/div/div[1]/div[1]/div[2]/table/tbody/tr[8]/td[2]/text()&apos;).extract_first()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>items.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MovieproItem(scrapy.Item):</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    kind = scrapy.Field()</span><br><span class="line">    actor = scrapy.Field()</span><br><span class="line">    language = scrapy.Field()</span><br><span class="line">    longTime = scrapy.Field()</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="问题来了，数据分散在两次请求里，如何处理数据？"><a href="#问题来了，数据分散在两次请求里，如何处理数据？" class="headerlink" title="问题来了，数据分散在两次请求里，如何处理数据？"></a>问题来了，数据分散在两次请求里，如何处理数据？</h4></blockquote>
<blockquote>
<p>请求传参</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">from moviePro.items import MovieproItem</span><br><span class="line">class MovieSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;movie&apos;</span><br><span class="line">    # allowed_domains = [&apos;www.id97.com&apos;]</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    www.id97.com 的域名已经换了 www.55xia.com</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    start_urls = [&apos;http://www.id97.com/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 电影名称，类型，导演，语言，片长</span><br><span class="line">        # 浏览器里 找到电影列表外层容器 copy-xpath</span><br><span class="line">        div_list = response.xpath(&apos;/html/body/div[1]/div[2]/div&apos;)</span><br><span class="line">        for div in div_list:</span><br><span class="line">            name = div.xpath(&apos;.//div[@class=&quot;meta&quot;]/h1/a/text()&apos;).extract_first()</span><br><span class="line">            kind = div.xpath(&apos;.//div[@class=&quot;otherinfo&quot;]//text()&apos;).extract_first()</span><br><span class="line">            url = div.xpath(&apos;.//div[@class=&quot;meta&quot;]/h1/a/@href&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 创建items对象</span><br><span class="line">            item = MovieproItem()</span><br><span class="line">            item[&apos;name&apos;] = name</span><br><span class="line">            item[&apos;kind&apos;] = kind</span><br><span class="line">            # 如何将剩下的数据进行传递呢？  ====》 请求传参(meta参数)</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            meta 需赋值为一个字典  将item封装到字典里</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">            # 而此时 导演和演员  不再当前页面 在对应的详情页面</span><br><span class="line">            # 需要对url发请求，获取页面数据，进行数据解析</span><br><span class="line"></span><br><span class="line">            yield scrapy.Request(url=url,callback=self.secondParse,meta=&#123;&apos;item&apos;:item&#125;)</span><br><span class="line"></span><br><span class="line">    # 专门用于处理二级自页面的解析函数</span><br><span class="line">    def secondParse(self,response):</span><br><span class="line">        author = response.xpath(&apos;/html/body/div[1]/div/div/div[1]/div[1]/div[2]/table/tbody/tr[1]/td[2]/a/text()&apos;).extract_first()</span><br><span class="line">        language = response.xpath(&apos;/html/body/div[1]/div/div/div[1]/div[1]/div[2]/table/tbody/tr[6]/td[2]/text()&apos;).extract_first()</span><br><span class="line">        longTime = response.xpath(&apos;/html/body/div[1]/div/div/div[1]/div[1]/div[2]/table/tbody/tr[8]/td[2]/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">        # 取出Request方法的meta参数</span><br><span class="line">        item = response.meta[&apos;item&apos;]</span><br><span class="line">        item[&apos;author&apos;] = author</span><br><span class="line">        item[&apos;language&apos;] = language</span><br><span class="line">        item[&apos;longTime&apos;] = longTime</span><br><span class="line"></span><br><span class="line">        # 将item提交给管道</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure>
<blockquote>
<p>编写pipelines.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">class MovieproPipeline(object):</span><br><span class="line">    fp = None</span><br><span class="line"></span><br><span class="line">    def open_spider(self,spider):</span><br><span class="line">        self.fp = open(&apos;movie.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">    def close_spider(self,spider):</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        detail = item[&apos;name&apos;]+&apos;:&apos;+item[&apos;kind&apos;]+&apos;:&apos;+item[&apos;author&apos;]+&apos;:&apos;+item[&apos;language&apos;]+&apos;:&apos;+item[&apos;longTime&apos;]</span><br><span class="line"></span><br><span class="line">        self.fp.write(detail)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<blockquote>
<p>修改settings.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br><span class="line"></span><br><span class="line"># 解开注释</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;moviePro.pipelines.MovieproPipeline&apos;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行命令</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl movie --nolog</span><br><span class="line"></span><br><span class="line">此时发现movie.txt 是空的  因为起始url是首页 并不是对应的电影列表页面</span><br><span class="line">实际url为</span><br><span class="line">https://www.55xia.com/movie/</span><br><span class="line"></span><br><span class="line">再次运行</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-11scrapy之请求传参/" data-id="cklhjp25k00k3ulfy2xb7kqrh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-10scrapy之日志等级" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-10scrapy之日志等级/" class="article-date">
  <time datetime="2018-12-02T09:48:01.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-10scrapy之日志等级/">Py007-02-10scrapy之日志等级</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>回顾我们之前的爬虫命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 应用名  --nolog</span><br><span class="line"></span><br><span class="line"># 此时的打印信息  很多完全看不懂！</span><br></pre></td></tr></table></figure>
<h4 id="日志等级-种类"><a href="#日志等级-种类" class="headerlink" title="日志等级(种类)"></a>日志等级(种类)</h4><ul>
<li>ERROR:错误</li>
<li>WARNING:警告</li>
<li>INFO:一般信息</li>
<li>DEBUG:调试信息(默认)</li>
</ul>
<blockquote>
<h4 id="控制日志的打印-settings-py"><a href="#控制日志的打印-settings-py" class="headerlink" title="控制日志的打印 settings.py"></a>控制日志的打印 settings.py</h4></blockquote>
<ul>
<li>LOG_LEVEL 指定输出某一种信息</li>
<li>LOG_FILE 将日志信息存到一个文件里</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 添加 LOG_LEVEL = &apos;种类&apos; 来控制显示哪一层级的日志</span><br><span class="line">LOG_LEVEL = &apos;ERROR&apos;</span><br><span class="line"># 此时只有错误的时候才会打印日志</span><br></pre></td></tr></table></figure>
<blockquote>
<p>将日志信息存到一个文件里</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOG_FILE = &apos;log.txt&apos;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-10scrapy之日志等级/" data-id="cklhjp25j00k2ulfybyfhuhdl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-09scrapy之代理操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-09scrapy之代理操作/" class="article-date">
  <time datetime="2018-12-02T09:15:35.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-09scrapy之代理操作/">Py007-02-09scrapy之代理操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="代理操作"><a href="#代理操作" class="headerlink" title="代理操作"></a>代理操作</h3><ul>
<li><a href="https://www.baidu.com/s?wd=ip" target="_blank" rel="noopener">https://www.baidu.com/s?wd=ip</a> </li>
</ul>
<p>这个url可以看本机ip</p>
<h4 id="代理操作初步——先看本机ip"><a href="#代理操作初步——先看本机ip" class="headerlink" title="代理操作初步——先看本机ip"></a>代理操作初步——先看本机ip</h4><blockquote>
<p>新建工程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject proxyPro</span><br><span class="line"></span><br><span class="line">cd proxyPro</span><br><span class="line"></span><br><span class="line">scrapy genspider proxyDemo www.baidu.com/s?wd=ip</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 修改 生成的spiders/proxyDemo.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class ProxydemoSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;proxyDemo&apos;</span><br><span class="line">    # allowed_domains = [&apos;www.baidu.com/s?wd=ip&apos;]</span><br><span class="line">    start_urls = [&apos;https://www.baidu.com/s?wd=ip&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        fp = open(&apos;proxy.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">        fp.write(response.text)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>修改一些配置 settings.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 伪装请求载体身份</span><br><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行爬虫命令</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl proxyDemo --nolog</span><br><span class="line"></span><br><span class="line"># 查看 proxy.html 看到本机ip</span><br></pre></td></tr></table></figure>
<h4 id="代理——中间件"><a href="#代理——中间件" class="headerlink" title="代理——中间件"></a>代理——中间件</h4><blockquote>
<p>middlewares.py 的修改</p>
</blockquote>
<p>自定义下载中间件的类，在类中事先 process_request (处理中间拦截到的请求的)方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class MyProxy(object):</span><br><span class="line">    # 此时的process_request 依旧是对父类方法的重写</span><br><span class="line">    def process_request(self,request,spider):</span><br><span class="line">        # 请求ip的更换</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        去 代理ip网 找个代理</span><br><span class="line">        http://www.goubanjia.com/</span><br><span class="line">        &apos;&apos;&apos;</span><br><span class="line">        request.meta[&apos;proxy&apos;] = &apos;https://123.1.150.244:80&apos;</span><br><span class="line"></span><br><span class="line"># 之前其他的无用代码可以移除</span><br></pre></td></tr></table></figure>
<blockquote>
<p>去settings.py里继续配置</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 找到 下载中间件 DOWNLOADER_MIDDLEWARES</span><br><span class="line"># 这个是之前 middlewares.py里的 下载中间件</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   &apos;proxyPro.middlewares.ProxyproDownloaderMiddleware&apos;: 543,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 修改成我们自己定义的 中间件类</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   &apos;proxyPro.middlewares.MyProxy&apos;: 543,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行爬虫命令(此时建议  不携带–nolog参数，因为代理ip可能会失败)</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl proxyDemo</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">代理和请求的url的协议头  要一致  要么是http ,要么是https</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote>
<p>1.下载中间件类的自定义：</p>
</blockquote>
<ul>
<li>继承 object </li>
<li>重写 process_request(self,request,spider) 方法</li>
</ul>
<blockquote>
<p>2.配置文件中 开启下载中间件</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-09scrapy之代理操作/" data-id="cklhjp25j00jzulfyy0yoo5a3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-08scrapy之cookie操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-08scrapy之cookie操作/" class="article-date">
  <time datetime="2018-12-02T08:36:45.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-08scrapy之cookie操作/">Py007-02-08scrapy之cookie操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="cookie操作"><a href="#cookie操作" class="headerlink" title="cookie操作"></a>cookie操作</h3><blockquote>
<p>豆瓣登陆指定页面</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject doubanPro</span><br><span class="line"></span><br><span class="line">cd doubanPro</span><br><span class="line"></span><br><span class="line">scrapy genspider douban www.douban.com</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第一步-先该settins-py里的一些配置"><a href="#第一步-先该settins-py里的一些配置" class="headerlink" title="第一步 先该settins.py里的一些配置"></a>第一步 先该settins.py里的一些配置</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos;</span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = False</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h4></blockquote>
<ul>
<li>修改起始url 也就是登陆的url</li>
<li>重写 start_requests方法  发起post请求 (这里有登录操作，携带post参数)</li>
<li>登录成功后 通过yield 来发送第二个请求 也就是个人主页页面数据</li>
<li>自定义 请求成功后的parse方法 来进行个人页面数据的解析操作</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class DoubanSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;douban&apos;</span><br><span class="line">    allowed_domains = [&apos;www.douban.com&apos;]</span><br><span class="line">    start_urls = [&apos;http://www.douban.com/accounts/login&apos;]</span><br><span class="line"></span><br><span class="line">    # 重写 start_requests</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # 将请求参数封装到 字典中</span><br><span class="line">        # 豆瓣登陆页面 https://accounts.douban.com/login</span><br><span class="line">        data = &#123;</span><br><span class="line">            &apos;source&apos;: &apos;index_nav&apos;,</span><br><span class="line">            &apos;form_email&apos;: &apos;15027900535&apos;,</span><br><span class="line">            &apos;form_password&apos;: &apos;bobo@15027900535&apos;,</span><br><span class="line">        &#125;</span><br><span class="line">        for url in self.start_urls:</span><br><span class="line">            yield scrapy.FormRequest(url=url,formdata=data,callback=self.parse)</span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        # 登陆成功后的页面数据进行存储</span><br><span class="line"></span><br><span class="line">        fp = open(&apos;main.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">        fp.write(response.text)</span><br><span class="line"></span><br><span class="line">        # 获取当前用户个人主页对应的页面数据</span><br><span class="line">        url = &apos;https://www.douban.com/people/185687620/&apos;</span><br><span class="line"></span><br><span class="line">        yield scrapy.Request(url=url,callback=self.parseSecondPage)</span><br><span class="line"></span><br><span class="line">    # 定义跳转指定页面的 parse方法</span><br><span class="line">    def parseSecondPage(self,response):</span><br><span class="line">        fp = open(&apos;second.html&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;)</span><br><span class="line">        fp.write(response.text)</span><br><span class="line"></span><br><span class="line">        # 可以对当前用户的个人主页的页面数据进行解析操作</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第三步-执行爬虫命令"><a href="#第三步-执行爬虫命令" class="headerlink" title="第三步 执行爬虫命令"></a>第三步 执行爬虫命令</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl douban --nolog</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote>
<p>通过scrapy进行cookie操作时，并不需要将cookie进行特定的提取，也不需要将cookie加载到我们的请求对象中去</p>
</blockquote>
<ul>
<li>涉及登录的时候，如果服务器给我们设置了cookie,scrapy在第二次请求页面的时候自动帮我们携带cookie </li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-08scrapy之cookie操作/" data-id="cklhjp25i00jyulfyolwwlqr3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-07scrapy发post请求" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-07scrapy发post请求/" class="article-date">
  <time datetime="2018-12-02T07:05:43.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-07scrapy发post请求/">Py007-02-07scrapy发post请求</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="scrapy里post请求"><a href="#scrapy里post请求" class="headerlink" title="scrapy里post请求"></a>scrapy里post请求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject postPro</span><br><span class="line"></span><br><span class="line">cd postPro</span><br><span class="line"></span><br><span class="line">scrapy genspider postDemo www.baidu.com</span><br></pre></td></tr></table></figure>
<h4 id="start-requests方法"><a href="#start-requests方法" class="headerlink" title="start_requests方法"></a>start_requests方法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class PostdemoSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;postDemo&apos;</span><br><span class="line">    # allowed_domains = [&apos;www.baidu.com&apos;]</span><br><span class="line">    start_urls = [&apos;https://fanyi.baidu.com/sug&apos;]</span><br><span class="line"></span><br><span class="line">    # start_requests是父类的方法：对start_urls里的url进行get请求发送</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 大概实现如下</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        for url in self.start_urls:</span><br><span class="line">            yield scrapy.Request(url=url,callback=self.parse)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    </span><br><span class="line">    def parse(self, response):</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<h4 id="发送post请求"><a href="#发送post请求" class="headerlink" title="发送post请求"></a>发送post请求</h4><p>start_requests 默认发送get 如何发送post呢？</p>
<blockquote>
<p>重写 start_requests 方法的实现</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 发起post</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    1.将Request方法的method赋值成post(不推荐)</span><br><span class="line">    2.FormRequest() 这个方法也可以发起post请求(推荐)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # post请求参数</span><br><span class="line">        data = &#123;</span><br><span class="line">            &apos;kw&apos;:&apos;dog&apos;</span><br><span class="line">        &#125;</span><br><span class="line">        for url in self.start_urls:</span><br><span class="line">            # formdata :请求参数对应的字典</span><br><span class="line">            yield scrapy.FormRequest(url=url,formdata=data,callback=self.parse)</span><br></pre></td></tr></table></figure>
<h3 id="需求：百度翻译中指定词条的翻译结果-进行获取"><a href="#需求：百度翻译中指定词条的翻译结果-进行获取" class="headerlink" title="需求：百度翻译中指定词条的翻译结果 进行获取"></a>需求：百度翻译中指定词条的翻译结果 进行获取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class PostdemoSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;postDemo&apos;</span><br><span class="line">    # allowed_domains = [&apos;www.baidu.com&apos;]</span><br><span class="line">    start_urls = [&apos;https://fanyi.baidu.com/sug&apos;]</span><br><span class="line"></span><br><span class="line">    # start_requests是父类的方法：对start_urls里的url进行get请求发送</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 大概实现如下</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        for url in self.start_urls:</span><br><span class="line">            yield scrapy.Request(url=url,callback=self.parse)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">    # 发起post</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    1.将Request方法的method赋值成post</span><br><span class="line">    2.FormRequest() 这个方法也可以发起post请求(推荐)</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # post请求参数</span><br><span class="line">        data = &#123;</span><br><span class="line">            &apos;kw&apos;:&apos;dog&apos;</span><br><span class="line">        &#125;</span><br><span class="line">        for url in self.start_urls:</span><br><span class="line">            # formdata :请求参数对应的字典</span><br><span class="line">            yield scrapy.FormRequest(url=url,formdata=data,callback=self.parse)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        print(response.text)</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：别忘了修改settings.py里的设置</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 伪装请求载体身份</span><br><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br></pre></td></tr></table></figure>
<blockquote>
<p>验证结果</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl postDemo --nolog</span><br></pre></td></tr></table></figure>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote>
<p>如何发post方法？</p>
</blockquote>
<p>一定要对start_requests方法进行重写</p>
<ul>
<li>1.将Request方法的method赋值成post</li>
<li>2.FormRequest() 这个方法也可以发起post请求(推荐)</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-07scrapy发post请求/" data-id="cklhjp25h00jvulfyitkrs16c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-02-06scrapy核心组件" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/02/Py007-02-06scrapy核心组件/" class="article-date">
  <time datetime="2018-12-02T06:39:47.000Z" itemprop="datePublished">2018-12-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/02/Py007-02-06scrapy核心组件/">Py007-02-06scrapy核心组件</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="scrapy五大核心组件"><a href="#scrapy五大核心组件" class="headerlink" title="scrapy五大核心组件"></a>scrapy五大核心组件</h3><p><img src="https://raw.githubusercontent.com/slTrust/note/master/img/py/py007_02_0601.png" alt></p>
<blockquote>
<p>引擎(Scrapy)</p>
</blockquote>
<ul>
<li>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
</ul>
<blockquote>
<p>调度器(Scheduler)</p>
</blockquote>
<ul>
<li>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
</ul>
<blockquote>
<p>下载器(Downloader)</p>
</blockquote>
<ul>
<li>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
<blockquote>
<p>爬虫(Spiders)</p>
</blockquote>
<ul>
<li>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
</ul>
<blockquote>
<p>项目管道(Pipeline)</p>
</blockquote>
<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/02/Py007-02-06scrapy核心组件/" data-id="cklhjp25h00juulfyxe8z02s6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/27/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="page-number" href="/page/27/">27</a><span class="page-number current">28</span><a class="page-number" href="/page/29/">29</a><a class="page-number" href="/page/30/">30</a><span class="space">&hellip;</span><a class="page-number" href="/page/55/">55</a><a class="extend next" rel="next" href="/page/29/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CSS/">CSS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ES6速学/">ES6速学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JS不知深浅/">JS不知深浅</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M01/">M01</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M02/">M02</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M03/">M03</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M04/">M04</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M06/">M06</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M07/">M07</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M08/">M08</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M09/">M09</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NodeWeb/">NodeWeb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Node后端/">Node后端</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ReactWheels/">ReactWheels</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React入门/">React入门</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TS入门/">TS入门</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/express/">express</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/fullstack/">fullstack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/http/">http</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jQuery/">jQuery</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mobile/">mobile</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node/">node</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node每日精进/">node每日精进</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/oak/">oak</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/">tools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vim/">vim</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/">vue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vultr/">vultr</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web性能优化/">web性能优化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web面经/">web面经</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前端知识点/">前端知识点</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CSS/" style="font-size: 13.91px;">CSS</a> <a href="/tags/ES6速学/" style="font-size: 14.35px;">ES6速学</a> <a href="/tags/JS不知深浅/" style="font-size: 11.3px;">JS不知深浅</a> <a href="/tags/M01/" style="font-size: 13.48px;">M01</a> <a href="/tags/M02/" style="font-size: 15.22px;">M02</a> <a href="/tags/M03/" style="font-size: 15.65px;">M03</a> <a href="/tags/M04/" style="font-size: 16.09px;">M04</a> <a href="/tags/M06/" style="font-size: 18.26px;">M06</a> <a href="/tags/M07/" style="font-size: 17.83px;">M07</a> <a href="/tags/M08/" style="font-size: 16.96px;">M08</a> <a href="/tags/M09/" style="font-size: 11.74px;">M09</a> <a href="/tags/NodeWeb/" style="font-size: 15.65px;">NodeWeb</a> <a href="/tags/Node后端/" style="font-size: 18.7px;">Node后端</a> <a href="/tags/ReactWheels/" style="font-size: 16.52px;">ReactWheels</a> <a href="/tags/React入门/" style="font-size: 15.22px;">React入门</a> <a href="/tags/TS入门/" style="font-size: 14.78px;">TS入门</a> <a href="/tags/express/" style="font-size: 10.43px;">express</a> <a href="/tags/fullstack/" style="font-size: 17.83px;">fullstack</a> <a href="/tags/http/" style="font-size: 12.17px;">http</a> <a href="/tags/jQuery/" style="font-size: 10px;">jQuery</a> <a href="/tags/java/" style="font-size: 19.13px;">java</a> <a href="/tags/linux/" style="font-size: 10.87px;">linux</a> <a href="/tags/mobile/" style="font-size: 11.3px;">mobile</a> <a href="/tags/mongodb/" style="font-size: 12.61px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 12.61px;">mysql</a> <a href="/tags/node/" style="font-size: 10.43px;">node</a> <a href="/tags/node每日精进/" style="font-size: 10px;">node每日精进</a> <a href="/tags/oak/" style="font-size: 20px;">oak</a> <a href="/tags/python/" style="font-size: 17.39px;">python</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vue/" style="font-size: 13.04px;">vue</a> <a href="/tags/vultr/" style="font-size: 10px;">vultr</a> <a href="/tags/web性能优化/" style="font-size: 10px;">web性能优化</a> <a href="/tags/web面经/" style="font-size: 10px;">web面经</a> <a href="/tags/前端知识点/" style="font-size: 19.57px;">前端知识点</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/02/23/Node-web05-09-03博客系统使用nginx/">Node-web05-09-03博客系统使用nginx</a>
          </li>
        
          <li>
            <a href="/2021/02/22/Node-web05-09-02博客系统部署最新版流程/">Node-web05-09-02博客系统部署最新版流程</a>
          </li>
        
          <li>
            <a href="/2021/02/20/Node-web05-09-01博客统部署2/">Node-web05-09-01博客统部署优化解释</a>
          </li>
        
          <li>
            <a href="/2021/02/20/Node-web05-08博客统页面完善/">Node-web05-08博客统页面完善</a>
          </li>
        
          <li>
            <a href="/2021/02/19/Node-web05-07-03服务器端构建/">Node-web05-07-03服务器端构建</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Stevin<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>