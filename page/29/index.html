<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Almost</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Almost">
<meta property="og:url" content="http://yoursite.com/page/29/index.html">
<meta property="og:site_name" content="Almost">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Almost">
  
    <link rel="alternate" href="/atom.xml" title="Almost" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Almost</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Py007-01-11bs4使用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/01/Py007-01-11bs4使用/" class="article-date">
  <time datetime="2018-11-30T16:26:21.000Z" itemprop="datePublished">2018-12-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/01/Py007-01-11bs4使用/">Py007-01-11bs4使用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="BeautifulSoup解析"><a href="#BeautifulSoup解析" class="headerlink" title="BeautifulSoup解析"></a>BeautifulSoup解析</h3><p>python独有</p>
<blockquote>
<p>环境安装</p>
</blockquote>
<ul>
<li>需要将pip源设置为国内源，阿里源、豆瓣源、网易源等<ul>
<li>windows<br>（1）打开文件资源管理器(文件夹地址栏中)<br>（2）地址栏上面输入 %appdata%<br>（3）在这里面新建一个文件夹  pip<br>（4）在pip文件夹里面新建一个文件叫做  pip.ini ,内容写如下即可<br>   [global]<br>   timeout = 6000<br>   index-url = <a href="https://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple/</a><br>   trusted-host = mirrors.aliyun.com</li>
<li>linux<br>（1）cd ~<br>（2）mkdir ~/.pip<br>（3）vi ~/.pip/pip.conf<br>（4）编辑内容，和windows一模一样</li>
</ul>
</li>
<li>需要安装：pip install bs4<br>   bs4在使用时候需要一个第三方库，把这个库也安装一下<br>   pip install lxml</li>
</ul>
<blockquote>
<p>基础使用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">使用流程：       </span><br><span class="line">    - 导包：from bs4 import BeautifulSoup</span><br><span class="line">    - 使用方式：可以将一个html文档，转化为BeautifulSoup对象，然后通过对象的方法或者属性去查找指定的节点内容</span><br><span class="line">        （1）转化本地文件：</span><br><span class="line">             - soup = BeautifulSoup(open(&apos;本地文件&apos;), &apos;lxml&apos;)</span><br><span class="line">        （2）转化网络文件：</span><br><span class="line">             - soup = BeautifulSoup(&apos;字符串类型或者字节类型&apos;, &apos;lxml&apos;)</span><br><span class="line">        （3）打印soup对象显示内容为html文件中的内容</span><br><span class="line"></span><br><span class="line">基础巩固：</span><br><span class="line">    （1）根据标签名查找</span><br><span class="line">        - soup.a   只能找到第一个符合要求的标签</span><br><span class="line">    （2）获取属性</span><br><span class="line">        - soup.a.attrs  获取a所有的属性和属性值，返回一个字典</span><br><span class="line">        - soup.a.attrs[&apos;href&apos;]   获取href属性</span><br><span class="line">        - soup.a[&apos;href&apos;]   也可简写为这种形式</span><br><span class="line">    （3）获取内容</span><br><span class="line">        - soup.a.string</span><br><span class="line">        - soup.a.text</span><br><span class="line">        - soup.a.get_text()</span><br><span class="line">       【注意】如果标签还有标签，那么string获取到的结果为None，而其它两个，可以获取文本内容</span><br><span class="line">    （4）find：找到第一个符合要求的标签</span><br><span class="line">        - soup.find(&apos;a&apos;)  找到第一个符合要求的</span><br><span class="line">        - soup.find(&apos;a&apos;, title=&quot;xxx&quot;)</span><br><span class="line">        - soup.find(&apos;a&apos;, alt=&quot;xxx&quot;)</span><br><span class="line">        - soup.find(&apos;a&apos;, class_=&quot;xxx&quot;)</span><br><span class="line">        - soup.find(&apos;a&apos;, id=&quot;xxx&quot;)</span><br><span class="line">    （5）find_all：找到所有符合要求的标签</span><br><span class="line">        - soup.find_all(&apos;a&apos;)</span><br><span class="line">        - soup.find_all([&apos;a&apos;,&apos;b&apos;]) 找到所有的a和b标签</span><br><span class="line">        - soup.find_all(&apos;a&apos;, limit=2)  限制前两个</span><br><span class="line">    （6）根据选择器选择指定的内容</span><br><span class="line">               select:soup.select(&apos;#feng&apos;)</span><br><span class="line">        - 常见的选择器：标签选择器(a)、类选择器(.)、id选择器(#)、层级选择器</span><br><span class="line">            - 层级选择器：</span><br><span class="line">                div .dudu #lala .meme .xixi  下面好多级</span><br><span class="line">                div &gt; p &gt; a &gt; .lala          只能是下面一级</span><br><span class="line">        【注意】select选择器返回永远是列表，需要通过下标提取指定的对象</span><br></pre></td></tr></table></figure>
<h4 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h4><blockquote>
<p>使用bs4实现将诗词名句网站中三国演义小说的每一章的内容爬去到本地磁盘进行存储   <a href="http://www.shicimingju.com/book/sanguoyanyi.html" target="_blank" rel="noopener">http://www.shicimingju.com/book/sanguoyanyi.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line">         &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">     &#125;</span><br><span class="line">def parse_content(url):</span><br><span class="line">    #获取标题正文页数据</span><br><span class="line">    page_text = requests.get(url,headers=headers).text</span><br><span class="line">    soup = BeautifulSoup(page_text,&apos;lxml&apos;)</span><br><span class="line">    #解析获得标签</span><br><span class="line">    ele = soup.find(&apos;div&apos;,class_=&apos;chapter_content&apos;)</span><br><span class="line">    content = ele.text #获取标签中的数据值</span><br><span class="line">    return content</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">     url = &apos;http://www.shicimingju.com/book/sanguoyanyi.html&apos;</span><br><span class="line">     reponse = requests.get(url=url,headers=headers)</span><br><span class="line">     page_text = reponse.text</span><br><span class="line"></span><br><span class="line">     #创建soup对象</span><br><span class="line">     soup = BeautifulSoup(page_text,&apos;lxml&apos;)</span><br><span class="line">     #解析数据</span><br><span class="line">     a_eles = soup.select(&apos;.book-mulu &gt; ul &gt; li &gt; a&apos;)</span><br><span class="line">     print(a_eles)</span><br><span class="line">     cap = 1</span><br><span class="line">     fp =  open(&apos;./sanguo.txt&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line">     for ele in a_eles:</span><br><span class="line">         print(&apos;开始下载第%d章节&apos;%cap)</span><br><span class="line">         cap+=1</span><br><span class="line">         title = ele.string</span><br><span class="line">         content_url = &apos;http://www.shicimingju.com&apos;+ele[&apos;href&apos;]</span><br><span class="line">         content = parse_content(content_url)</span><br><span class="line"></span><br><span class="line">         fp.write(title+&quot;:&quot;+content+&apos;\n\n\n&apos;)</span><br><span class="line">         print(&apos;结束下载第%d章节&apos;%cap)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/01/Py007-01-11bs4使用/" data-id="cklcb6bjo00j2wo91hkw4zbl0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-10xpath使用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/30/Py007-01-10xpath使用/" class="article-date">
  <time datetime="2018-11-30T15:11:36.000Z" itemprop="datePublished">2018-11-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/30/Py007-01-10xpath使用/">Py007-01-10xpath使用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="xpath在爬虫中使用流程"><a href="#xpath在爬虫中使用流程" class="headerlink" title="xpath在爬虫中使用流程"></a>xpath在爬虫中使用流程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 1.下载: pip install lxml</span><br><span class="line">- 2.导包: from lxml import etree</span><br><span class="line">- 3.创建etree对象 进行指定数据的解析</span><br><span class="line">    - 本地   etree = etree.parse(&apos;本地文件路径&apos;)</span><br><span class="line">            etree.xpath(&apos;xpath表达式&apos;)</span><br><span class="line">    - 网络   etree = etree.HTML(&apos;网络请求到的页面数据&apos;)</span><br><span class="line">            etree.xpath(&apos;xpath表达式&apos;)</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="常用xpath表达式"><a href="#常用xpath表达式" class="headerlink" title="常用xpath表达式"></a>常用xpath表达式</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">属性定位：</span><br><span class="line">    #找到class属性值为song的div标签</span><br><span class="line">    //div[@class=&quot;song&quot;] </span><br><span class="line">层级&amp;索引定位：</span><br><span class="line">    #找到class属性值为tang的div的直系子标签ul下的第二个子标签li下的直系子标签a</span><br><span class="line">    //div[@class=&quot;tang&quot;]/ul/li[2]/a</span><br><span class="line">逻辑运算：</span><br><span class="line">    #找到href属性值为空且class属性值为du的a标签</span><br><span class="line">    //a[@href=&quot;&quot; and @class=&quot;du&quot;]</span><br><span class="line">模糊匹配：</span><br><span class="line">    # 当前所有div 同时class属性包含了 ng  </span><br><span class="line">    # 如 &lt;div class=&quot;song&quot;&gt; 和&lt;div class=&quot;tang&quot;&gt; 都会被匹配</span><br><span class="line">    //div[contains(@class, &quot;ng&quot;)]</span><br><span class="line">    # 当前所有div 的class属性 以 ta开头的</span><br><span class="line">    //div[starts-with(@class, &quot;ta&quot;)]</span><br><span class="line">取文本：</span><br><span class="line">    # /表示获取某个标签下的文本内容</span><br><span class="line">    //div[@class=&quot;song&quot;]/p[1]/text()</span><br><span class="line">    # //表示获取某个标签下的文本内容和所有子标签下的文本内容</span><br><span class="line">    //div[@class=&quot;tang&quot;]//text()</span><br><span class="line">取属性：</span><br><span class="line">    //div[@class=&quot;tang&quot;]//li[2]/a/@href</span><br></pre></td></tr></table></figure>
<h4 id="xpath插件"><a href="#xpath插件" class="headerlink" title="xpath插件"></a>xpath插件</h4><p>我们不可能每次都把html下载到本地  来写xpath表达式</p>
<blockquote>
<p>xpath插件可以直接作用在浏览器当中来校验xpath表达式</p>
</blockquote>
<ul>
<li>安装</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 更多工具</span><br><span class="line">2. 扩展程序</span><br><span class="line">3. 开启开发者模式</span><br><span class="line">4. XPath Helper 找到它 安装</span><br><span class="line">5. 快捷键</span><br><span class="line">    - 开启和关闭 xpath插件  ctrl + shift + x</span><br></pre></td></tr></table></figure>
<h4 id="段子网的段子内容和标题解析"><a href="#段子网的段子内容和标题解析" class="headerlink" title="段子网的段子内容和标题解析"></a>段子网的段子内容和标题解析</h4><ul>
<li><a href="https://ishuo.cn/joke" target="_blank" rel="noopener">https://ishuo.cn/joke</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">from lxml import etree</span><br><span class="line">import requests</span><br><span class="line"># 1. 指定url</span><br><span class="line">url=&apos;https://ishuo.cn/joke&apos;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"># 2 发请求</span><br><span class="line">response=requests.get(url,headers=headers).text</span><br><span class="line"># 3 获取页面内容</span><br><span class="line">page_text = response.text</span><br><span class="line">#4 解析</span><br><span class="line">tree=etree.HTML(page_text)</span><br><span class="line"># 获取所有li</span><br><span class="line">li_list=tree.xpath(&apos;//div[@id=&quot;list&quot;]/ul/li&apos;)</span><br><span class="line"># 注意 Element类型对象可以继续用xpath函数，该对象表示的局部内容进行指定内容的解析</span><br><span class="line"></span><br><span class="line">fp = open(&apos;./duanzi.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">for li in li_list:</span><br><span class="line">    content = li.xpath(&apos;./div[@class=&quot;content&quot;]/text()&apos;)[0]</span><br><span class="line">    title = li.xpath(&apos;./div[@class=&quot;info&quot;]/a/text()&apos;)[0]</span><br><span class="line">    # 5 持久化</span><br><span class="line">    fp.write(title+&apos;:&apos;+content+&apos;\n\n&apos;)</span><br><span class="line">    print(&apos;写入成功&apos;)</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/30/Py007-01-10xpath使用/" data-id="cklcb6bjm00j0wo91826fb3gs" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-09爬虫之数据解析以及re操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/30/Py007-01-09爬虫之数据解析以及re操作/" class="article-date">
  <time datetime="2018-11-30T14:27:28.000Z" itemprop="datePublished">2018-11-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/30/Py007-01-09爬虫之数据解析以及re操作/">Py007-01-09爬虫之数据解析以及re操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h3><ol>
<li>指定url</li>
<li>发请求</li>
<li>获取页面数据</li>
<li>数据解析</li>
<li>进行持久化</li>
</ol>
<blockquote>
<h4 id="三种数据解析方式"><a href="#三种数据解析方式" class="headerlink" title="三种数据解析方式"></a>三种数据解析方式</h4></blockquote>
<ul>
<li>正则</li>
<li>bs4</li>
<li>xpath</li>
</ul>
<h3 id="正则re模块"><a href="#正则re模块" class="headerlink" title="正则re模块"></a>正则re模块</h3><p>用法回顾</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">单字符：</span><br><span class="line">        . : 除换行以外所有字符</span><br><span class="line">        [] ：[aoe] [a-w] 匹配集合中任意一个字符</span><br><span class="line">        \d ：数字  [0-9]</span><br><span class="line">        \D : 非数字</span><br><span class="line">        \w ：数字、字母、下划线、中文</span><br><span class="line">        \W : 非\w</span><br><span class="line">        \s ：所有的空白字符包,括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。</span><br><span class="line">        \S : 非空白</span><br><span class="line">    数量修饰：</span><br><span class="line">        * : 任意多次  &gt;=0</span><br><span class="line">        + : 至少1次   &gt;=1</span><br><span class="line">        ? : 可有可无  0次或者1次</span><br><span class="line">        &#123;m&#125; ：固定m次 hello&#123;3,&#125;</span><br><span class="line">        &#123;m,&#125; ：至少m次</span><br><span class="line">        &#123;m,n&#125; ：m-n次</span><br><span class="line">    边界：</span><br><span class="line">        $ : 以某某结尾 </span><br><span class="line">        ^ : 以某某开头</span><br><span class="line">    分组：</span><br><span class="line">        (ab)  </span><br><span class="line">    贪婪模式： .*</span><br><span class="line">    非贪婪（惰性）模式： .*?</span><br><span class="line"></span><br><span class="line">    re.I : 忽略大小写</span><br><span class="line">    re.M ：多行匹配</span><br><span class="line">    re.S ：单行匹配</span><br><span class="line"></span><br><span class="line">    re.sub(正则表达式, 替换内容, 字符串)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>代码示例</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line"># 提取python</span><br><span class="line">key = &apos;javapythonc++php&apos;</span><br><span class="line">res =re.findall(&apos;python&apos;,key)</span><br><span class="line">print(res) # [&apos;python&apos;]</span><br><span class="line"></span><br><span class="line"># 提取hello world</span><br><span class="line">key = &apos;&lt;html&gt;&lt;h1&gt;hello world&lt;/h1&gt;&lt;/html&gt;&apos;</span><br><span class="line">res =re.findall(&apos;&lt;h1&gt;(hello world)&lt;/h1&gt;&apos;,key)</span><br><span class="line">print(res) # [&apos;hello world&apos;]</span><br><span class="line"></span><br><span class="line"># 提取170</span><br><span class="line">key = &apos;我喜欢身高170的女孩&apos;</span><br><span class="line">res =re.findall(&apos;\d+&apos;,key)</span><br><span class="line">print(res) # [&apos;170&apos;]</span><br><span class="line"></span><br><span class="line"># 提取 http 和 https</span><br><span class="line">key = &apos;http://www.baidu.com and https://boob.com&apos;</span><br><span class="line">res =re.findall(&apos;https?&apos;,key)</span><br><span class="line">res2 = re.findall(&apos;https&#123;0,1&#125;&apos;,key)</span><br><span class="line">print(res,res2) # [&apos;http&apos;, &apos;https&apos;] [&apos;http&apos;, &apos;https&apos;]</span><br><span class="line"></span><br><span class="line"># 提取hit.</span><br><span class="line">key = &apos;bobo@hit.edu.com&apos;</span><br><span class="line">res =re.findall(&apos;h.*\.&apos;,key) # [&apos;hit.edu.&apos;]</span><br><span class="line">print(res) # [&apos;hit.edu.&apos;] 贪婪模式（默认）尽可能多的提取数据</span><br><span class="line">res2 =re.findall(&apos;h.*?\.&apos;,key)</span><br><span class="line">print(res2) # [&apos;hit.&apos;]</span><br><span class="line"></span><br><span class="line"># 提取 sas saas</span><br><span class="line">key = &apos;saas and sas and saaas&apos;</span><br><span class="line">res =re.findall(&apos;sa&#123;1,2&#125;s&apos;,key)</span><br><span class="line">print(res) # [&apos;saas&apos;, &apos;sas&apos;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 提取i开头的行</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">re.S 基于单行</span><br><span class="line">re.M 基于多行</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">key = &apos;&apos;&apos;fall in love with you</span><br><span class="line">i love you very much</span><br><span class="line">i love she</span><br><span class="line">i love her&apos;&apos;&apos;</span><br><span class="line">res = re.findall(&apos;^i.*&apos;,key,re.M)</span><br><span class="line">print(res)</span><br><span class="line"># [&apos;i love you very much&apos;, &apos;i love she&apos;, &apos;i love her&apos;]</span><br><span class="line"></span><br><span class="line"># 匹配所有全部行</span><br><span class="line">key = &apos;&apos;&apos;&lt;div&gt;静夜思</span><br><span class="line">窗前明月光</span><br><span class="line">疑是地上霜</span><br><span class="line">举头望明月</span><br><span class="line">低头思故乡</span><br><span class="line">&lt;/div&gt;&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">此时key是多行</span><br><span class="line">但是匹配的时候想要当作一行处理</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">res = re.findall(&apos;&lt;div&gt;.*&lt;/div&gt;&apos;,key,re.S)</span><br><span class="line">print(res)</span><br><span class="line"># [&apos;&lt;div&gt;静夜思\n窗前明月光\n疑是地上霜\n举头望明月\n低头思故乡\n&lt;/div&gt;&apos;]</span><br></pre></td></tr></table></figure>
<h3 id="需求：用正则对糗百的（糗图）图片数据进行解析和下载"><a href="#需求：用正则对糗百的（糗图）图片数据进行解析和下载" class="headerlink" title="需求：用正则对糗百的（糗图）图片数据进行解析和下载"></a>需求：用正则对糗百的（糗图）图片数据进行解析和下载</h3><ul>
<li><a href="https://www.qiushibaike.com/pic/" target="_blank" rel="noopener">https://www.qiushibaike.com/pic/</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line">import os</span><br><span class="line">#1指定url</span><br><span class="line">url = &apos;https://www.qiushibaike.com/pic/&apos;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#2发请求</span><br><span class="line">response = requests.get(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">#3获取页面数据</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">#4数据解析</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">&lt;div class=&quot;thumb&quot;&gt;</span><br><span class="line">    &lt;a href=&quot;/article/121304175&quot; target=&quot;_blank&quot;&gt;</span><br><span class="line">        &lt;img src=&quot;//pic.qiushibaike.com/system/pictures/12130/121304175/medium/7RG0RFB30I0KWFMI.jpg&quot; alt=&quot;要把下面烫卷&quot;&gt;</span><br><span class="line">    &lt;/a&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 该列表中存储的就是当前页面源码中所有图片的url</span><br><span class="line">img_arr = re.findall(&apos;&lt;div class=&quot;thumb&quot;&gt;.*?&lt;img src=&quot;(.*?)&quot;.*?&gt;.*?&lt;/div&gt;&apos;,page_text,re.S)</span><br><span class="line"></span><br><span class="line"># 创建存储图片数据的文件夹</span><br><span class="line">if not os.path.exists(&apos;./imgs&apos;):</span><br><span class="line">    os.mkdir(&apos;imgs&apos;)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 发现是不含协议部分的</span><br><span class="line">[&apos;//pic.qiushibaike.com/system/pictures/12130/121304185/medium/6X8SQWYH0BBUO1KV.jpg&apos;,....]&apos;&apos;&apos;</span><br><span class="line"># 添加上协议</span><br><span class="line">for url in img_arr:</span><br><span class="line">    img_url = &apos;https:&apos; + url</span><br><span class="line">    # 持久化存储：存储的是图片的数据而不是url</span><br><span class="line">    # 获取图片二进制数据值</span><br><span class="line">    img_data = requests.get(url=img_url,headers=headers).content</span><br><span class="line"></span><br><span class="line">    # 将 6X8SQWYH0BBUO1KV.jpg 当做图片的名字</span><br><span class="line">    img_name = url.split(&apos;/&apos;)[-1]</span><br><span class="line">    img_path = &apos;imgs/&apos; + img_name</span><br><span class="line">    with open(img_path,&apos;wb&apos;) as f:</span><br><span class="line">        f.write(img_data)</span><br><span class="line">        print(img_name+&apos;写入成功&apos;)</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/30/Py007-01-09爬虫之数据解析以及re操作/" data-id="cklcb6bjm00iywo91wjjk62ib" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-08requests处理验证码" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/30/Py007-01-08requests处理验证码/" class="article-date">
  <time datetime="2018-11-30T14:23:15.000Z" itemprop="datePublished">2018-11-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/30/Py007-01-08requests处理验证码/">Py007-01-08requests处理验证码</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="待更新"><a href="#待更新" class="headerlink" title="待更新"></a>待更新</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/30/Py007-01-08requests处理验证码/" data-id="cklcb6bjl00iwwo914cylg0eh" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-07requests高级操作cookie和代理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/30/Py007-01-07requests高级操作cookie和代理/" class="article-date">
  <time datetime="2018-11-30T13:27:14.000Z" itemprop="datePublished">2018-11-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/30/Py007-01-07requests高级操作cookie和代理/">Py007-01-07requests高级操作cookie和代理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="requests模块高级："><a href="#requests模块高级：" class="headerlink" title="requests模块高级："></a>requests模块高级：</h3><h4 id="cookie："><a href="#cookie：" class="headerlink" title="cookie："></a>cookie：</h4><blockquote>
<p>基于用户的用户数据</p>
<ul>
<li>需求：爬取张三用户的豆瓣网的个人主页页面数据</li>
</ul>
</blockquote>
<ul>
<li>在没有登录的情况下浏览器访问 <a href="https://www.douban.com/people/185687620/" target="_blank" rel="noopener">https://www.douban.com/people/185687620/</a></li>
<li>发现不是对应主页信息而是要求你登录的页面</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#问题：没有获取个人主页的页面数据</span><br><span class="line">#原因：爬虫程序没有严格遵从浏览器的请求流程</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://www.douban.com/people/185687620/&apos;</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">    &#125;</span><br><span class="line">#2.发起请求</span><br><span class="line">response = requests.get(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">#3.获取页面数据</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">#4.持久化存储</span><br><span class="line">with open(&apos;./douban.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(page_text)</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="无法访问对应页码的成因"><a href="#无法访问对应页码的成因" class="headerlink" title="无法访问对应页码的成因"></a>无法访问对应页码的成因</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">结果发现，写入到文件中的数据，不是张三个人页面的数据，而是豆瓣登陆的首页面，why？首先我们来回顾下cookie的相关概念及作用：</span><br></pre></td></tr></table></figure>
<ul>
<li><p>cookie概念：当用户通过浏览器首次访问一个域名时，访问的web服务器会给客户端发送数据，以保持web服务器与客户端之间的状态保持，这些数据就是cookie。</p>
</li>
<li><p>cookie作用：我们在浏览器中，经常涉及到数据的交换，比如你登录邮箱，登录一个页面。我们经常会在此时设置30天内记住我，或者自动登录选项。那么它们是怎么记录信息的呢，答案就是今天的主角cookie了，Cookie是由HTTP服务器设置的，保存在浏览器中，但HTTP协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。就像我们去超市买东西，没有积分卡的情况下，我们买完东西之后，超市没有我们的任何消费信息，但我们办了积分卡之后，超市就有了我们的消费信息。cookie就像是积分卡，可以保存积分，商品就是我们的信息，超市的系统就像服务器后台，http协议就是交易的过程。</p>
</li>
<li><p>经过cookie的相关介绍，其实你已经知道了为什么上述案例中爬取到的不是张三个人信息页，而是登录页面。那应该如何抓取到张三的个人信息页呢？</p>
</li>
</ul>
<blockquote>
<p>思路：</p>
</blockquote>
<ol>
<li>我们需要使用爬虫程序对豆瓣的登录时的请求进行一次抓取，获取请求中的cookie数据</li>
<li>在使用个人信息页的url进行请求时，该请求需要携带 1 中的cookie，只有携带了cookie后，服务器才可识别这次请求的用户信息，方可响应回指定的用户信息页数据</li>
</ol>
<h4 id="cookie作用：服务器端使用cookie来记录客户端的状态信息。"><a href="#cookie作用：服务器端使用cookie来记录客户端的状态信息。" class="headerlink" title="cookie作用：服务器端使用cookie来记录客户端的状态信息。"></a>cookie作用：服务器端使用cookie来记录客户端的状态信息。</h4><blockquote>
<p>实现流程：</p>
</blockquote>
<ol>
<li>执行登录操作（获取cookie）</li>
<li>在发起个人主页请求时，需要将cookie携带到该请求中</li>
</ol>
<p>注意：session对象：发送请求（会将cookie对象进行自动存储）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">session = requests.session()</span><br><span class="line">#1.发起登录请求：将cookie获取，切存储到session对象中</span><br><span class="line">login_url = &apos;https://accounts.douban.com/login&apos;</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;source&quot;: &quot;None&quot;,</span><br><span class="line">    &quot;redir&quot;: &quot;https://www.douban.com/people/185687620/&quot;,</span><br><span class="line">    &quot;form_email&quot;: &quot;15027900535&quot;,</span><br><span class="line">    &quot;form_password&quot;: &quot;bobo@15027900535&quot;,</span><br><span class="line">    &quot;login&quot;: &quot;登录&quot;,</span><br><span class="line">&#125;</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line">#使用session发起post请求</span><br><span class="line">login_response = session.post(url=login_url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line">#2.对个人主页发起请求（session（cookie）），获取响应页面数据</span><br><span class="line">url = &apos;https://www.douban.com/people/185687620/&apos;</span><br><span class="line">response = session.get(url=url,headers=headers)</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">with open(&apos;./douban110.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(page_text)</span><br></pre></td></tr></table></figure>
<h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><blockquote>
<p>基于requests模块的代理操作</p>
</blockquote>
<ul>
<li>什么是代理</li>
</ul>
<p>代理就是第三方代替本体处理相关事务。例如：生活中的代理：代购，中介，微商……</p>
<ul>
<li>爬虫中为什么需要使用代理</li>
</ul>
<p>一些网站会有相应的反爬虫措施，例如很多网站会检测某一段时间某个IP的访问次数，如果访问频率太快以至于看起来不像正常访客，它可能就会会禁止这个IP的访问。所以我们需要设置一些代理IP，每隔一段时间换一个代理IP，就算IP被禁止，依然可以换个IP继续爬取。</p>
<ul>
<li>代理的分类：</li>
</ul>
<p>正向代理：代理客户端获取数据。正向代理是为了保护客户端防止被追究责任。</p>
<p>反向代理：代理服务器提供数据。反向代理是为了保护服务器或负责负载均衡。</p>
<ul>
<li>免费代理ip提供网站</li>
</ul>
<ol>
<li><p><a href="http://www.goubanjia.com/" target="_blank" rel="noopener">http://www.goubanjia.com/</a> (推荐)</p>
</li>
<li><p>西祠代理</p>
</li>
<li><p>快代理</p>
</li>
</ol>
<blockquote>
<p>代理实例</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.baidu.com/s?ie=utf-8&amp;wd=ip&apos;</span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#将代理ip封装到字典</span><br><span class="line">proxy = &#123;</span><br><span class="line">    &apos;http&apos;:&apos;77.73.69.120:3128&apos;,</span><br><span class="line">   # &apos;https&apos;:&apos;xxxx&apos; 也可以是https</span><br><span class="line">&#125;</span><br><span class="line">#更换网路IP</span><br><span class="line">response = requests.get(url=url,proxies=proxy,headers=headers)</span><br><span class="line"></span><br><span class="line">with open(&apos;./daili.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(response.text)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意点</p>
</blockquote>
<ul>
<li>请求的协议要与代理ip的协议统一</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/30/Py007-01-07requests高级操作cookie和代理/" data-id="cklcb6bjk00iuwo914qtoe8k7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-06requests综合实战" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/30/Py007-01-06requests综合实战/" class="article-date">
  <time datetime="2018-11-30T13:08:18.000Z" itemprop="datePublished">2018-11-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/30/Py007-01-06requests综合实战/">Py007-01-06requests综合实战</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="综合项目实战"><a href="#综合项目实战" class="headerlink" title="综合项目实战"></a>综合项目实战</h3><ul>
<li>需求：爬取搜狗知乎某一个词条对应一定范围页码表示的页面数据</li>
</ul>
<blockquote>
<p>分析网址</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 原始url</span><br><span class="line">baseUrl = &apos;https://zhihu.sogou.com/zhihu?query=%E9%92%89%E9%92%89&amp;ie=utf8&amp;w=&amp;oq=dd&amp;ri=1&amp;sourceid=sugg&amp;stj=1%3B0%3B0%3B0&amp;stj2=0&amp;stj0=1&amp;stj1=0&amp;hp=0&amp;hp1=&amp;sut=1909&amp;sst0=1543542920653&amp;lkt=3%2C1543542918642%2C1543542920549&apos;</span><br><span class="line"></span><br><span class="line"># 移除get参数的url</span><br><span class="line">url2 = &apos;https://zhihu.sogou.com/zhihu&apos;</span><br><span class="line"></span><br><span class="line"># get 参数 = &apos;query=%E9%92%89%E9%92%89&amp;ie=utf8&amp;w=&amp;oq=dd&amp;ri=1&amp;sourceid=sugg&amp;stj=1%3B0%3B0%3B0&amp;stj2=0&amp;stj0=1&amp;stj1=0&amp;hp=0&amp;hp1=&amp;sut=1909&amp;sst0=1543542920653&amp;lkt=3%2C1543542918642%2C1543542920549&apos;</span><br><span class="line">params = &#123;</span><br><span class="line">    query: 钉钉</span><br><span class="line">    ie: utf8</span><br><span class="line">    w: </span><br><span class="line">    oq: dd</span><br><span class="line">    ri: 1</span><br><span class="line">    sourceid: sugg</span><br><span class="line">    stj: 1;0;0;0</span><br><span class="line">    stj2: 0</span><br><span class="line">    stj0: 1</span><br><span class="line">    stj1: 0</span><br><span class="line">    hp: 0</span><br><span class="line">    hp1: </span><br><span class="line">    sut: 1909</span><br><span class="line">    sst0: 1543542920653</span><br><span class="line">    lkt: 3,1543542918642,1543542920549</span><br><span class="line">&#125;</span><br><span class="line"># 移除无用参数</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;query&apos;: 搜索词,</span><br><span class="line">    &apos;page&apos;: 页码,</span><br><span class="line">    &apos;ie&apos;: &apos;utf-8&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>代码实现</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># 前三页页面数据（1，2，3）</span><br><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 创建一个文件夹</span><br><span class="line">if not os.path.exists(&apos;./pages&apos;):</span><br><span class="line">    os.mkdir(&apos;./pages&apos;)</span><br><span class="line"></span><br><span class="line">word = input(&apos;enter a word:&apos;)</span><br><span class="line"></span><br><span class="line"># 动态指定页码的范围</span><br><span class="line">start_pageNum = int(input(&apos;enter a start pageNum:&apos;))</span><br><span class="line">end_pageNum = int(input(&apos;enter a end pageNum:&apos;))</span><br><span class="line"># 自定义请求头信息</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"># 1.指定url:设计成一个具有通用的url</span><br><span class="line"></span><br><span class="line"># 请求url</span><br><span class="line">url = &apos;https://zhihu.sogou.com/zhihu&apos;</span><br><span class="line">for page in range(start_pageNum, end_pageNum + 1):</span><br><span class="line">    param = &#123;</span><br><span class="line">        &apos;query&apos;: word,</span><br><span class="line">        &apos;page&apos;: page,</span><br><span class="line">        &apos;ie&apos;: &apos;utf-8&apos;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url=url, params=param, headers=headers)</span><br><span class="line"></span><br><span class="line">    # 获取响应中的页面数据（指定页码（page））</span><br><span class="line">    page_text = response.text</span><br><span class="line"></span><br><span class="line">    # 进行持久化存储</span><br><span class="line">    fileName = word + str(page) + &apos;.html&apos;</span><br><span class="line">    filePath = &apos;pages/&apos; + fileName</span><br><span class="line">    with open(filePath, &apos;w&apos;, encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line">        print(&apos;第%d页数据写入成功&apos; % page)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/30/Py007-01-06requests综合实战/" data-id="cklcb6bjj00iqwo91x4m0np77" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-05requests模块之post和ajax" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/29/Py007-01-05requests模块之post和ajax/" class="article-date">
  <time datetime="2018-11-29T15:52:05.000Z" itemprop="datePublished">2018-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/29/Py007-01-05requests模块之post和ajax/">Py007-01-05requests模块之post和ajax</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="requests模块之post请求"><a href="#requests模块之post请求" class="headerlink" title="requests模块之post请求"></a>requests模块之post请求</h3><blockquote>
<p>简单需求，登录豆瓣网，获取成功后的数据</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 1.post请求的url</span><br><span class="line">url = &apos;https://accounts.douban.com/login&apos;</span><br><span class="line"></span><br><span class="line"># 封装post请求参数数据</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;source&quot;:&apos;movie&apos;,</span><br><span class="line">    &apos;redir&apos;:&apos;https://movie.douban.com/&apos;,</span><br><span class="line">    &apos;form_email&apos;:&apos;15027900535&apos;,</span><br><span class="line">    &apos;form_password&apos;:&apos;bobo@15027900535&apos;,</span><br><span class="line">    &apos;login&apos;:&apos;登录&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"># 2 发起post请求</span><br><span class="line">response = requests.post(url=url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line"># 3 获取响应对象中的页面数据</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line"># 4 持久化存储</span><br><span class="line">with open(&apos;movie.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">    f.write(page_text)</span><br></pre></td></tr></table></figure>
<h3 id="ajax的请求"><a href="#ajax的请求" class="headerlink" title="ajax的请求"></a>ajax的请求</h3><h4 id="基于ajax的get请求"><a href="#基于ajax的get请求" class="headerlink" title="基于ajax的get请求"></a>基于ajax的get请求</h4><blockquote>
<p>抓取豆瓣电影首页-排行榜-某一分类的电影的下拉加载数据</p>
</blockquote>
<ul>
<li>排行榜 <a href="https://movie.douban.com/chart" target="_blank" rel="noopener">https://movie.douban.com/chart</a></li>
<li>喜剧分类 <a href="https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&amp;type=24&amp;interval_id=100:90&amp;action=" target="_blank" rel="noopener">https://movie.douban.com/typerank?type_name=%E5%96%9C%E5%89%A7&amp;type=24&amp;interval_id=100:90&amp;action=</a></li>
<li>下拉加载的动态数据接口 <a href="https://movie.douban.com/j/chart/top_list?type=24&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20" target="_blank" rel="noopener">https://movie.douban.com/j/chart/top_list?type=24&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 原始url</span><br><span class="line"># baseurl = &apos;https://movie.douban.com/j/chart/top_list?type=24&amp;interval_id=100%3A90&amp;action=&amp;start=40&amp;limit=20&apos;</span><br><span class="line"></span><br><span class="line"># 指定url</span><br><span class="line">url = &apos;https://movie.douban.com/j/chart/top_list&apos;</span><br><span class="line"></span><br><span class="line"># 封装ajax中  get参数</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;type&apos;:24,</span><br><span class="line">    &apos;interval_id&apos;:&apos;100:90&apos;,</span><br><span class="line">    &apos;action&apos;:&apos;&apos;,</span><br><span class="line">    &apos;start&apos;:40,</span><br><span class="line">    &apos;limit&apos;:20</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params,headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<h4 id="基于ajax的post请求"><a href="#基于ajax的post请求" class="headerlink" title="基于ajax的post请求"></a>基于ajax的post请求</h4><p>获取肯德基城市餐厅位置数据</p>
<ul>
<li>肯德基官网 <a href="http://www.kfc.com.cn/kfccda/index.aspx" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/index.aspx</a></li>
<li>餐厅位置查询页面 <a href="http://www.kfc.com.cn/kfccda/storelist/index.aspx" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/storelist/index.aspx</a></li>
<li>搜索查询 北京 餐厅列表  看浏览器里的network里的抓包信息 获得url = <a href="http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword" target="_blank" rel="noopener">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword</a></li>
<li>餐厅列表有分页但是是  post的 继续分析他的请求参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword</span><br><span class="line"></span><br><span class="line">参数</span><br><span class="line">cname: </span><br><span class="line">pid: </span><br><span class="line">keyword: 上海</span><br><span class="line">pageIndex: 2</span><br><span class="line">pageSize: 10</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 封装ajax中  post参数</span><br><span class="line">data = &#123;</span><br><span class="line">    &apos;cname&apos;:&apos;&apos;,</span><br><span class="line">    &apos;pid&apos;:&apos;&apos;,</span><br><span class="line">    &apos;keyword&apos;: &apos;上海&apos;,</span><br><span class="line">    &apos;pageIndex&apos;:2,</span><br><span class="line">    &apos;pageSize&apos;:10</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url=url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结论</p>
</blockquote>
<ul>
<li>如果使用requests发起ajax请求 跟普通get/post请求是一样的</li>
<li>无法通过地址栏请求，通过浏览器开发者工具抓包信息获取请求的url</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/29/Py007-01-05requests模块之post和ajax/" data-id="cklcb6bjj00iswo91z05g5cjb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-04requests模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/29/Py007-01-04requests模块/" class="article-date">
  <time datetime="2018-11-29T15:09:15.000Z" itemprop="datePublished">2018-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/29/Py007-01-04requests模块/">Py007-01-04requests模块</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="requests模块"><a href="#requests模块" class="headerlink" title="requests模块"></a>requests模块</h3><ul>
<li>什么是requests模块</li>
</ul>
<p>requests模块是python中原生的基于网络请求的模块，其主要作用是用来模拟浏览器发起请求。功能强大，用法简洁高效。在爬虫领域中占据着半壁江山的地位。</p>
<ul>
<li>为什么要使用requests模块</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt; 因为在使用urllib模块的时候，会有诸多不便之处，总结如下：</span><br><span class="line">    1 手动处理url编码</span><br><span class="line">    2 手动处理post请求参数</span><br><span class="line">    3 处理cookie和代理操作繁琐</span><br><span class="line">        &gt; cookie</span><br><span class="line">        - 创建一个cookiejar对象</span><br><span class="line">        - 创建一个handler对象</span><br><span class="line">        - 创建一个opener对象</span><br><span class="line"></span><br><span class="line">        &gt; 代理</span><br><span class="line">        - 创建handler对象，代理ip和端口封装到该对象</span><br><span class="line">        - 创建opener对象</span><br><span class="line"></span><br><span class="line">&gt; 使用requests模块：</span><br><span class="line">    - 自动处理url编码</span><br><span class="line">    - 自动处理post请求参数</span><br><span class="line">    - 大大简化cookie和代理操作</span><br><span class="line">    。。。</span><br></pre></td></tr></table></figure>
<h4 id="requests如何使用"><a href="#requests如何使用" class="headerlink" title="requests如何使用"></a>requests如何使用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 安装</span><br><span class="line">pip install requests</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用流程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 指定url</span><br><span class="line">2. 使用requests模块发请求</span><br><span class="line">3. 获取响应数据</span><br><span class="line">4. 进行持久化存储</span><br></pre></td></tr></table></figure>
<h4 id="通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固"><a href="#通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固" class="headerlink" title="通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固"></a>通过5个基于requests模块的爬虫项目对该模块进行系统的学习和巩固</h4><ul>
<li>get</li>
<li>post</li>
<li>ajax的get</li>
<li>ajax的post</li>
<li>综合</li>
</ul>
<h4 id="get请求"><a href="#get请求" class="headerlink" title="get请求"></a>get请求</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 指定url</span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line"># 发get请求,get方法会返回请求成功的响应内容</span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"></span><br><span class="line"># 获取响应中的数据值：text可以获取响应对象中字符串形式的页面数据</span><br><span class="line">page_data = response.text</span><br><span class="line"></span><br><span class="line">print(page_data)</span><br><span class="line"></span><br><span class="line"># 持久化存储</span><br><span class="line">with open(&apos;./sogou.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">    fp.write(page_data)</span><br></pre></td></tr></table></figure>
<h4 id="requests其他常用属性"><a href="#requests其他常用属性" class="headerlink" title="requests其他常用属性"></a>requests其他常用属性</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"></span><br><span class="line">page_data = response.text</span><br><span class="line"># 获取响应对象中二进制(byte)类型的数据</span><br><span class="line">print(response.content)</span><br><span class="line"></span><br><span class="line"># 返回响应的状态码 </span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"># 获取响应的头信息(字典的形式)</span><br><span class="line">print(response.headers)</span><br><span class="line"></span><br><span class="line"># 获取请求的url</span><br><span class="line">print(response.url)</span><br></pre></td></tr></table></figure>
<h4 id="request携带参数的get请求方式一"><a href="#request携带参数的get请求方式一" class="headerlink" title="request携带参数的get请求方式一"></a>request携带参数的get请求方式一</h4><blockquote>
<p>获取搜狗搜索结果对应的界面数据爬取 </p>
</blockquote>
<p>获取周杰伦对应的搜狗页面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.sogou.com/web?query=周杰伦&amp;ie=utf-8&apos;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url)</span><br><span class="line"></span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">with open(&apos;./zhou.html&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as f:</span><br><span class="line">    f.write(page_text)</span><br></pre></td></tr></table></figure>
<p>此时url里的中文参数无需手动处理(urllib需要手动处理)</p>
<h4 id="request携带参数的get请求方式二"><a href="#request携带参数的get请求方式二" class="headerlink" title="request携带参数的get请求方式二"></a>request携带参数的get请求方式二</h4><ul>
<li>params参数(字典的形势传递get参数)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># get请求方式二</span><br><span class="line">url = &apos;https://www.sogou.com/web&apos;</span><br><span class="line"></span><br><span class="line"># 将参数封装到字典里</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;,</span><br><span class="line">    &apos;ie&apos;:&apos;utf-8&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params)</span><br><span class="line"></span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">print(page_text)</span><br></pre></td></tr></table></figure>
<h4 id="requests之自定义请求头信息"><a href="#requests之自定义请求头信息" class="headerlink" title="requests之自定义请求头信息"></a>requests之自定义请求头信息</h4><ul>
<li>headers 请求头参数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 将参数封装到字典里</span><br><span class="line">params = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;,</span><br><span class="line">    &apos;ie&apos;:&apos;utf-8&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url=url,params=params,headers=headers)</span><br><span class="line"></span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/29/Py007-01-04requests模块/" data-id="cklcb6bjh00ilwo916wt6hjj3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-03内置urllib模块补充" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/29/Py007-01-03内置urllib模块补充/" class="article-date">
  <time datetime="2018-11-29T14:59:14.000Z" itemprop="datePublished">2018-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/29/Py007-01-03内置urllib模块补充/">Py007-01-03内置urllib模块补充</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="urllib补充"><a href="#urllib补充" class="headerlink" title="urllib补充"></a>urllib补充</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 补充说明：</span><br><span class="line">urlopen函数原型：</span><br><span class="line">    urllib.request.urlopen(url, data=None, timeout=&lt;object object at 0x10af327d0&gt;, *, cafile=None, capath=None, cadefault=False, context=None)</span><br><span class="line"></span><br><span class="line">在上一节中我们只使用了该函数中的第一个参数url。在日常开发中，我们能用的只有url和data这两个参数。</span><br><span class="line"></span><br><span class="line">url参数：指定向哪个url发起请求</span><br><span class="line">data参数：可以将post请求中携带的参数封装成字典的形式传递给该参数</span><br><span class="line"></span><br><span class="line">urlopen函数返回的响应对象，相关函数调用介绍：</span><br><span class="line">response.headers()：获取响应头信息</span><br><span class="line">response.getcode()：获取响应状态码</span><br><span class="line">response.geturl()：获取请求的url</span><br><span class="line">response.read()：获取响应中的数据值（字节类型）</span><br></pre></td></tr></table></figure>
<h4 id="二进制数据的爬取"><a href="#二进制数据的爬取" class="headerlink" title="二进制数据的爬取"></a>二进制数据的爬取</h4><blockquote>
<p>爬取网络上的某张图片数据，且存储到磁盘</p>
</blockquote>
<ul>
<li>方法 1：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://pic.qiushibaike.com/system/pictures/12112/121121212/medium/ZOAND29U4NKNEWEF.jpg&apos;</span><br><span class="line">#2.发起请求:使用urlopen函数发起请求，该函数返回一个响应对象</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">#3.获取响应对象中的图片二进制类型的数据</span><br><span class="line">img_data = response.read()</span><br><span class="line">#4.持久化存储：将爬取的图片写入本地进行保存</span><br><span class="line">with open(&apos;./tupian.png&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(img_data)</span><br></pre></td></tr></table></figure>
<ul>
<li>方法 2：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">url = &apos;https://pic.qiushibaike.com/system/pictures/12112/121121212/medium/ZOAND29U4NKNEWEF.jpg&apos;</span><br><span class="line"># 函数原型：urllib.request.urlretrieve(url, filename=None)</span><br><span class="line"># 作用：对url发起请求，且将响应中的数据值写入磁盘进行存储</span><br><span class="line">urllib.request.urlretrieve(url=url,filename=&apos;./img.png&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="url的特性"><a href="#url的特性" class="headerlink" title="url的特性"></a>url的特性</h4><p>url必须为ASCII编码的数据值。所以我们在爬虫代码中编写url时，如果url中存在非ASCII编码的数据值，则必须对其进行ASCII编码后，该url方可被使用。</p>
<blockquote>
<p>案例：爬取使用搜狗根据指定词条搜索到的页面数据（例如爬取词条为‘周杰伦’的页面数据）</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?query=周杰伦&apos;</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<p>【注意】上述代码中url存在非ascii编码的数据，则该url无效。如果对其发起请求，则会报如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 15-17: ordinal not in range</span><br></pre></td></tr></table></figure>
<blockquote>
<p>所以必须对url中的非ascii的数据进行ascii的编码，则该url方可被发起请求：</p>
</blockquote>
<ul>
<li>方法 1：使用quote函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?query=%s&apos;</span><br><span class="line">#对url中的非ascii进行编码.quote函数可以对非ascii的数据值进行ascii的编码</span><br><span class="line">word = urllib.parse.quote(&apos;周杰伦&apos;)</span><br><span class="line">#将编码后的数据值拼接回url中</span><br><span class="line">url = format(url%word)</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<ul>
<li>方法2： 使用urlencode函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?&apos;</span><br><span class="line">#将get请求中url携带的参数封装至字典中</span><br><span class="line">param = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;</span><br><span class="line">&#125;</span><br><span class="line">#对url中的非ascii进行编码</span><br><span class="line">param = urllib.parse.urlencode(param)</span><br><span class="line">#将编码后的数据值拼接回url中</span><br><span class="line">url += param </span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦1.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="通过自定义请求对象，用于伪装爬虫程序请求的身份。"><a href="#通过自定义请求对象，用于伪装爬虫程序请求的身份。" class="headerlink" title="通过自定义请求对象，用于伪装爬虫程序请求的身份。"></a>通过自定义请求对象，用于伪装爬虫程序请求的身份。</h4><p>之前在讲解http常用请求头信息时，我们讲解过User-Agent参数，简称为UA，该参数的作用是用于表明本次请求载体的身份标识。如果我们通过浏览器发起的请求，则该请求的载体为当前浏览器，则UA参数的值表明的是当前浏览器的身份标识表示的一串数据。如果我们使用爬虫程序发起的一个请求，则该请求的载体为爬虫程序，那么该请求的UA为爬虫程序的身份标识表示的一串数据。有些网站会通过辨别请求的UA来判别该请求的载体是否为爬虫程序，如果为爬虫程序，则不会给该请求返回响应，那么我们的爬虫程序则也无法通过请求爬取到该网站中的数据值，这也是反爬虫的一种初级技术手段。那么为了防止该问题的出现，则我们可以给爬虫程序的UA进行伪装，伪装成某款浏览器的身份标识。</p>
<p>上述案例中，我们是通过request模块中的urlopen发起的请求，该请求对象为urllib中内置的默认请求对象，我们无法对其进行UA进行更改操作。urllib还为我们提供了一种自定义请求对象的方式，我们可以通过自定义请求对象的方式，给该请求对象中的UA进行伪装（更改）操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">url = &apos;https://www.sogou.com/web?&apos;</span><br><span class="line">#将get请求中url携带的参数封装至字典中</span><br><span class="line">param = &#123;</span><br><span class="line">    &apos;query&apos;:&apos;周杰伦&apos;</span><br><span class="line">&#125;</span><br><span class="line">#对url中的非ascii进行编码</span><br><span class="line">param = urllib.parse.urlencode(param)</span><br><span class="line">#将编码后的数据值拼接回url中</span><br><span class="line">url += param </span><br><span class="line"></span><br><span class="line">#封装自定义的请求头信息的字典：</span><br><span class="line">#将浏览器的UA数据获取，封装到一个字典中。该UA值可以通过抓包工具或者浏览器自带的开发者工具中获取某请求，从中获取UA的值</span><br><span class="line">#注意：在headers字典中可以封装任意的请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos; : &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos;</span><br><span class="line">    &#125;</span><br><span class="line">#自定义请求对象，可以在该请求对象中添加自定义的请求头信息</span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line">#使用自定义请求对象发起请求</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">data = response.read()</span><br><span class="line">with open(&apos;./周杰伦.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(data)</span><br><span class="line">print(&apos;写入文件完毕&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="携带参数的post请求"><a href="#携带参数的post请求" class="headerlink" title="携带参数的post请求"></a>携带参数的post请求</h4><blockquote>
<p>案例：百度翻译发起post请求</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">#通过抓包工具抓取post请求的url</span><br><span class="line">post_url=&apos;https://fanyi.baidu.com/sug&apos;</span><br><span class="line">#封装post请求参数</span><br><span class="line">data=&#123;</span><br><span class="line">    &quot;kw&quot;:&quot;dog&quot;</span><br><span class="line">&#125;</span><br><span class="line">data=urllib.parse.urlencode(data)</span><br><span class="line">#自定义请求头信息字典</span><br><span class="line">headers=&#123;</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#自定义请求对象，然后将封装好的post请求参数赋值给Requst方法的data参数。</span><br><span class="line">#data参数：用来存储post请求的参数</span><br><span class="line">request=urllib.request.Request(post_url,data=data.encode(),headers=headers)</span><br><span class="line">#自定义的请求对象中的参数（data必须为bytes类型）</span><br><span class="line">response=urllib.request.urlopen(request)</span><br><span class="line">response.read()</span><br></pre></td></tr></table></figure>
<h3 id="urllib模块的高级操作"><a href="#urllib模块的高级操作" class="headerlink" title="urllib模块的高级操作"></a>urllib模块的高级操作</h3><blockquote>
<ol>
<li>代理</li>
</ol>
</blockquote>
<ul>
<li><p>什么是代理：代理就是第三方代替本体处理相关事务。例如：生活中的代理：代购，中介，微商……</p>
</li>
<li><p>爬虫中为什么需要使用代理？</p>
<blockquote>
<p>一些网站会有相应的反爬虫措施，例如很多网站会检测某一段时间某个IP的访问次数，如果访问频率太快以至于看起来不像正常访客，它可能就会会禁止这个IP的访问。所以我们需要设置一些代理IP，每隔一段时间换一个代理IP，就算IP被禁止，依然可以换个IP继续爬取。</p>
</blockquote>
</li>
<li><p>代理的分类：</p>
</li>
</ul>
<p>正向代理：代理客户端获取数据。正向代理是为了保护客户端防止被追究责任。</p>
<p>反向代理：代理服务器提供数据。反向代理是为了保护服务器或负责负载均衡。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#1.创建处理器对象，在其内部封装代理ip和端口</span><br><span class="line">handler=urllib.request.ProxyHandler(proxies=&#123;&apos;http&apos;:&apos;95.172.58.224:52608&apos;&#125;)</span><br><span class="line">#2.创建opener对象，然后使用该对象发起一个请求</span><br><span class="line">opener=urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">url=&apos;http://www.baidu.com/s?ie=UTF-8&amp;wd=ip&apos;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url, headers=headers)</span><br><span class="line"></span><br><span class="line">#使用opener对象发起请求，该请求对应的ip即为我们设置的代理ip</span><br><span class="line">response = opener.open(request)</span><br><span class="line"></span><br><span class="line">with open(&apos;./daili.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(response.read())</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 2.cookie</p>
</blockquote>
<p>引言：有些时候，我们在使用爬虫程序去爬取一些用户相关信息的数据（爬取张三“人人网”个人主页数据）时，如果使用之前requests模块常规操作时，往往达不到我们想要的目的，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">#指定url</span><br><span class="line">url = &apos;http://www.renren.com/289676607/profile&apos;</span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">    &#125;</span><br><span class="line">#自定义请求对象</span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line">#发起请求</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">with open(&apos;./renren.html&apos;,&apos;w&apos;) as fp:</span><br><span class="line">    fp.write(response.read().decode())</span><br></pre></td></tr></table></figure>
<p>【注意】上述代码中，我们爬取到的是登录首页面，而不是张三的个人主页也面。why？首先我们来回顾下cookie的相关概念及作用</p>
<pre><code>- cookie概念：当用户通过浏览器首次访问一个域名时，访问的web服务器会给客户端发送数据，以保持web服务器与客户端之间的状态保持，这些数据就是cookie。

- cookie作用：我们在浏览器中，经常涉及到数据的交换，比如你登录邮箱，登录一个页面。我们经常会在此时设置30天内记住我，或者自动登录选项。那么它们是怎么记录信息的呢，答案就是今天的主角cookie了，Cookie是由HTTP服务器设置的，保存在浏览器中，但HTTP协议是一种无状态协议，在数据交换完毕后，服务器端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。就像我们去超市买东西，没有积分卡的情况下，我们买完东西之后，超市没有我们的任何消费信息，但我们办了积分卡之后，超市就有了我们的消费信息。cookie就像是积分卡，可以保存积分，商品就是我们的信息，超市的系统就像服务器后台，http协议就是交易的过程。 

- 经过cookie的相关介绍，其实你已经知道了为什么上述案例中爬取到的不是张三个人信息页，而是登录页面。那应该如何抓取到张三的个人信息页呢？
</code></pre><p>　　思路：</p>
<p>　　　　1.我们需要使用爬虫程序对人人网的登录时的请求进行一次抓取，获取请求中的cookie数据</p>
<p>　　　　2.在使用个人信息页的url进行请求时，该请求需要携带 1 中的cookie，只有携带了cookie后，服务器才可识别这次请求的用户信息，方可响应回指定的用户信息页数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cookiejar对象：</span><br><span class="line">    - 作用：自动保存请求中的cookie数据信息</span><br><span class="line">    - 注意：必须和handler和opener一起使用</span><br><span class="line">cookiejar使用流程：</span><br><span class="line">    - 创建一个cookiejar对象</span><br><span class="line">      import http.cookiejar</span><br><span class="line">      cj = http.cookiejar.CookieJar()</span><br><span class="line">    - 通过cookiejar创建一个handler</span><br><span class="line">      handler = urllib.request.HTTPCookieProcessor(cj)</span><br><span class="line">    - 根据handler创建一个opener</span><br><span class="line">      opener = urllib.request.build_opener(handler)</span><br><span class="line">    - 使用opener.open方法去发送请求，且将响应中的cookie存储到openner对象中，后续的请求如果使用openner发起，则请求中就会携带了cookie</span><br></pre></td></tr></table></figure>
<p>使用cookiejar实现爬取人人网个人主页页面数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#使用cookiejar实现人人网的登陆</span><br><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line">import http.cookiejar</span><br><span class="line">cj = http.cookiejar.CookieJar() #请求中的cookie会自动存储到cj对象中</span><br><span class="line">#创建处理器对象(携带cookiejar对象的)</span><br><span class="line">handler=urllib.request.HTTPCookieProcessor(cj)</span><br><span class="line">#创建opener对象 （携带cookiejar对象）</span><br><span class="line">opener=urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">#要让cookiejar获取请求中的cookie数据值</span><br><span class="line">url=&apos;http://www.renren.com/ajaxLogin/login?1=1&amp;uniqueTimestamp=201873958471&apos;</span><br><span class="line">#自定义一个请求对象，让该对象作为opener的open函数中的参数</span><br><span class="line">data=&#123;</span><br><span class="line">    &quot;email&quot;:&quot;www.zhangbowudi@qq.com&quot;,</span><br><span class="line">    &quot;icode&quot;:&quot;&quot;,</span><br><span class="line">    &quot;origURL&quot;:&quot;http://www.renren.com/home&quot;,</span><br><span class="line">    &quot;domain&quot;:&quot;renren.com&quot;,</span><br><span class="line">    &quot;key_id&quot;:&quot;1&quot;,</span><br><span class="line">    &quot;captcha_type&quot;:&quot;web_login&quot;,</span><br><span class="line">    &quot;password&quot;:&quot;40dc65b82edd06d064b54a0fc6d202d8a58c4cb3d2942062f0f7dd128511fb9b&quot;,</span><br><span class="line">    &quot;rkey&quot;:&quot;41b44b0d062d3ca23119bc8b58983104&quot;,</span><br><span class="line">  </span><br><span class="line"> &apos;f&apos;:&quot;https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3DpPKf2680yRLbbZMVdntJpyPGwrSk2BtpKlEaAuKFTsW%26wd%3D%26eqid%3Deee20f380002988c000000025b7cbb80&quot;</span><br><span class="line">&#125;</span><br><span class="line">data=urllib.parse.urlencode(data).encode()</span><br><span class="line">request=urllib.request.Request(url,data=data)</span><br><span class="line">opener.open(request)</span><br><span class="line"></span><br><span class="line">#获取当前用户的二级子页面</span><br><span class="line">s_url=&apos;http://www.renren.com/289676607/profile&apos;</span><br><span class="line">#该次请求中就携带了cookie</span><br><span class="line">resonse=opener.open(s_url)</span><br><span class="line"></span><br><span class="line">with open(&apos;./renren.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(resonse.read())</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/29/Py007-01-03内置urllib模块补充/" data-id="cklcb6bjh00inwo913h6nbwso" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Py007-01-02内置urllib模块" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/29/Py007-01-02内置urllib模块/" class="article-date">
  <time datetime="2018-11-29T14:58:37.000Z" itemprop="datePublished">2018-11-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/29/Py007-01-02内置urllib模块/">Py007-01-02内置urllib模块</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>urllib模块非爬虫的重点，核心为后面的request以及各种框架</p>
</blockquote>
<h3 id="内置urllib"><a href="#内置urllib" class="headerlink" title="内置urllib"></a>内置urllib</h3><p>概念：urllib是Python自带的一个用于爬虫的库，其主要作用就是可以通过代码模拟浏览器发送请求。其常被用到的子模块在Python3中的为urllib.request和urllib.parse，在Python2中是urllib和urllib2。</p>
<ul>
<li>作用：可以使用代码模拟浏览器发起请求。request  parse</li>
</ul>
<blockquote>
<p>使用流程：</p>
</blockquote>
<ol>
<li>指定url</li>
<li>发请求</li>
<li>获取页面数据</li>
<li>持久化存储</li>
</ol>
<h4 id="需求1爬取搜狗首页的页面数据"><a href="#需求1爬取搜狗首页的页面数据" class="headerlink" title="需求1爬取搜狗首页的页面数据"></a>需求1爬取搜狗首页的页面数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://www.sogou.com/&apos;</span><br><span class="line"></span><br><span class="line">#2.发起请求:urlopen可以根据指定的url发起请求，切返回一个响应对象</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line"></span><br><span class="line">#3.获取页面数据:read函数返回的就是响应对象中存储的页面数据(byte)</span><br><span class="line">page_text = response.read()</span><br><span class="line"></span><br><span class="line">#4.持久化存储</span><br><span class="line">with open(&apos;./sougou.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    # fp.write(page_text)</span><br><span class="line">    # print(page_text) # 字节码</span><br><span class="line">    print(page_text.decode()) # 转化为字符串</span><br><span class="line">    fp.write(page_text.decode())#使用decode将page_text转成字符串类型</span><br><span class="line">    print(&apos;写入数据成功&apos;)</span><br></pre></td></tr></table></figure>
<h4 id="需求2搜狗搜关键字"><a href="#需求2搜狗搜关键字" class="headerlink" title="需求2搜狗搜关键字"></a>需求2搜狗搜关键字</h4><blockquote>
<p>注意：</p>
</blockquote>
<ul>
<li>urllib模块的get参数如果有中文要自行转义，否则报错UnicodeEncodeError</li>
<li>使用urllib.parse.quote(“中文参数”) 转义中文</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 需求：爬取指定词条所对应的页面数据</span><br><span class="line"></span><br><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#指定url</span><br><span class="line">url = &apos;https://www.sogou.com/web?query=&apos;</span><br><span class="line">#url特性：url不可以存在非ASCII编码的字符数据</span><br><span class="line">word = urllib.parse.quote(&quot;人民币&quot;)</span><br><span class="line">url += word #有效的url</span><br><span class="line"></span><br><span class="line">#发请求</span><br><span class="line">response = urllib.request.urlopen(url=url)</span><br><span class="line"></span><br><span class="line">#获取页面数据</span><br><span class="line">page_text = response.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with open(&apos;renminbi.html&apos;,&apos;wb&apos;) as fp:</span><br><span class="line">    fp.write(page_text)</span><br><span class="line">    </span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">如果</span><br><span class="line">word=&apos;人民币&apos;</span><br><span class="line">不经过转义则直接报错</span><br><span class="line">UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 15-17: ordinal not in range(128)</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<p>是不是很麻烦如果多个参数就要多次处理？</p>
<h4 id="使用UA来伪装"><a href="#使用UA来伪装" class="headerlink" title="使用UA来伪装"></a>使用UA来伪装</h4><ul>
<li>反爬机制：网站检查请求的UA，如果发现UA是爬虫程序，则拒绝提供网站数据。</li>
<li>User-Agent(UA)：请求载体的身份标识.</li>
<li>反反爬机制：伪装爬虫程序请求的UA</li>
</ul>
<blockquote>
<p>爬去百度首页</p>
</blockquote>
<ul>
<li>如果不用UA伪装，爬去的页面是不对的(百度已经做了反爬处理)</li>
<li>获取UA打开浏览器</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.开发者工具</span><br><span class="line">2.检查</span><br><span class="line">3.Network</span><br><span class="line">4.访问百度</span><br><span class="line">5.找到Request Headers的请求信息里的User-Agent字符串</span><br><span class="line">形如</span><br><span class="line">Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">url = &apos;https://www.baidu.com/&apos;</span><br><span class="line"></span><br><span class="line">#UA伪装</span><br><span class="line">#1.自定制一个请求对象</span><br><span class="line">headers = &#123;</span><br><span class="line">    #存储任意的请求头信息</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.67 Safari/537.36&apos;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line">#该请求对象的UA进行了成功的伪装</span><br><span class="line">request = urllib.request.Request(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">#2.针对自定制的请求对象发起请求</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"></span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>
<h4 id="使用urllib发起post请求"><a href="#使用urllib发起post请求" class="headerlink" title="使用urllib发起post请求"></a>使用urllib发起post请求</h4><blockquote>
<p>需求：爬取百度翻译的翻译结果</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">#1.指定url</span><br><span class="line">url = &apos;https://fanyi.baidu.com/sug&apos;</span><br><span class="line"></span><br><span class="line">#post请求携带的参数进行处理  流程：</span><br><span class="line">#1.将post请求参数封装到字典</span><br><span class="line">data = &#123;</span><br><span class="line">    &apos;kw&apos;:&apos;西瓜&apos;</span><br><span class="line">&#125;</span><br><span class="line">#2.使用parse模块中的urlencode(返回值类型为str)进行编码处理</span><br><span class="line">data = urllib.parse.urlencode(data)</span><br><span class="line">#3.将步骤2的编码结果转换成byte类型</span><br><span class="line">data = data.encode()</span><br><span class="line"></span><br><span class="line">#2.发起post请求:urlopen函数的data参数表示的就是经过处理之后的post请求携带的参数</span><br><span class="line">response = urllib.request.urlopen(url=url,data=data)</span><br><span class="line"></span><br><span class="line">response.read()</span><br><span class="line">res = response.read()</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/29/Py007-01-02内置urllib模块/" data-id="cklcb6bjg00ijwo91jylmxrtp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/M07/">M07</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/28/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="page-number" href="/page/28/">28</a><span class="page-number current">29</span><a class="page-number" href="/page/30/">30</a><a class="page-number" href="/page/31/">31</a><span class="space">&hellip;</span><a class="page-number" href="/page/54/">54</a><a class="extend next" rel="next" href="/page/30/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CSS/">CSS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ES6速学/">ES6速学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JS不知深浅/">JS不知深浅</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M01/">M01</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M02/">M02</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M03/">M03</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M04/">M04</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M06/">M06</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M07/">M07</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M08/">M08</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/M09/">M09</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NodeWeb/">NodeWeb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Node后端/">Node后端</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ReactWheels/">ReactWheels</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React入门/">React入门</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TS入门/">TS入门</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/express/">express</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/fullstack/">fullstack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/http/">http</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jQuery/">jQuery</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mobile/">mobile</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node/">node</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node每日精进/">node每日精进</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/oak/">oak</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/">tools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vim/">vim</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vue/">vue</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vultr/">vultr</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web性能优化/">web性能优化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web面经/">web面经</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前端知识点/">前端知识点</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CSS/" style="font-size: 13.75px;">CSS</a> <a href="/tags/ES6速学/" style="font-size: 14.17px;">ES6速学</a> <a href="/tags/JS不知深浅/" style="font-size: 11.25px;">JS不知深浅</a> <a href="/tags/M01/" style="font-size: 13.33px;">M01</a> <a href="/tags/M02/" style="font-size: 15.42px;">M02</a> <a href="/tags/M03/" style="font-size: 15.83px;">M03</a> <a href="/tags/M04/" style="font-size: 16.25px;">M04</a> <a href="/tags/M06/" style="font-size: 18.33px;">M06</a> <a href="/tags/M07/" style="font-size: 17.92px;">M07</a> <a href="/tags/M08/" style="font-size: 17.08px;">M08</a> <a href="/tags/M09/" style="font-size: 11.67px;">M09</a> <a href="/tags/NodeWeb/" style="font-size: 14.58px;">NodeWeb</a> <a href="/tags/Node后端/" style="font-size: 18.75px;">Node后端</a> <a href="/tags/ReactWheels/" style="font-size: 16.67px;">ReactWheels</a> <a href="/tags/React入门/" style="font-size: 15.42px;">React入门</a> <a href="/tags/TS入门/" style="font-size: 15px;">TS入门</a> <a href="/tags/express/" style="font-size: 10.42px;">express</a> <a href="/tags/fullstack/" style="font-size: 17.92px;">fullstack</a> <a href="/tags/http/" style="font-size: 12.08px;">http</a> <a href="/tags/jQuery/" style="font-size: 10px;">jQuery</a> <a href="/tags/java/" style="font-size: 19.17px;">java</a> <a href="/tags/linux/" style="font-size: 10.83px;">linux</a> <a href="/tags/mobile/" style="font-size: 11.25px;">mobile</a> <a href="/tags/mongodb/" style="font-size: 12.5px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 12.5px;">mysql</a> <a href="/tags/node/" style="font-size: 10.42px;">node</a> <a href="/tags/node每日精进/" style="font-size: 10px;">node每日精进</a> <a href="/tags/oak/" style="font-size: 20px;">oak</a> <a href="/tags/python/" style="font-size: 17.5px;">python</a> <a href="/tags/tools/" style="font-size: 10px;">tools</a> <a href="/tags/vim/" style="font-size: 10px;">vim</a> <a href="/tags/vue/" style="font-size: 12.92px;">vue</a> <a href="/tags/vultr/" style="font-size: 10px;">vultr</a> <a href="/tags/web性能优化/" style="font-size: 10px;">web性能优化</a> <a href="/tags/web面经/" style="font-size: 10px;">web面经</a> <a href="/tags/前端知识点/" style="font-size: 19.58px;">前端知识点</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/02/18/Node-web05-07博客系统部署/">Node-web05-07博客系统部署</a>
          </li>
        
          <li>
            <a href="/2021/02/18/Node-web05-06博客系统后端分页/">Node-web05-06博客系统后端分页</a>
          </li>
        
          <li>
            <a href="/2021/02/18/Node-web05-05博客系统创建博客/">Node-web05-05博客系统创建博客</a>
          </li>
        
          <li>
            <a href="/2021/02/18/Node-web05-04博客系统登录/">Node-web05-04博客系统登录</a>
          </li>
        
          <li>
            <a href="/2021/02/18/Node-web05-03博客系统注册/">Node-web05-03博客系统注册</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Stevin<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>