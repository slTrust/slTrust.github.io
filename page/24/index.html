
 <!DOCTYPE HTML>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
  
    <title>Almost</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Stevin">
    

    
    <meta property="og:type" content="website">
<meta property="og:title" content="Almost">
<meta property="og:url" content="http://yoursite.com/page/24/index.html">
<meta property="og:site_name" content="Almost">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Almost">

    
    <link rel="alternative" href="/atom.xml" title="Almost" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>
</html>
  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Almost" title="Almost"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Almost">Almost</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:yoursite.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/02/Py007-02-06scrapy核心组件/" title="Py007-02-06scrapy核心组件" itemprop="url">Py007-02-06scrapy核心组件</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-02T06:39:47.000Z" itemprop="datePublished"> Published 2018-12-02</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="scrapy五大核心组件"><a href="#scrapy五大核心组件" class="headerlink" title="scrapy五大核心组件"></a>scrapy五大核心组件</h3><p><img src="https://raw.githubusercontent.com/slTrust/note/master/img/py/py007_02_0601.png" alt></p>
<blockquote>
<p>引擎(Scrapy)</p>
</blockquote>
<ul>
<li>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
</ul>
<blockquote>
<p>调度器(Scheduler)</p>
</blockquote>
<ul>
<li>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
</ul>
<blockquote>
<p>下载器(Downloader)</p>
</blockquote>
<ul>
<li>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
<blockquote>
<p>爬虫(Spiders)</p>
</blockquote>
<ul>
<li>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
</ul>
<blockquote>
<p>项目管道(Pipeline)</p>
</blockquote>
<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/02/Py007-02-05scrapy多个url爬取/" title="Py007-02-05scrapy多个url爬取" itemprop="url">Py007-02-05scrapy多个url爬取</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-02T05:51:40.000Z" itemprop="datePublished"> Published 2018-12-02</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="爬取多个页面url数据"><a href="#爬取多个页面url数据" class="headerlink" title="爬取多个页面url数据"></a>爬取多个页面url数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 糗事百科  文字标签页面的数据   它有13页</span><br><span class="line">https://www.qiushibaike.com/text/</span><br></pre></td></tr></table></figure>
<h4 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 切换到桌面</span><br><span class="line">cd ~/Desktop </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建项目</span><br><span class="line">scarpy startproject qiubaiByPages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建成功后进入该目录</span><br><span class="line">cd qiubaiByPages/</span><br><span class="line"></span><br><span class="line"># 创建spider 爬虫应用</span><br><span class="line">scrapy genspider qiubai www.qiushibaike.com/text</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第一步-修改spiders-qiubai-py"><a href="#第一步-修改spiders-qiubai-py" class="headerlink" title="第一步 修改spiders/qiubai.py"></a>第一步 修改spiders/qiubai.py</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line">    allowed_domains = [&apos;www.qiushibaike.com/text&apos;]</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/text/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        div_list = response.xpath(&apos;//*[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            author = div.xpath(&apos;./div[@class=&quot;author clearfix&quot;]/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 创建items对象，将解析内容存储到items对象里</span><br><span class="line"></span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第二步-创建items对象-items-py"><a href="#第二步-创建items对象-items-py" class="headerlink" title="第二步 创建items对象  items.py"></a>第二步 创建items对象  items.py</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QiubaibypagesItem(scrapy.Item):</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第三步-在spiders-qiubai-py里导入items-并把item数据提交给管道文件"><a href="#第三步-在spiders-qiubai-py里导入items-并把item数据提交给管道文件" class="headerlink" title="第三步 在spiders/qiubai.py里导入items,并把item数据提交给管道文件"></a>第三步 在spiders/qiubai.py里导入items,并把item数据提交给管道文件</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from qiubaiByPages.items import QiubaibypagesItem</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line">    # 建议注释掉 allowed_domains 图片的资源域名可能与此不一致</span><br><span class="line">    # allowed_domains = [&apos;www.qiushibaike.com/text&apos;]</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/text/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        div_list = response.xpath(&apos;//*[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            author = div.xpath(&apos;./div[@class=&quot;author clearfix&quot;]/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 创建items对象，将解析内容存储到items对象里</span><br><span class="line">            item = QiubaibypagesItem()</span><br><span class="line">            item[&apos;author&apos;] = author</span><br><span class="line">            item[&apos;content&apos;] = content</span><br><span class="line"></span><br><span class="line">            # 将item对象提交给管道文件</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>
<blockquote>
<p>修改管道文件pipelines.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">class QiubaibypagesPipeline(object):</span><br><span class="line">    fp = None</span><br><span class="line">    def open_spider(self,spider):</span><br><span class="line">        print(&apos;开始爬虫&apos;)</span><br><span class="line">        self.fp = open(&apos;./qiubai.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">    def close_spider(self,spider):</span><br><span class="line">        print(&apos;结束爬虫&apos;)</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.fp.write(item[&apos;author&apos;]+&apos;:&apos;+item[&apos;content&apos;]+&apos;\n\n\n&apos;)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="第四步-修改配置文件settings-py"><a href="#第四步-修改配置文件settings-py" class="headerlink" title="第四步 修改配置文件settings.py"></a>第四步 修改配置文件settings.py</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 伪装请求载体身份</span><br><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 解开注释 管道操作配置</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;qiubaiByPages.pipelines.QiubaibypagesPipeline&apos;: 300,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="最后-回到刚刚的命令行-执行爬虫命令"><a href="#最后-回到刚刚的命令行-执行爬虫命令" class="headerlink" title="最后 回到刚刚的命令行 执行爬虫命令"></a>最后 回到刚刚的命令行 执行爬虫命令</h4></blockquote>
<p>把基础的流程跑通</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl qiubai --nolog</span><br></pre></td></tr></table></figure>
<h3 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h3><ul>
<li>上述流程跑通后，仅仅是一页的数据而</li>
<li>我们的需求是它的所有分页数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 该url的分页数据</span><br><span class="line">https://www.qiushibaike.com/text/</span><br></pre></td></tr></table></figure>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li>请求的手动发送</li>
</ul>
<blockquote>
<p>分析spiders/qiubai.py</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    ...</span><br><span class="line">    # 将item对象提交给管道文件</span><br><span class="line">    yield item</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">当yield item都执行完毕  意味着：</span><br><span class="line"></span><br><span class="line">https://www.qiushibaike.com/text/</span><br><span class="line">这一页面的数据爬取完毕</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>手动处理下一次请求(仅仅以第二页数据为例)理解版本的parse函数修改</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    div_list = response.xpath(&apos;//*[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">    for div in div_list:</span><br><span class="line">        author = div.xpath(&apos;./div[@class=&quot;author clearfix&quot;]/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">        content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">        # 创建items对象，将解析内容存储到items对象里</span><br><span class="line">        item = QiubaibypagesItem()</span><br><span class="line">        item[&apos;author&apos;] = author</span><br><span class="line">        item[&apos;content&apos;] = content</span><br><span class="line"></span><br><span class="line">        # 将item对象提交给管道文件</span><br><span class="line">        yield item</span><br><span class="line"></span><br><span class="line">    # 请求的手动发送</span><br><span class="line">    url = &apos;https://www.qiushibaike.com/text/page/2&apos;</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    url代表下一次请求的 url</span><br><span class="line">    callback代表将请求到的页面数据进行解析  而这个解析函数正是我们这里的parse函数</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    yield scrapy.Request(url=url,callback=self.parse)</span><br></pre></td></tr></table></figure>
<blockquote>
<h5 id="注意-这个版本-手动发送请求属于递归调用-由于没有结束条件所以会一直递归下去。。。。。"><a href="#注意-这个版本-手动发送请求属于递归调用-由于没有结束条件所以会一直递归下去。。。。。" class="headerlink" title="注意 这个版本 手动发送请求属于递归调用 由于没有结束条件所以会一直递归下去。。。。。"></a>注意 这个版本 手动发送请求属于递归调用 由于没有结束条件所以会一直递归下去。。。。。</h5></blockquote>
<blockquote>
<h5 id="注意-这个版本-手动发送请求属于递归调用-由于没有结束条件所以会一直递归下去。。。。。-1"><a href="#注意-这个版本-手动发送请求属于递归调用-由于没有结束条件所以会一直递归下去。。。。。-1" class="headerlink" title="注意 这个版本 手动发送请求属于递归调用 由于没有结束条件所以会一直递归下去。。。。。"></a>注意 这个版本 手动发送请求属于递归调用 由于没有结束条件所以会一直递归下去。。。。。</h5></blockquote>
<blockquote>
<h5 id="注意-这个版本-手动发送请求属于递归调用-由于没有结束条件所以会一直递归下去。。。。。-2"><a href="#注意-这个版本-手动发送请求属于递归调用-由于没有结束条件所以会一直递归下去。。。。。-2" class="headerlink" title="注意 这个版本 手动发送请求属于递归调用 由于没有结束条件所以会一直递归下去。。。。。"></a>注意 这个版本 手动发送请求属于递归调用 由于没有结束条件所以会一直递归下去。。。。。</h5></blockquote>
<blockquote>
<h4 id="完整版的qiubai-py文件"><a href="#完整版的qiubai-py文件" class="headerlink" title="完整版的qiubai.py文件"></a>完整版的qiubai.py文件</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from qiubaiByPages.items import QiubaibypagesItem</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line">    # 注视掉这个 allowed_domains</span><br><span class="line">    # allowed_domains = [&apos;www.qiushibaike.com/text&apos;]</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/text/&apos;]</span><br><span class="line"></span><br><span class="line">    # 设计通用的url模版</span><br><span class="line">    url = &apos;https://www.qiushibaike.com/text/page/%d&apos;</span><br><span class="line">    pageNum = 1</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        div_list = response.xpath(&apos;//*[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            author = div.xpath(&apos;./div[@class=&quot;author clearfix&quot;]/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 创建items对象，将解析内容存储到items对象里</span><br><span class="line">            item = QiubaibypagesItem()</span><br><span class="line">            item[&apos;author&apos;] = author</span><br><span class="line">            item[&apos;content&apos;] = content</span><br><span class="line"></span><br><span class="line">            # 将item对象提交给管道文件</span><br><span class="line">            yield item</span><br><span class="line"></span><br><span class="line">        # 请求的手动发送</span><br><span class="line">        if self.pageNum &lt; 13: # 13表示是最后一页的页码</span><br><span class="line">            print(&apos;爬取到了第%d的页面数据&apos;%self.pageNum)</span><br><span class="line">            self.pageNum += 1 # 页码增加</span><br><span class="line">            new_url = format(self.url%self.pageNum)</span><br><span class="line">            yield scrapy.Request(url=new_url,callback=self.parse)</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/01/Py007-02-04scrapy管道高级操作/" title="Py007-02-04scrapy管道高级操作" itemprop="url">Py007-02-04scrapy管道高级操作</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-01T15:11:45.000Z" itemprop="datePublished"> Published 2018-12-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="需求：将爬取的数据分别存到本地磁盘-redis数据库-mysql数据库"><a href="#需求：将爬取的数据分别存到本地磁盘-redis数据库-mysql数据库" class="headerlink" title="需求：将爬取的数据分别存到本地磁盘/redis数据库/mysql数据库"></a>需求：将爬取的数据分别存到本地磁盘/redis数据库/mysql数据库</h3><ol>
<li>需要在管道文件 pipelines.py 里编写对应的管道类</li>
<li>在配置文件里 settings.py 对自定义管道类进行生效操作</li>
</ol>
<blockquote>
<p>依旧基于之前的qiubaiPro项目</p>
</blockquote>
<p>pipelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># Define your item pipelines here</span><br><span class="line">#</span><br><span class="line"># Don&apos;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="line"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="line"></span><br><span class="line"># 基于管道  文件存储</span><br><span class="line"># class QiubaiproPipeline(object):</span><br><span class="line">#     fp = None</span><br><span class="line">#     # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次</span><br><span class="line">#     def open_spider(self,spider):</span><br><span class="line">#         print(&apos;开始爬虫&apos;)</span><br><span class="line">#         self.fp = open(&apos;./qiubai_pipe.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;)</span><br><span class="line">#</span><br><span class="line">#     # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次</span><br><span class="line">#     def close_spider(self, spider):</span><br><span class="line">#         print(&apos;结束爬虫&apos;)</span><br><span class="line">#         self.fp.close()</span><br><span class="line">#</span><br><span class="line">#     # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">#     &apos;&apos;&apos;</span><br><span class="line">#     :param</span><br><span class="line">#     # 参数item:表示接收到的item对象</span><br><span class="line">#     &apos;&apos;&apos;</span><br><span class="line">#     def process_item(self, item, spider):</span><br><span class="line">#         # 取出item对应的数据值</span><br><span class="line">#         author = item[&apos;author&apos;]</span><br><span class="line">#         content = item[&apos;content&apos;]</span><br><span class="line">#</span><br><span class="line">#         # 持久化存储</span><br><span class="line">#         self.fp.write(author+&apos;:&apos;+content+&apos;\n\n\n&apos;)</span><br><span class="line">#         return item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># # mysql</span><br><span class="line"># import pymysql</span><br><span class="line">#</span><br><span class="line"># class QiubaiproPipeline(object):</span><br><span class="line">#     conn = None</span><br><span class="line">#     # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次</span><br><span class="line">#     def open_spider(self,spider):</span><br><span class="line">#         print(&apos;开始爬虫&apos;)</span><br><span class="line">#         # 连接数据库</span><br><span class="line">#         self.conn = pymysql.connect(host=&apos;127.0.0.1&apos;,port=3306,user=&apos;root&apos;,password=&apos;831015&apos;,db=&apos;qiubai&apos;)</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line">#     # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次</span><br><span class="line">#     def close_spider(self, spider):</span><br><span class="line">#         print(&apos;结束爬虫&apos;)</span><br><span class="line">#         self.cursor.close()</span><br><span class="line">#         self.conn.close()</span><br><span class="line">#</span><br><span class="line">#     # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">#     &apos;&apos;&apos;</span><br><span class="line">#     :param</span><br><span class="line">#     # 参数item:表示接收到的item对象</span><br><span class="line">#     &apos;&apos;&apos;</span><br><span class="line">#     def process_item(self, item, spider):</span><br><span class="line">#         # 取出item对应的数据值</span><br><span class="line">#         author = item[&apos;author&apos;]</span><br><span class="line">#         content = item[&apos;content&apos;]</span><br><span class="line">#</span><br><span class="line">#         # 连接数据库</span><br><span class="line">#         # 执行sql</span><br><span class="line">#         sql = &apos;insert into qiubai values(null,&quot;%s&quot;,&quot;%s&quot;)&apos;%(author,content)</span><br><span class="line">#         self.cursor = self.conn.cursor()</span><br><span class="line">#         try:</span><br><span class="line">#             self.cursor.execute(sql)</span><br><span class="line">#             self.conn.commit()</span><br><span class="line">#             # 提交事务</span><br><span class="line">#         except Exception as e:</span><br><span class="line">#             print(e)</span><br><span class="line">#             self.conn.rollback()</span><br><span class="line">#</span><br><span class="line">#         return item</span><br><span class="line"></span><br><span class="line"># redis   安装 pip3 install redis</span><br><span class="line">import redis</span><br><span class="line">import json</span><br><span class="line">class QiubaiproPipeline(object):</span><br><span class="line">    conn = None</span><br><span class="line">    # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次</span><br><span class="line">    def open_spider(self,spider):</span><br><span class="line">        print(&apos;开始爬虫&apos;)</span><br><span class="line">        # 连接数据库</span><br><span class="line"></span><br><span class="line">        self.conn = redis.Redis(host=&apos;127.0.0.1&apos;, port=6379)</span><br><span class="line"></span><br><span class="line">    # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        print(&apos;结束爬虫&apos;)</span><br><span class="line">        print(self.conn.lrange(&apos;data&apos;,0,-1))</span><br><span class="line"></span><br><span class="line">    # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param</span><br><span class="line">    # 参数item:表示接收到的item对象</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 取出item对应的数据值</span><br><span class="line">        author = item[&apos;author&apos;]</span><br><span class="line">        content = item[&apos;content&apos;]</span><br><span class="line"></span><br><span class="line">        obj = &#123;</span><br><span class="line">            &apos;author&apos;:author,</span><br><span class="line">            &apos;content&apos;:content</span><br><span class="line">        &#125;</span><br><span class="line">        # 录入数据</span><br><span class="line">        # self.conn.lpush(&apos;data&apos;, obj) 5.0版本报错  发现 序列化之后就好了</span><br><span class="line">        self.conn.lpush(&apos;data&apos;,json.dumps(obj))</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 封装另外的类  存储操作</span><br><span class="line"></span><br><span class="line"># 将数据存入到本地磁盘</span><br><span class="line">class QiubaiByFiles(object):</span><br><span class="line">    def process_item(self,item,spider):</span><br><span class="line">        print(&apos;数据写入磁盘&apos;)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line"># 将数据存入到 Mysql</span><br><span class="line">class QiubaiByMySql(object):</span><br><span class="line">    def process_item(self,item,spider):</span><br><span class="line">        print(&apos;数据写入mysql&apos;)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将数据存入到 Redis</span><br><span class="line">class QiubaiByRedis(object):</span><br><span class="line">    def process_item(self,item,spider):</span><br><span class="line">        print(&apos;数据写入redis&apos;)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="如何将自定义的类-加入管道呢？"><a href="#如何将自定义的类-加入管道呢？" class="headerlink" title="如何将自定义的类 加入管道呢？"></a>如何将自定义的类 加入管道呢？</h4></blockquote>
<p>settings.py里</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># settings.py里 ITEM_PIPELINES 进行配置</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;qiubaiPro.pipelines.QiubaiproPipeline&apos;: 300,</span><br><span class="line">   &apos;qiubaiPro.pipelines.QiubaiByFiles&apos;: 400,</span><br><span class="line">   &apos;qiubaiPro.pipelines.QiubaiByMySql&apos;: 500,</span><br><span class="line">   &apos;qiubaiPro.pipelines.QiubaiByRedis&apos;: 600,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 300代表执行的优先级 </span><br><span class="line"># 此时 QiubaiByRedis的优先级最高</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/01/Py007-02-03redis安装以及使用/" title="Py007-02-03redis安装以及使用" itemprop="url">Py007-02-03redis安装以及使用</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-01T13:54:22.000Z" itemprop="datePublished"> Published 2018-12-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">自行去官网下载对应安装文件 我下的是5.0版本</span><br><span class="line">https://redis.io/</span><br><span class="line"></span><br><span class="line">http://www.redis.cn/</span><br></pre></td></tr></table></figure>
<p>下载完成后解压缩后===&gt; redis-5.0.0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">该目录文件如下：</span><br><span class="line">00-RELEASENOTES		Makefile		runtest-sentinel</span><br><span class="line">BUGS			README.md		sentinel.conf</span><br><span class="line">CONTRIBUTING		deps			src</span><br><span class="line">COPYING			redis.conf		tests</span><br><span class="line">INSTALL			runtest			utils</span><br><span class="line"></span><br><span class="line">---------------------------------------------------</span><br><span class="line">redis.conf这是一个配置文件（分布式爬虫时会要求对其进行配置）</span><br><span class="line">src目录 这里是redis的源码文件 需要编译才能变成使用</span><br></pre></td></tr></table></figure>
<blockquote>
<p>安装redis</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 进入redis5.0.0 这个目录</span><br><span class="line"># 执行</span><br><span class="line">make </span><br><span class="line"># 成功之后再次进入 redis5.0.0/src</span><br><span class="line"># 发现多了几个命令行文件</span><br><span class="line"># 其中有俩重要的文件</span><br><span class="line">redis-server 用来打开redis服务器</span><br><span class="line">redis-cli 打开redis的客户端</span><br></pre></td></tr></table></figure>
<blockquote>
<p>启动redis服务器</p>
</blockquote>
<ul>
<li>redis-server 通常结合一个配置(上一级目录的redis.conf)然后启动 </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 进入reids-server目录</span><br><span class="line">cd redis5.0.0/src</span><br><span class="line"></span><br><span class="line">./redis-server  ../redis.conf </span><br><span class="line"># 看到一个正方体 代表你启动成功</span><br></pre></td></tr></table></figure>
<blockquote>
<p>启动redis客户端</p>
</blockquote>
<p>再开一个终端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 进入reids-server目录</span><br><span class="line">cd redis5.0.0/src</span><br><span class="line"></span><br><span class="line">redis-cli</span><br><span class="line"></span><br><span class="line">成功之后就可以进行相应的操作</span><br><span class="line"></span><br><span class="line">如输入</span><br><span class="line"></span><br><span class="line">&gt; set name &apos;bobo&apos;</span><br><span class="line">&gt; ok</span><br><span class="line"></span><br><span class="line">&gt; get name</span><br><span class="line">&gt; &apos;bobo&apos;</span><br></pre></td></tr></table></figure>
<h4 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h4><blockquote>
<p>依旧基于之前的糗百爬虫项目</p>
</blockquote>
<p>修改 pipelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># redis   安装 pip3 install redis</span><br><span class="line">import redis</span><br><span class="line"></span><br><span class="line">class QiubaiproPipeline(object):</span><br><span class="line">    conn = None</span><br><span class="line">    # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次</span><br><span class="line">    def open_spider(self,spider):</span><br><span class="line">        print(&apos;开始爬虫&apos;)</span><br><span class="line">        # 连接数据库</span><br><span class="line">        self.conn = redis.Redis(host=&apos;127.0.0.1&apos;,port=6379)</span><br><span class="line"></span><br><span class="line">    # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        print(&apos;结束爬虫&apos;)</span><br><span class="line"></span><br><span class="line">    # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param</span><br><span class="line">    # 参数item:表示接收到的item对象</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 取出item对应的数据值</span><br><span class="line">        author = item[&apos;author&apos;]</span><br><span class="line">        content = item[&apos;content&apos;]</span><br><span class="line"></span><br><span class="line">        obj = &#123;</span><br><span class="line">            &apos;author&apos;:author,</span><br><span class="line">            &apos;content&apos;:content</span><br><span class="line">        &#125;</span><br><span class="line">        # 录入数据</span><br><span class="line">        self.conn.lpush(&apos;data&apos;,obj)</span><br><span class="line"></span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<blockquote>
<p>启动redis服务</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd redis5.0.0/src</span><br><span class="line"></span><br><span class="line">./redis-server   ../redis.conf</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行爬虫程序</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl qiubai --nolog</span><br></pre></td></tr></table></figure>
<blockquote>
<p>启动redis客户端</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd redis5.0.0/src</span><br><span class="line"></span><br><span class="line">./redis-cli</span><br><span class="line"># 查看所有列表的数据</span><br><span class="line">lrange data 0 -1</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/01/Py007-02-02scrapy持久化存储/" title="Py007-02-02scrapy持久化存储" itemprop="url">Py007-02-02scrapy持久化存储</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-01T07:11:54.000Z" itemprop="datePublished"> Published 2018-12-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="持久化存储操作"><a href="#持久化存储操作" class="headerlink" title="持久化存储操作"></a>持久化存储操作</h3><p>两种方式</p>
<ul>
<li>a 磁盘文件</li>
<li>b 数据库</li>
</ul>
<blockquote>
<h4 id="磁盘文件方式"><a href="#磁盘文件方式" class="headerlink" title="磁盘文件方式"></a>磁盘文件方式</h4></blockquote>
<ul>
<li>基于终端指令的方式</li>
<li>基于管道的方式</li>
</ul>
<h4 id="终端指令的方式"><a href="#终端指令的方式" class="headerlink" title="终端指令的方式"></a>终端指令的方式</h4><ul>
<li>保证爬虫文件的parse方法中有可迭代类型对象（通常为列表or字典）的返回，该返回值</li>
<li>通过终端指令的形式写入指定格式的文件中进行持久化操作。</li>
</ul>
<blockquote>
<h4 id="基于上一篇内容来处理"><a href="#基于上一篇内容来处理" class="headerlink" title="基于上一篇内容来处理"></a>基于上一篇内容来处理</h4></blockquote>
<p>qiubai.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line">    # 最好注释这个允许域名   因为图片如果是其他域名的就获取不到了</span><br><span class="line">    # allowed_domains = [&apos;www.qiushibaike.com/text&apos;]</span><br><span class="line"></span><br><span class="line">    # 默认生成的协议是http的需要手动修改为 https</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/text/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 推荐xpath(scrapy里集成了xpath解析接口)</span><br><span class="line"></span><br><span class="line">        # 每个段子的外层容器</span><br><span class="line">        div_list = response.xpath(&apos;//div[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        # step001 数据容器（可迭代对象）</span><br><span class="line">        data_list = []</span><br><span class="line">        for div in div_list:</span><br><span class="line">            author = div.xpath(&apos;./div/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            obj = &#123;</span><br><span class="line">                &apos;author&apos;:author,</span><br><span class="line">                &apos;content&apos;:content</span><br><span class="line">            &#125;</span><br><span class="line">            data_list.append(obj)</span><br><span class="line"></span><br><span class="line">        # step002 返回可迭代对象</span><br><span class="line">        return data_list</span><br></pre></td></tr></table></figure>
<blockquote>
<p>通过命令来存储数据</p>
</blockquote>
<p>执行输出指定格式进行存储：将爬取到的数据写入不同格式的文件中进行存储</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">scrapy crawl 爬虫名称 -o xxx.json</span><br><span class="line">scrapy crawl 爬虫名称 -o xxx.xml</span><br><span class="line">scrapy crawl 爬虫名称 -o xxx.csv</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">scrapy crawl qiubai -o qiubai.csv --nolog</span><br><span class="line"></span><br><span class="line">scrapy crawl qiubai -o qiubai.txt --nolog</span><br><span class="line"></span><br><span class="line">scrapy crawl qiubai -o qiubai.json --nolog</span><br></pre></td></tr></table></figure>
<h4 id="基于管道"><a href="#基于管道" class="headerlink" title="基于管道"></a>基于管道</h4><ul>
<li>1.items 存储解析到的页面数据 类似django里的models</li>
<li>2.pipelines 处理持久化存储的相关操作</li>
<li>3.代码实现流程  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 将解析的数据存到 items对象</span><br><span class="line">2. 使用yield 关键字将 items提交给管道文件进行处理</span><br><span class="line">3. 在管道文件中编写代码完成数据存储</span><br><span class="line">4. 在配置文件里开启&apos;管道&apos;操作</span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<h3 id="继续操作刚刚的爬虫文件"><a href="#继续操作刚刚的爬虫文件" class="headerlink" title="继续操作刚刚的爬虫文件"></a>继续操作刚刚的爬虫文件</h3></blockquote>
<h4 id="第一步-将解析的数据存到-items对象"><a href="#第一步-将解析的数据存到-items对象" class="headerlink" title="第一步 将解析的数据存到 items对象"></a>第一步 将解析的数据存到 items对象</h4><blockquote>
<h5 id="step001-qiubai-py"><a href="#step001-qiubai-py" class="headerlink" title="step001 qiubai.py"></a>step001 qiubai.py</h5></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 引入items模块</span><br><span class="line">from qiubaiPro.qiubaiPro.items import QiubaiproItem</span><br></pre></td></tr></table></figure>
<blockquote>
<h5 id="step002-声明items对应的字段-如-author和-content"><a href="#step002-声明items对应的字段-如-author和-content" class="headerlink" title="step002 声明items对应的字段  如 author和 content"></a>step002 声明items对应的字段  如 author和 content</h5></blockquote>
<p>items.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QiubaiproItem(scrapy.Item):</span><br><span class="line">    # 声明字段</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<blockquote>
<h5 id="step003添加-数据到items"><a href="#step003添加-数据到items" class="headerlink" title="step003添加 数据到items"></a>step003添加 数据到items</h5></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from qiubaiPro.items import QiubaiproItem</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line"></span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/text/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 推荐xpath(scrapy里集成了xpath解析接口)</span><br><span class="line"></span><br><span class="line">        # 每个段子的外层容器</span><br><span class="line">        div_list = response.xpath(&apos;//div[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            author = div.xpath(&apos;./div/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # step001 将解析到的数据 存到items对象</span><br><span class="line">            item = QiubaiproItem()</span><br><span class="line">            item[&apos;author&apos;] = author</span><br><span class="line">            item[&apos;content&apos;] = content</span><br></pre></td></tr></table></figure>
<h4 id="第二步-：使用yield-关键字将-items提交给管道文件进行处理"><a href="#第二步-：使用yield-关键字将-items提交给管道文件进行处理" class="headerlink" title="第二步 ：使用yield 关键字将 items提交给管道文件进行处理"></a>第二步 ：使用yield 关键字将 items提交给管道文件进行处理</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 将解析到的数据 存到items对象</span><br><span class="line">item = QiubaiproItem()</span><br><span class="line">item[&apos;author&apos;] = author</span><br><span class="line">item[&apos;content&apos;] = content</span><br><span class="line"></span><br><span class="line"># step002 将item对象提交给管道</span><br><span class="line">yield item</span><br><span class="line"></span><br><span class="line"># 去编写对象的 pipelines.py</span><br></pre></td></tr></table></figure>
<h4 id="第三步-：在管道文件中编写代码完成数据存储"><a href="#第三步-：在管道文件中编写代码完成数据存储" class="headerlink" title="第三步 ：在管道文件中编写代码完成数据存储"></a>第三步 ：在管道文件中编写代码完成数据存储</h4><p>pipelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class QiubaiproPipeline(object):</span><br><span class="line">    # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param</span><br><span class="line">    # 参数item:表示接收到的item对象</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 每向管道文件提交一次item，则该方法就会执行一次</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 取出item对应的数据值</span><br><span class="line">        author = item[&apos;author&apos;]</span><br><span class="line">        content = item[&apos;content&apos;]</span><br><span class="line"></span><br><span class="line">        # 持久化存储</span><br><span class="line">        with open(&apos;./qiubai_pipe.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">            fp.write(author+&apos;:&apos;+content+&apos;\n\n\n&apos;)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<h4 id="第四步-在配置文件里开启’管道’操作"><a href="#第四步-在配置文件里开启’管道’操作" class="headerlink" title="第四步 在配置文件里开启’管道’操作"></a>第四步 在配置文件里开启’管道’操作</h4><p>settings.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 搜索pipeline</span><br><span class="line"></span><br><span class="line"># 解开注释</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   &apos;qiubaiPro.pipelines.QiubaiproPipeline&apos;: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 300代表优先级</span><br></pre></td></tr></table></figure>
<h4 id="执行爬虫命令"><a href="#执行爬虫命令" class="headerlink" title="执行爬虫命令"></a>执行爬虫命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl qiubai </span><br><span class="line"></span><br><span class="line"># 此时发现qiubai_pipe.txt里只有一条数据</span><br><span class="line"></span><br><span class="line"># 原因是</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">qiubai.py里的 yield item 写在for循环里</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 推荐xpath(scrapy里集成了xpath解析接口)</span><br><span class="line"></span><br><span class="line">        # 每个段子的外层容器</span><br><span class="line">        div_list = response.xpath(&apos;//div[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            author = div.xpath(&apos;./div/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line"></span><br><span class="line">            # step001 将解析到的数据 存到items对象</span><br><span class="line">            item = QiubaiproItem()</span><br><span class="line">            item[&apos;author&apos;] = author</span><br><span class="line">            item[&apos;content&apos;] = content</span><br><span class="line"></span><br><span class="line">            # step002 将item对象提交给管道</span><br><span class="line">            yield item</span><br><span class="line">------------------------------------------</span><br><span class="line">每向管道文件提交一次item，则该方法就会执行一次</span><br><span class="line">pipelines.py里的</span><br><span class="line"></span><br><span class="line">def process_item(self, item, spider):</span><br><span class="line">    # 取出item对应的数据值</span><br><span class="line">    author = item[&apos;author&apos;]</span><br><span class="line">    content = item[&apos;content&apos;]</span><br><span class="line"></span><br><span class="line">    # 持久化存储</span><br><span class="line">    with open(&apos;./qiubai_pipe.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as fp:</span><br><span class="line">        fp.write(author+&apos;:&apos;+content+&apos;\n\n\n&apos;)</span><br><span class="line">    return item</span><br><span class="line">所以只存了 最后一次的爬虫数据</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<h4 id="解决存储bug"><a href="#解决存储bug" class="headerlink" title="解决存储bug"></a>解决存储bug</h4><p>pipelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class QiubaiproPipeline(object):</span><br><span class="line">    fp = None</span><br><span class="line">    # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次</span><br><span class="line">    def open_spider(self,spider):</span><br><span class="line">        print(&apos;开始爬虫&apos;)</span><br><span class="line">        self.fp = open(&apos;./qiubai_pipe.txt&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">    # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        print(&apos;结束爬虫&apos;)</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line">    # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param</span><br><span class="line">    # 参数item:表示接收到的item对象</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 取出item对应的数据值</span><br><span class="line">        author = item[&apos;author&apos;]</span><br><span class="line">        content = item[&apos;content&apos;]</span><br><span class="line"></span><br><span class="line">        # 持久化存储</span><br><span class="line">        self.fp.write(author+&apos;:&apos;+content+&apos;\n\n\n&apos;)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="再次执行爬虫命令"><a href="#再次执行爬虫命令" class="headerlink" title="再次执行爬虫命令"></a>再次执行爬虫命令</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl qiubai --nolog</span><br></pre></td></tr></table></figure>
<h4 id="基于mysql持久化存储"><a href="#基于mysql持久化存储" class="headerlink" title="基于mysql持久化存储"></a>基于mysql持久化存储</h4><blockquote>
<p>与管道方式大体相同,唯一不同的是第三步  此时不是存入文件而是 sql录入</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 将解析的数据存到 items对象</span><br><span class="line">2. 使用yield 关键字将 items提交给管道文件进行处理</span><br><span class="line">3. 在管道文件中编写代码sql录入语句完成数据存储</span><br><span class="line">4. 在配置文件里开启&apos;管道&apos;操作</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">class QiubaiproPipeline(object):</span><br><span class="line">    conn = None</span><br><span class="line">    # 在爬虫过程中，该方法只会在开始爬虫的时候调用一次</span><br><span class="line">    def open_spider(self,spider):</span><br><span class="line">        print(&apos;开始爬虫&apos;)</span><br><span class="line">        # 连接数据库</span><br><span class="line">        self.conn = pymysql.Connect(host=&apos;127.0.0.1&apos;,port=3306,user=&apos;root&apos;,password=&apos;831015&apos;,db=&apos;qiubai&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # 在爬虫过程中，该方法只会在爬虫结束的时候调用一次</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        print(&apos;结束爬虫&apos;)</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.conn.close()</span><br><span class="line"></span><br><span class="line">    # 该方法接收 爬虫文件中提交过来的item对象，并对item对象中存储的数据进行持久化存储</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    :param</span><br><span class="line">    # 参数item:表示接收到的item对象</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 取出item对应的数据值</span><br><span class="line">        author = item[&apos;author&apos;]</span><br><span class="line">        content = item[&apos;content&apos;]</span><br><span class="line"></span><br><span class="line">        # 连接数据库</span><br><span class="line">        # 执行sql</span><br><span class="line">        sql = &apos;insert into qiubai values(null,&quot;%s&quot;,&quot;%s&quot;)&apos;%(author,content)</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line">        try:</span><br><span class="line">            self.cursor.execute(sql)</span><br><span class="line">            self.conn.commit()</span><br><span class="line">            # 提交事务</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(e)</span><br><span class="line">            self.conn.rollback()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.fp.write(author+&apos;:&apos;+content+&apos;\n\n\n&apos;)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<ul>
<li>建库 建表</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -pxxx</span><br><span class="line"></span><br><span class="line">create database qiubai;</span><br><span class="line"></span><br><span class="line">use qiubai</span><br><span class="line"></span><br><span class="line">create table qiubai (</span><br><span class="line">    id int primary key auto_increment,</span><br><span class="line">    author varchar(100) not null,</span><br><span class="line">    content varchar(500) not null</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行爬虫命令</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl qiubai --nolog</span><br></pre></td></tr></table></figure>
<p>mysql终端里</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from qiubai;</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/01/Py007-02-01scrapy简介/" title="Py007-02-01scrapy简介" itemprop="url">Py007-02-01scrapy简介</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-01T04:33:28.000Z" itemprop="datePublished"> Published 2018-12-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="scrapy简介"><a href="#scrapy简介" class="headerlink" title="scrapy简介"></a>scrapy简介</h3><ul>
<li>环境安装</li>
<li>基础使用</li>
</ul>
<h4 id="一-什么是Scrapy？"><a href="#一-什么是Scrapy？" class="headerlink" title="一.什么是Scrapy？"></a>一.什么是Scrapy？</h4><blockquote>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。</p>
</blockquote>
<h4 id="二-安装"><a href="#二-安装" class="headerlink" title="二.安装"></a>二.安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">　　Linux：</span><br><span class="line"></span><br><span class="line">pip3 install scrapy</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">　　Windows：</span><br><span class="line"></span><br><span class="line">a. pip3 install wheel</span><br><span class="line"></span><br><span class="line">b. 下载twisted http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line"></span><br><span class="line">c. 进入下载目录，执行 pip3 install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl</span><br><span class="line"></span><br><span class="line">d. pip3 install pywin32</span><br><span class="line"></span><br><span class="line">e. pip3 install scrapy</span><br></pre></td></tr></table></figure>
<h4 id="三-基础使用"><a href="#三-基础使用" class="headerlink" title="三.基础使用"></a>三.基础使用</h4><blockquote>
<p>1.创建项目：scrapy startproject 项目名称</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">项目结构：</span><br><span class="line"></span><br><span class="line">project_name/</span><br><span class="line">   scrapy.cfg：</span><br><span class="line">   project_name/</span><br><span class="line">       __init__.py</span><br><span class="line">       items.py</span><br><span class="line">       pipelines.py</span><br><span class="line">       settings.py</span><br><span class="line">       spiders/</span><br><span class="line">           __init__.py</span><br><span class="line"></span><br><span class="line">scrapy.cfg   项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）</span><br><span class="line">items.py     设置数据存储模板，用于结构化数据，如：Django的Model</span><br><span class="line">pipelines    数据持久化处理</span><br><span class="line">settings.py  配置文件，如：递归的层数、并发数，延迟下载等</span><br><span class="line">spiders      爬虫目录，如：创建文件，编写爬虫解析规则</span><br></pre></td></tr></table></figure>
<blockquote>
<p>　2.创建爬虫应用程序：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd project_name（进入项目目录）</span><br><span class="line"></span><br><span class="line">scrapy genspider 应用名称 爬取网页的起始url （例如：scrapy genspider qiubai www.qiushibaike.com）</span><br></pre></td></tr></table></figure>
<blockquote>
<p>3.编写爬虫文件:在步骤2执行完毕后，会在项目的spiders中生成一个应用名的py爬虫文件，文件源码如下：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    #爬虫文件名称  ：通过这个name可以指定的定位到某一个具体的爬虫文件</span><br><span class="line">    name = &apos;qiubai&apos; </span><br><span class="line">    #允许的域名（爬取指定域名下的页面数据）</span><br><span class="line">    allowed_domains = [&apos;https://www.qiushibaike.com/&apos;]</span><br><span class="line">    #起始url： 列表的形式，当前工程将要爬取页面对应的url</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/&apos;]</span><br><span class="line"></span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    # 不能对百度进行爬取  因为allowed_domains的域名 跟百度不是一个域名</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/&apos;,&apos;www.baidu.com&apos;] </span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">     # （回调）解析方法：对获取的页面数据进行指定内容的解析</span><br><span class="line">     # response：根据起始url列表发请求，请求成功后返回的响应对象</span><br><span class="line">     # 该函数返回值必须为可迭代对象或者NUll</span><br><span class="line">     def parse(self, response):</span><br><span class="line">        print(response.text) #获取字符串类型的响应内容</span><br><span class="line">        print(response.body)#获取字节类型的相应内容</span><br></pre></td></tr></table></figure>
<blockquote>
<p>4.设置修改settings.py配置文件相关配置:</p>
</blockquote>
<p>修改内容及其结果如下：(初学阶段仅仅做如下设置)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 伪装请求载体身份</span><br><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br></pre></td></tr></table></figure>
<blockquote>
<p>5.执行爬虫程序：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl  应用名称</span><br><span class="line">如</span><br><span class="line">scrapy crawl qiubai</span><br><span class="line"># 会显示一堆 日志信息</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="忽略日志信息"><a href="#忽略日志信息" class="headerlink" title="忽略日志信息"></a>忽略日志信息</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 应用名称 --nolog</span><br></pre></td></tr></table></figure>
<h4 id="四-小试牛刀：将糗百首页中文字标签对应的内容和标题进行爬取"><a href="#四-小试牛刀：将糗百首页中文字标签对应的内容和标题进行爬取" class="headerlink" title="四.小试牛刀：将糗百首页中文字标签对应的内容和标题进行爬取"></a>四.小试牛刀：将糗百首页中文字标签对应的内容和标题进行爬取</h4><ul>
<li><a href="https://www.qiushibaike.com/text/" target="_blank" rel="noopener">https://www.qiushibaike.com/text/</a></li>
</ul>
<blockquote>
<h4 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1. 创建工程"></a>1. 创建工程</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject qiubaiPro</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="2-创建一个爬虫程序"><a href="#2-创建一个爬虫程序" class="headerlink" title="2. 创建一个爬虫程序"></a>2. 创建一个爬虫程序</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 进入到工程目录</span><br><span class="line">cd qiubaiPro</span><br><span class="line">scrapy genspider qiubai www.qiushibaike.com/text</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="3-编写代码"><a href="#3-编写代码" class="headerlink" title="3. 编写代码"></a>3. 编写代码</h4></blockquote>
<ul>
<li>注意默认的协议是http 所以需要手动改对应的网址协议</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 默认</span><br><span class="line">start_urls = [&apos;http://www.qiushibaike.com/&apos;]</span><br><span class="line"># 更改后</span><br><span class="line">start_urls = [&apos;https://www.qiushibaike.com/&apos;]</span><br></pre></td></tr></table></figure>
<ul>
<li>建议注释掉 allowed_domains</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 因为爬取内容的图片不一定是本域名下的  如果设置了  可能获取不到</span><br><span class="line">allowed_domains = [&apos;https://www.qiushibaike.com/&apos;]</span><br></pre></td></tr></table></figure>
<ul>
<li>解析方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">推荐  xpath (scrapy里集成了  xpath解析接口)</span><br></pre></td></tr></table></figure>
<ul>
<li>修改配置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 伪装请求载体身份</span><br><span class="line">19行：USER_AGENT = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36&apos; </span><br><span class="line"></span><br><span class="line">22行：ROBOTSTXT_OBEY = False  #可以忽略或者不遵守robots协议</span><br><span class="line"># 不遵守robots协议</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QiubaiSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;qiubai&apos;</span><br><span class="line">    # 最好注释这个允许域名   因为图片如果是其他域名的就获取不到了</span><br><span class="line">    # allowed_domains = [&apos;www.qiushibaike.com/text&apos;]</span><br><span class="line"></span><br><span class="line">    # 默认生成的协议是http的需要手动修改为 https</span><br><span class="line">    start_urls = [&apos;https://www.qiushibaike.com/text/&apos;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 推荐xpath(scrapy里集成了xpath解析接口)</span><br><span class="line"></span><br><span class="line">        # 每个段子的外层容器</span><br><span class="line">        div_list = response.xpath(&apos;//div[@id=&quot;content-left&quot;]/div&apos;)</span><br><span class="line"></span><br><span class="line">        for div in div_list:</span><br><span class="line">            # xpath解析指定内容被存到一个Selector对象里</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line">            author = div.xpath(&apos;./div/a[2]/h2/text()&apos;)</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;)</span><br><span class="line"></span><br><span class="line">            print(author) # [&lt;Selector xpath=&apos;./div/a[2]/h2/text()&apos; data=&apos;\n好吃的焦糖饼干～\n&apos;&gt;]</span><br><span class="line">            # xpath返回的 不是直接的内容而是一个列表  列表里是一个selector对象</span><br><span class="line">            &apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">            # extract()方法  可以将selector里存储的数据值拿到</span><br><span class="line">            # author = div.xpath(&apos;./div/a[2]/h2/text()&apos;).extract()</span><br><span class="line">            # content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract()</span><br><span class="line">            # print(author) # [&apos;\n好吃的焦糖饼干～\n&apos;]</span><br><span class="line">            author = div.xpath(&apos;./div/a[2]/h2/text()&apos;).extract_first()</span><br><span class="line">            content = div.xpath(&apos;.//div[@class=&quot;content&quot;]/span/text()&apos;).extract_first()</span><br><span class="line">            print(author) #  \n好吃的焦糖饼干～\n</span><br><span class="line"></span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>
<blockquote>
<p>执行命令</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 爬虫名称 ：该种执行形式会显示执行的日志信息</span><br><span class="line">scrapy crawl 爬虫名称 --nolog：该种执行形式不会显示执行的日志信息</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/01/Py007-01-12selenium和phantomJs使用/" title="Py007-01-12selenium使用" itemprop="url">Py007-01-12selenium使用</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-12-01T03:12:18.000Z" itemprop="datePublished"> Published 2018-12-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="selenium使用"><a href="#selenium使用" class="headerlink" title="selenium使用"></a>selenium使用</h3><ul>
<li>问题：处理页面动态加载数据的爬取</li>
</ul>
<ol>
<li>selenium</li>
<li>phantomJs</li>
</ol>
<h4 id="selenium安装"><a href="#selenium安装" class="headerlink" title="selenium安装"></a>selenium安装</h4><p>三方库，可以实现让浏览器完成自动化操作</p>
<ul>
<li>环境搭建</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 1 安装依赖</span><br><span class="line">pip install selenium</span><br><span class="line"># 2 获取某一款浏览器的驱动程序（以谷歌浏览器为例）</span><br><span class="line"># 谷歌浏览器驱动下载地址：http://chromedriver.storage.googleapis.com/index.html</span><br><span class="line"># </span><br><span class="line">下载的驱动程序必须和浏览器的版本统一，大家可以根据http://blog.csdn.net/huilan_same/article/details/51896672</span><br><span class="line">中提供的版本映射表进行对应</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="代码介绍"><a href="#代码介绍" class="headerlink" title="代码介绍"></a>代码介绍</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#导包</span><br><span class="line">from selenium import webdriver  </span><br><span class="line">#创建浏览器对象，通过该对象可以操作浏览器</span><br><span class="line">browser = webdriver.Chrome(&apos;驱动路径&apos;)</span><br><span class="line">#使用浏览器发起指定请求</span><br><span class="line">browser.get(url)</span><br><span class="line"></span><br><span class="line">#使用下面的方法，查找指定的元素进行操作即可</span><br><span class="line">    find_element_by_id            根据id找节点</span><br><span class="line">    find_elements_by_name         根据name找</span><br><span class="line">    find_elements_by_xpath        根据xpath查找</span><br><span class="line">    find_elements_by_tag_name     根据标签名找</span><br><span class="line">    find_elements_by_class_name   根据class名字查找</span><br></pre></td></tr></table></figure>
<blockquote>
<p>自动化打开百度搜索关键字后页面</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 编码流程</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from time import sleep</span><br><span class="line"># 创建一个浏览器对象 executable_path为驱动的路径</span><br><span class="line">bro = webdriver.Chrome(executable_path=&apos;./chromedriver&apos;)</span><br><span class="line"># get 方法可以指定一个url，让浏览器进行请求</span><br><span class="line">bro.get(&apos;https://www.baidu.com&apos;)</span><br><span class="line">sleep(3)</span><br><span class="line"># 让百度根据指定 搜索词进行搜索</span><br><span class="line">text = bro.find_element_by_id(&apos;kw&apos;) # 定位文本框</span><br><span class="line">text.send_keys(&apos;abc&apos;) # 向文本框录入指定内容</span><br><span class="line">sleep(1)</span><br><span class="line">button = bro.find_element_by_id(&apos;su&apos;)</span><br><span class="line">button.click() # 表示点击操作</span><br><span class="line">sleep(3)</span><br><span class="line"></span><br><span class="line">bro.quit()# 退出浏览器</span><br></pre></td></tr></table></figure>
<h3 id="phantomJs-不推荐使用，官网已经停更了"><a href="#phantomJs-不推荐使用，官网已经停更了" class="headerlink" title="phantomJs(不推荐使用，官网已经停更了)"></a>phantomJs(不推荐使用，官网已经停更了)</h3><ul>
<li>下载</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://phantomjs.org/download.html</span><br></pre></td></tr></table></figure>
<p>PhantomJS是一款无界面的浏览器，其自动化操作流程和上述操作谷歌浏览器是一致的。由于是无界面的，为了能够展示自动化操作流程，PhantomJS为用户提供了一个截屏的功能，使用save_screenshot函数实现。<br>代码演示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">from selenium import webdriver</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># phantomjs路径</span><br><span class="line">path = r&apos;/Users/almost/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs&apos;</span><br><span class="line">browser = webdriver.PhantomJS(path)</span><br><span class="line"></span><br><span class="line"># 打开百度</span><br><span class="line">url = &apos;http://www.baidu.com/&apos;</span><br><span class="line">browser.get(url)</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"></span><br><span class="line">browser.save_screenshot(r&apos;phantomjs\baidu.png&apos;)</span><br><span class="line"></span><br><span class="line"># 查找input输入框</span><br><span class="line">my_input = browser.find_element_by_id(&apos;kw&apos;)</span><br><span class="line"># 往框里面写文字</span><br><span class="line">my_input.send_keys(&apos;美女&apos;)</span><br><span class="line">time.sleep(3)</span><br><span class="line">#截屏</span><br><span class="line">browser.save_screenshot(r&apos;phantomjs\meinv.png&apos;)</span><br><span class="line"></span><br><span class="line"># 查找搜索按钮</span><br><span class="line">button = browser.find_elements_by_class_name(&apos;s_btn&apos;)[0]</span><br><span class="line">button.click()</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"></span><br><span class="line">browser.save_screenshot(r&apos;phantomjs\show.png&apos;)</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br><span class="line"></span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure>
<h4 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h4><blockquote>
<p>selenium+phantomjs 就是爬虫终极解决方案:有些网站上的内容信息是通过动态加载js形成的，所以使用普通爬虫程序无法回去动态加载的js内容。例如豆瓣电影中的电影信息是通过下拉操作动态加载更多的电影信息。</p>
</blockquote>
<blockquote>
<h4 id="综合操作：需求是尽可能多的爬取豆瓣网中的电影信息"><a href="#综合操作：需求是尽可能多的爬取豆瓣网中的电影信息" class="headerlink" title="综合操作：需求是尽可能多的爬取豆瓣网中的电影信息"></a>综合操作：需求是尽可能多的爬取豆瓣网中的电影信息</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from time import sleep</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    url = &apos;https://movie.douban.com/typerank?type_name=%E6%81%90%E6%80%96&amp;type=20&amp;interval_id=100:90&amp;action=&apos;</span><br><span class="line">    # 发起请求前，可以让url表示的页面动态加载出更多的数据</span><br><span class="line">    path = r&apos;你的phantomjs路径&apos;</span><br><span class="line">    # 创建无界面的浏览器对象</span><br><span class="line">    bro = webdriver.PhantomJS(path)</span><br><span class="line">    # 发起url请求</span><br><span class="line">    bro.get(url)</span><br><span class="line">    time.sleep(3)</span><br><span class="line">    # 截图</span><br><span class="line">    bro.save_screenshot(&apos;1.png&apos;)</span><br><span class="line"></span><br><span class="line">    # 执行js代码（让滚动条向下偏移n个像素（作用：动态加载了更多的电影信息））</span><br><span class="line">    js = &apos;window.scrollTo(0,document.body.scrollHeight)&apos;</span><br><span class="line">    bro.execute_script(js)  # 该函数可以执行一组字符串形式的js代码</span><br><span class="line">    time.sleep(2)</span><br><span class="line"></span><br><span class="line">    bro.execute_script(js)  # 该函数可以执行一组字符串形式的js代码</span><br><span class="line">    time.sleep(2)</span><br><span class="line">    bro.save_screenshot(&apos;2.png&apos;) </span><br><span class="line">    time.sleep(2) </span><br><span class="line">    # 使用爬虫程序爬去当前url中的内容 </span><br><span class="line">    html_source = bro.page_source # 该属性可以获取当前浏览器的当前页的源码（html） </span><br><span class="line">    with open(&apos;./source.html&apos;, &apos;w&apos;, encoding=&apos;utf-8&apos;) as fp: </span><br><span class="line">        fp.write(html_source) </span><br><span class="line">    bro.quit()</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="谷歌无头浏览器"><a href="#谷歌无头浏览器" class="headerlink" title="谷歌无头浏览器"></a>谷歌无头浏览器</h4></blockquote>
<p>由于PhantomJs最近已经停止了更新和维护，所以推荐大家可以使用谷歌的无头浏览器，是一款无界面的谷歌浏览器。</p>
<ul>
<li>代码展示：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.chrome.options import Options</span><br><span class="line">import time</span><br><span class="line"> </span><br><span class="line"># 创建一个参数对象，用来控制chrome以无界面模式打开</span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(&apos;--headless&apos;)</span><br><span class="line">chrome_options.add_argument(&apos;--disable-gpu&apos;)</span><br><span class="line"># 驱动路径</span><br><span class="line">path = r&apos;你的路径chromedriver.exe&apos;</span><br><span class="line"> </span><br><span class="line"># 创建浏览器对象</span><br><span class="line">browser = webdriver.Chrome(executable_path=path, chrome_options=chrome_options)</span><br><span class="line"> </span><br><span class="line"># 上网</span><br><span class="line">url = &apos;http://www.baidu.com/&apos;</span><br><span class="line">browser.get(url)</span><br><span class="line">time.sleep(3)</span><br><span class="line"> </span><br><span class="line">browser.save_screenshot(&apos;baidu.png&apos;)</span><br><span class="line"> </span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/12/01/Py007-01-11bs4使用/" title="Py007-01-11bs4使用" itemprop="url">Py007-01-11bs4使用</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-30T16:26:21.000Z" itemprop="datePublished"> Published 2018-12-01</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="BeautifulSoup解析"><a href="#BeautifulSoup解析" class="headerlink" title="BeautifulSoup解析"></a>BeautifulSoup解析</h3><p>python独有</p>
<blockquote>
<p>环境安装</p>
</blockquote>
<ul>
<li>需要将pip源设置为国内源，阿里源、豆瓣源、网易源等<ul>
<li>windows<br>（1）打开文件资源管理器(文件夹地址栏中)<br>（2）地址栏上面输入 %appdata%<br>（3）在这里面新建一个文件夹  pip<br>（4）在pip文件夹里面新建一个文件叫做  pip.ini ,内容写如下即可<br>   [global]<br>   timeout = 6000<br>   index-url = <a href="https://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple/</a><br>   trusted-host = mirrors.aliyun.com</li>
<li>linux<br>（1）cd ~<br>（2）mkdir ~/.pip<br>（3）vi ~/.pip/pip.conf<br>（4）编辑内容，和windows一模一样</li>
</ul>
</li>
<li>需要安装：pip install bs4<br>   bs4在使用时候需要一个第三方库，把这个库也安装一下<br>   pip install lxml</li>
</ul>
<blockquote>
<p>基础使用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">使用流程：       </span><br><span class="line">    - 导包：from bs4 import BeautifulSoup</span><br><span class="line">    - 使用方式：可以将一个html文档，转化为BeautifulSoup对象，然后通过对象的方法或者属性去查找指定的节点内容</span><br><span class="line">        （1）转化本地文件：</span><br><span class="line">             - soup = BeautifulSoup(open(&apos;本地文件&apos;), &apos;lxml&apos;)</span><br><span class="line">        （2）转化网络文件：</span><br><span class="line">             - soup = BeautifulSoup(&apos;字符串类型或者字节类型&apos;, &apos;lxml&apos;)</span><br><span class="line">        （3）打印soup对象显示内容为html文件中的内容</span><br><span class="line"></span><br><span class="line">基础巩固：</span><br><span class="line">    （1）根据标签名查找</span><br><span class="line">        - soup.a   只能找到第一个符合要求的标签</span><br><span class="line">    （2）获取属性</span><br><span class="line">        - soup.a.attrs  获取a所有的属性和属性值，返回一个字典</span><br><span class="line">        - soup.a.attrs[&apos;href&apos;]   获取href属性</span><br><span class="line">        - soup.a[&apos;href&apos;]   也可简写为这种形式</span><br><span class="line">    （3）获取内容</span><br><span class="line">        - soup.a.string</span><br><span class="line">        - soup.a.text</span><br><span class="line">        - soup.a.get_text()</span><br><span class="line">       【注意】如果标签还有标签，那么string获取到的结果为None，而其它两个，可以获取文本内容</span><br><span class="line">    （4）find：找到第一个符合要求的标签</span><br><span class="line">        - soup.find(&apos;a&apos;)  找到第一个符合要求的</span><br><span class="line">        - soup.find(&apos;a&apos;, title=&quot;xxx&quot;)</span><br><span class="line">        - soup.find(&apos;a&apos;, alt=&quot;xxx&quot;)</span><br><span class="line">        - soup.find(&apos;a&apos;, class_=&quot;xxx&quot;)</span><br><span class="line">        - soup.find(&apos;a&apos;, id=&quot;xxx&quot;)</span><br><span class="line">    （5）find_all：找到所有符合要求的标签</span><br><span class="line">        - soup.find_all(&apos;a&apos;)</span><br><span class="line">        - soup.find_all([&apos;a&apos;,&apos;b&apos;]) 找到所有的a和b标签</span><br><span class="line">        - soup.find_all(&apos;a&apos;, limit=2)  限制前两个</span><br><span class="line">    （6）根据选择器选择指定的内容</span><br><span class="line">               select:soup.select(&apos;#feng&apos;)</span><br><span class="line">        - 常见的选择器：标签选择器(a)、类选择器(.)、id选择器(#)、层级选择器</span><br><span class="line">            - 层级选择器：</span><br><span class="line">                div .dudu #lala .meme .xixi  下面好多级</span><br><span class="line">                div &gt; p &gt; a &gt; .lala          只能是下面一级</span><br><span class="line">        【注意】select选择器返回永远是列表，需要通过下标提取指定的对象</span><br></pre></td></tr></table></figure>
<h4 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h4><blockquote>
<p>使用bs4实现将诗词名句网站中三国演义小说的每一章的内容爬去到本地磁盘进行存储   <a href="http://www.shicimingju.com/book/sanguoyanyi.html" target="_blank" rel="noopener">http://www.shicimingju.com/book/sanguoyanyi.html</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">headers=&#123;</span><br><span class="line">         &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">     &#125;</span><br><span class="line">def parse_content(url):</span><br><span class="line">    #获取标题正文页数据</span><br><span class="line">    page_text = requests.get(url,headers=headers).text</span><br><span class="line">    soup = BeautifulSoup(page_text,&apos;lxml&apos;)</span><br><span class="line">    #解析获得标签</span><br><span class="line">    ele = soup.find(&apos;div&apos;,class_=&apos;chapter_content&apos;)</span><br><span class="line">    content = ele.text #获取标签中的数据值</span><br><span class="line">    return content</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">     url = &apos;http://www.shicimingju.com/book/sanguoyanyi.html&apos;</span><br><span class="line">     reponse = requests.get(url=url,headers=headers)</span><br><span class="line">     page_text = reponse.text</span><br><span class="line"></span><br><span class="line">     #创建soup对象</span><br><span class="line">     soup = BeautifulSoup(page_text,&apos;lxml&apos;)</span><br><span class="line">     #解析数据</span><br><span class="line">     a_eles = soup.select(&apos;.book-mulu &gt; ul &gt; li &gt; a&apos;)</span><br><span class="line">     print(a_eles)</span><br><span class="line">     cap = 1</span><br><span class="line">     fp =  open(&apos;./sanguo.txt&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line">     for ele in a_eles:</span><br><span class="line">         print(&apos;开始下载第%d章节&apos;%cap)</span><br><span class="line">         cap+=1</span><br><span class="line">         title = ele.string</span><br><span class="line">         content_url = &apos;http://www.shicimingju.com&apos;+ele[&apos;href&apos;]</span><br><span class="line">         content = parse_content(content_url)</span><br><span class="line"></span><br><span class="line">         fp.write(title+&quot;:&quot;+content+&apos;\n\n\n&apos;)</span><br><span class="line">         print(&apos;结束下载第%d章节&apos;%cap)</span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/30/Py007-01-10xpath使用/" title="Py007-01-10xpath使用" itemprop="url">Py007-01-10xpath使用</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-30T15:11:36.000Z" itemprop="datePublished"> Published 2018-11-30</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="xpath在爬虫中使用流程"><a href="#xpath在爬虫中使用流程" class="headerlink" title="xpath在爬虫中使用流程"></a>xpath在爬虫中使用流程</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- 1.下载: pip install lxml</span><br><span class="line">- 2.导包: from lxml import etree</span><br><span class="line">- 3.创建etree对象 进行指定数据的解析</span><br><span class="line">    - 本地   etree = etree.parse(&apos;本地文件路径&apos;)</span><br><span class="line">            etree.xpath(&apos;xpath表达式&apos;)</span><br><span class="line">    - 网络   etree = etree.HTML(&apos;网络请求到的页面数据&apos;)</span><br><span class="line">            etree.xpath(&apos;xpath表达式&apos;)</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="常用xpath表达式"><a href="#常用xpath表达式" class="headerlink" title="常用xpath表达式"></a>常用xpath表达式</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">属性定位：</span><br><span class="line">    #找到class属性值为song的div标签</span><br><span class="line">    //div[@class=&quot;song&quot;] </span><br><span class="line">层级&amp;索引定位：</span><br><span class="line">    #找到class属性值为tang的div的直系子标签ul下的第二个子标签li下的直系子标签a</span><br><span class="line">    //div[@class=&quot;tang&quot;]/ul/li[2]/a</span><br><span class="line">逻辑运算：</span><br><span class="line">    #找到href属性值为空且class属性值为du的a标签</span><br><span class="line">    //a[@href=&quot;&quot; and @class=&quot;du&quot;]</span><br><span class="line">模糊匹配：</span><br><span class="line">    # 当前所有div 同时class属性包含了 ng  </span><br><span class="line">    # 如 &lt;div class=&quot;song&quot;&gt; 和&lt;div class=&quot;tang&quot;&gt; 都会被匹配</span><br><span class="line">    //div[contains(@class, &quot;ng&quot;)]</span><br><span class="line">    # 当前所有div 的class属性 以 ta开头的</span><br><span class="line">    //div[starts-with(@class, &quot;ta&quot;)]</span><br><span class="line">取文本：</span><br><span class="line">    # /表示获取某个标签下的文本内容</span><br><span class="line">    //div[@class=&quot;song&quot;]/p[1]/text()</span><br><span class="line">    # //表示获取某个标签下的文本内容和所有子标签下的文本内容</span><br><span class="line">    //div[@class=&quot;tang&quot;]//text()</span><br><span class="line">取属性：</span><br><span class="line">    //div[@class=&quot;tang&quot;]//li[2]/a/@href</span><br></pre></td></tr></table></figure>
<h4 id="xpath插件"><a href="#xpath插件" class="headerlink" title="xpath插件"></a>xpath插件</h4><p>我们不可能每次都把html下载到本地  来写xpath表达式</p>
<blockquote>
<p>xpath插件可以直接作用在浏览器当中来校验xpath表达式</p>
</blockquote>
<ul>
<li>安装</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 更多工具</span><br><span class="line">2. 扩展程序</span><br><span class="line">3. 开启开发者模式</span><br><span class="line">4. XPath Helper 找到它 安装</span><br><span class="line">5. 快捷键</span><br><span class="line">    - 开启和关闭 xpath插件  ctrl + shift + x</span><br></pre></td></tr></table></figure>
<h4 id="段子网的段子内容和标题解析"><a href="#段子网的段子内容和标题解析" class="headerlink" title="段子网的段子内容和标题解析"></a>段子网的段子内容和标题解析</h4><ul>
<li><a href="https://ishuo.cn/joke" target="_blank" rel="noopener">https://ishuo.cn/joke</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">from lxml import etree</span><br><span class="line">import requests</span><br><span class="line"># 1. 指定url</span><br><span class="line">url=&apos;https://ishuo.cn/joke&apos;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"># 2 发请求</span><br><span class="line">response=requests.get(url,headers=headers).text</span><br><span class="line"># 3 获取页面内容</span><br><span class="line">page_text = response.text</span><br><span class="line">#4 解析</span><br><span class="line">tree=etree.HTML(page_text)</span><br><span class="line"># 获取所有li</span><br><span class="line">li_list=tree.xpath(&apos;//div[@id=&quot;list&quot;]/ul/li&apos;)</span><br><span class="line"># 注意 Element类型对象可以继续用xpath函数，该对象表示的局部内容进行指定内容的解析</span><br><span class="line"></span><br><span class="line">fp = open(&apos;./duanzi.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">for li in li_list:</span><br><span class="line">    content = li.xpath(&apos;./div[@class=&quot;content&quot;]/text()&apos;)[0]</span><br><span class="line">    title = li.xpath(&apos;./div[@class=&quot;info&quot;]/a/text()&apos;)[0]</span><br><span class="line">    # 5 持久化</span><br><span class="line">    fp.write(title+&apos;:&apos;+content+&apos;\n\n&apos;)</span><br><span class="line">    print(&apos;写入成功&apos;)</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2018/11/30/Py007-01-09爬虫之数据解析以及re操作/" title="Py007-01-09爬虫之数据解析以及re操作" itemprop="url">Py007-01-09爬虫之数据解析以及re操作</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Stevin" target="_blank" itemprop="author">Stevin</a>
		
  <p class="article-time">
    <time datetime="2018-11-30T14:27:28.000Z" itemprop="datePublished"> Published 2018-11-30</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h3><ol>
<li>指定url</li>
<li>发请求</li>
<li>获取页面数据</li>
<li>数据解析</li>
<li>进行持久化</li>
</ol>
<blockquote>
<h4 id="三种数据解析方式"><a href="#三种数据解析方式" class="headerlink" title="三种数据解析方式"></a>三种数据解析方式</h4></blockquote>
<ul>
<li>正则</li>
<li>bs4</li>
<li>xpath</li>
</ul>
<h3 id="正则re模块"><a href="#正则re模块" class="headerlink" title="正则re模块"></a>正则re模块</h3><p>用法回顾</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">单字符：</span><br><span class="line">        . : 除换行以外所有字符</span><br><span class="line">        [] ：[aoe] [a-w] 匹配集合中任意一个字符</span><br><span class="line">        \d ：数字  [0-9]</span><br><span class="line">        \D : 非数字</span><br><span class="line">        \w ：数字、字母、下划线、中文</span><br><span class="line">        \W : 非\w</span><br><span class="line">        \s ：所有的空白字符包,括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。</span><br><span class="line">        \S : 非空白</span><br><span class="line">    数量修饰：</span><br><span class="line">        * : 任意多次  &gt;=0</span><br><span class="line">        + : 至少1次   &gt;=1</span><br><span class="line">        ? : 可有可无  0次或者1次</span><br><span class="line">        &#123;m&#125; ：固定m次 hello&#123;3,&#125;</span><br><span class="line">        &#123;m,&#125; ：至少m次</span><br><span class="line">        &#123;m,n&#125; ：m-n次</span><br><span class="line">    边界：</span><br><span class="line">        $ : 以某某结尾 </span><br><span class="line">        ^ : 以某某开头</span><br><span class="line">    分组：</span><br><span class="line">        (ab)  </span><br><span class="line">    贪婪模式： .*</span><br><span class="line">    非贪婪（惰性）模式： .*?</span><br><span class="line"></span><br><span class="line">    re.I : 忽略大小写</span><br><span class="line">    re.M ：多行匹配</span><br><span class="line">    re.S ：单行匹配</span><br><span class="line"></span><br><span class="line">    re.sub(正则表达式, 替换内容, 字符串)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>代码示例</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line"># 提取python</span><br><span class="line">key = &apos;javapythonc++php&apos;</span><br><span class="line">res =re.findall(&apos;python&apos;,key)</span><br><span class="line">print(res) # [&apos;python&apos;]</span><br><span class="line"></span><br><span class="line"># 提取hello world</span><br><span class="line">key = &apos;&lt;html&gt;&lt;h1&gt;hello world&lt;/h1&gt;&lt;/html&gt;&apos;</span><br><span class="line">res =re.findall(&apos;&lt;h1&gt;(hello world)&lt;/h1&gt;&apos;,key)</span><br><span class="line">print(res) # [&apos;hello world&apos;]</span><br><span class="line"></span><br><span class="line"># 提取170</span><br><span class="line">key = &apos;我喜欢身高170的女孩&apos;</span><br><span class="line">res =re.findall(&apos;\d+&apos;,key)</span><br><span class="line">print(res) # [&apos;170&apos;]</span><br><span class="line"></span><br><span class="line"># 提取 http 和 https</span><br><span class="line">key = &apos;http://www.baidu.com and https://boob.com&apos;</span><br><span class="line">res =re.findall(&apos;https?&apos;,key)</span><br><span class="line">res2 = re.findall(&apos;https&#123;0,1&#125;&apos;,key)</span><br><span class="line">print(res,res2) # [&apos;http&apos;, &apos;https&apos;] [&apos;http&apos;, &apos;https&apos;]</span><br><span class="line"></span><br><span class="line"># 提取hit.</span><br><span class="line">key = &apos;bobo@hit.edu.com&apos;</span><br><span class="line">res =re.findall(&apos;h.*\.&apos;,key) # [&apos;hit.edu.&apos;]</span><br><span class="line">print(res) # [&apos;hit.edu.&apos;] 贪婪模式（默认）尽可能多的提取数据</span><br><span class="line">res2 =re.findall(&apos;h.*?\.&apos;,key)</span><br><span class="line">print(res2) # [&apos;hit.&apos;]</span><br><span class="line"></span><br><span class="line"># 提取 sas saas</span><br><span class="line">key = &apos;saas and sas and saaas&apos;</span><br><span class="line">res =re.findall(&apos;sa&#123;1,2&#125;s&apos;,key)</span><br><span class="line">print(res) # [&apos;saas&apos;, &apos;sas&apos;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 提取i开头的行</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">re.S 基于单行</span><br><span class="line">re.M 基于多行</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">key = &apos;&apos;&apos;fall in love with you</span><br><span class="line">i love you very much</span><br><span class="line">i love she</span><br><span class="line">i love her&apos;&apos;&apos;</span><br><span class="line">res = re.findall(&apos;^i.*&apos;,key,re.M)</span><br><span class="line">print(res)</span><br><span class="line"># [&apos;i love you very much&apos;, &apos;i love she&apos;, &apos;i love her&apos;]</span><br><span class="line"></span><br><span class="line"># 匹配所有全部行</span><br><span class="line">key = &apos;&apos;&apos;&lt;div&gt;静夜思</span><br><span class="line">窗前明月光</span><br><span class="line">疑是地上霜</span><br><span class="line">举头望明月</span><br><span class="line">低头思故乡</span><br><span class="line">&lt;/div&gt;&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">此时key是多行</span><br><span class="line">但是匹配的时候想要当作一行处理</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">res = re.findall(&apos;&lt;div&gt;.*&lt;/div&gt;&apos;,key,re.S)</span><br><span class="line">print(res)</span><br><span class="line"># [&apos;&lt;div&gt;静夜思\n窗前明月光\n疑是地上霜\n举头望明月\n低头思故乡\n&lt;/div&gt;&apos;]</span><br></pre></td></tr></table></figure>
<h3 id="需求：用正则对糗百的（糗图）图片数据进行解析和下载"><a href="#需求：用正则对糗百的（糗图）图片数据进行解析和下载" class="headerlink" title="需求：用正则对糗百的（糗图）图片数据进行解析和下载"></a>需求：用正则对糗百的（糗图）图片数据进行解析和下载</h3><ul>
<li><a href="https://www.qiushibaike.com/pic/" target="_blank" rel="noopener">https://www.qiushibaike.com/pic/</a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line">#-*- encoding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line">import os</span><br><span class="line">#1指定url</span><br><span class="line">url = &apos;https://www.qiushibaike.com/pic/&apos;</span><br><span class="line"></span><br><span class="line">#自定义请求头信息</span><br><span class="line">headers=&#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#2发请求</span><br><span class="line">response = requests.get(url=url,headers=headers)</span><br><span class="line"></span><br><span class="line">#3获取页面数据</span><br><span class="line">page_text = response.text</span><br><span class="line"></span><br><span class="line">#4数据解析</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">&lt;div class=&quot;thumb&quot;&gt;</span><br><span class="line">    &lt;a href=&quot;/article/121304175&quot; target=&quot;_blank&quot;&gt;</span><br><span class="line">        &lt;img src=&quot;//pic.qiushibaike.com/system/pictures/12130/121304175/medium/7RG0RFB30I0KWFMI.jpg&quot; alt=&quot;要把下面烫卷&quot;&gt;</span><br><span class="line">    &lt;/a&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 该列表中存储的就是当前页面源码中所有图片的url</span><br><span class="line">img_arr = re.findall(&apos;&lt;div class=&quot;thumb&quot;&gt;.*?&lt;img src=&quot;(.*?)&quot;.*?&gt;.*?&lt;/div&gt;&apos;,page_text,re.S)</span><br><span class="line"></span><br><span class="line"># 创建存储图片数据的文件夹</span><br><span class="line">if not os.path.exists(&apos;./imgs&apos;):</span><br><span class="line">    os.mkdir(&apos;imgs&apos;)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"># 发现是不含协议部分的</span><br><span class="line">[&apos;//pic.qiushibaike.com/system/pictures/12130/121304185/medium/6X8SQWYH0BBUO1KV.jpg&apos;,....]&apos;&apos;&apos;</span><br><span class="line"># 添加上协议</span><br><span class="line">for url in img_arr:</span><br><span class="line">    img_url = &apos;https:&apos; + url</span><br><span class="line">    # 持久化存储：存储的是图片的数据而不是url</span><br><span class="line">    # 获取图片二进制数据值</span><br><span class="line">    img_data = requests.get(url=img_url,headers=headers).content</span><br><span class="line"></span><br><span class="line">    # 将 6X8SQWYH0BBUO1KV.jpg 当做图片的名字</span><br><span class="line">    img_name = url.split(&apos;/&apos;)[-1]</span><br><span class="line">    img_path = &apos;imgs/&apos; + img_name</span><br><span class="line">    with open(img_path,&apos;wb&apos;) as f:</span><br><span class="line">        f.write(img_data)</span><br><span class="line">        print(img_name+&apos;写入成功&apos;)</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/M07/">M07</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/23/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><a class="page-number" href="/page/26/">26</a><span class="space">&hellip;</span><a class="page-number" href="/page/49/">49</a><a class="extend next" rel="next" href="/page/25/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  


  

  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/oak/" title="oak">oak<sup>71</sup></a></li>
			
		
			
				<li><a href="/tags/前端知识点/" title="前端知识点">前端知识点<sup>43</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>37</sup></a></li>
			
		
			
				<li><a href="/tags/Node后端/" title="Node后端">Node后端<sup>34</sup></a></li>
			
		
			
				<li><a href="/tags/M06/" title="M06">M06<sup>29</sup></a></li>
			
		
			
				<li><a href="/tags/fullstack/" title="fullstack">fullstack<sup>27</sup></a></li>
			
		
			
				<li><a href="/tags/M07/" title="M07">M07<sup>27</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>26</sup></a></li>
			
		
			
				<li><a href="/tags/M08/" title="M08">M08<sup>25</sup></a></li>
			
		
			
				<li><a href="/tags/M04/" title="M04">M04<sup>22</sup></a></li>
			
		
			
				<li><a href="/tags/M03/" title="M03">M03<sup>20</sup></a></li>
			
		
			
				<li><a href="/tags/M02/" title="M02">M02<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/React入门/" title="React入门">React入门<sup>17</sup></a></li>
			
		
			
				<li><a href="/tags/ReactWheels/" title="ReactWheels">ReactWheels<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/TS入门/" title="TS入门">TS入门<sup>16</sup></a></li>
			
		
			
				<li><a href="/tags/M01/" title="M01">M01<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/vue/" title="vue">vue<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/ES6速学/" title="ES6速学">ES6速学<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/mongodb/" title="mongodb">mongodb<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>7</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">Weibo</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=&verifier=b3593ceb&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Larry Page in Google. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/2176287895" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2020 
		
		<a href="/about" target="_blank" title="Stevin">Stevin</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>












<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
